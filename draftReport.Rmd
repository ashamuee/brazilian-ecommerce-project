---
title: "CYO Project Report - Brazilian Ecommerce"
author: "Asham Vohra"
date: "6/21/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Clustering:

Some customers didn't write a review. But why are they happy or mad?

Sales Prediction:

With purchase date information you'll be able to predict future sales.

Delivery Performance:

You will also be able to work through delivery performance and find ways to optimize delivery times.

Product Quality: (Let's see if we can find this)

Enjoy yourself discovering the products categories that are more prone to customer insatisfaction.

Attention

- An order might have multiple items.
- Each item might be fulfilled by a distinct seller.
- All text identifying stores and partners where replaced by the names of Game of Thrones great houses.

olist_customers_dataset.csv
This dataset has information about the customer and its location. Use it to identify unique customers in the orders dataset and to find the orders delivery location.
At our system each order is assigned to a unique customerid. This means that the same customer will get different ids for different orders. The purpose of having a customerunique_id on the dataset is to allow you to identify customers that made repurchases at the store. Otherwise you would find that each order had a different customer associated with.

olist_order_reviews_dataset.csv
This dataset includes data about the reviews made by the customers.

After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.


olist_orders_dataset.csv
This is the core dataset. From each order you might find all other information.

olist_products_dataset.csv
Products Dataset
This dataset includes data about the products sold by Olist.


# Overview
Data set: https://www.kaggle.com/olistbr/brazilian-ecommerce

# Analysis

Product Quality: (Let's see if we can find this)

Trying to identify the products categories that are more prone to customer insatisfaction. Some things come to mind
- categories with more poorly rated products? data set--> order_reviews_dataset
- categories which tend to be delivered late? data set-->
- expensive products maybe? do not get value for money?
- bulky products maybe?
- some categories maybe limited to few sellers and they may not be selling good products?


Likely classification problem that you select category and given paramters should it come undder customer satisfaction or not.
Anything less than 3 is unsatisfied
3 or more is satisfied

but for same product you may have some people satisfied and some as not. 
( I think that's where knn and related algorithms were introduced, to find mean of the points)

Would be good clustering project but may not be ideal as it based on unsupervised learning and its  coverage was limited.

Maybe it would be wiser to look at sales or delivery times problems.



Clustering:

So given order of an item, delivery time, product etc  can we try to predict customer rating. But will be so miliar to movielens project can become boring



Delivery Performance:

You will also be able to work through delivery performance and find ways to optimize delivery times.

Let's predict delivery time/date
-- delivery distance
-- location of customer as some can be remotely connected and need special delivery merchanism. This is independent of delivery distance.
-- location of seller as shipping from some locations maybe difficult. This is independent of delivery distance.
-- size of product? 
-- price of product (expensive nature of product as it could mean extra care or it maybe fragile?)
-- purchase time....if in night, delivery time maybe longer
-- order approved at time-> if payment approval came multiple hours after purchase time, this maybe longer
-- product category -- products of same category may show similar behavior.
-- or specific product - some products may have higher delivery time.
-- deliveries maybe impacted due to sales, festival season.

we should consider if we need time stamp or we can take purchase time as 0 and calculated approved at from it. though we may lose granularity in terms of time as truck may come everyday at 4. so after 4 you can ship , before 4 you cant


estimated delivery date is present in ---> order_estimated_delivery_date
We can calculate RMSE using that and it will be our benchmark

We can see if we improved than that or not. It will give us indication of how close are we to the production model RMSE.






```{r Install or load packages}
# TODO use conditional loading or installation of library
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
if (!require('lubridate')) install.packages('lubridate'); library('lubridate')
if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')
if (!require('geosphere')) install.packages('geosphere'); library('geosphere')
if (!require('caret')) install.packages('caret'); library('caret')
if (!require('plotly')) install.packages('plotly'); library('plotly')
```

```{r Basic analysis}

products_dataset <- read.csv(file.path('data','olist_products_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% select(-product_name_lenght,-product_description_lenght,-product_photos_qty)

products_dataset %>% head()
products_dataset %>% str()

product_category_name_translation <- read.csv(file.path('data','product_category_name_translation.csv'),header=TRUE,stringsAsFactors = FALSE)

product_category_name_translation %>% head()
product_category_name_translation %>% str()



# convert product_name to english form wherever possible
products_dataset <- products_dataset %>% left_join(product_category_name_translation,by='product_category_name') %>% mutate(product_category_name=if_else(is.na(product_category_name_english),product_category_name,product_category_name_english)) %>% select(-product_category_name_english) 

# further remove products for which data is not available i.e is NA
products_dataset <- products_dataset %>% filter(!is.na(product_weight_g) & !is.na(product_length_cm) & !is.na(product_height_cm) & !is.na(product_width_cm)  )

# assign category name for blank category
products_dataset <- products_dataset %>% mutate(product_category_name=if_else(product_category_name=='','UNCATEGORIZED',product_category_name))


# customers data set  with zip code prefix and can be used to uniquely identify a customer
# customer_id ----> key to the orders dataset. Each order has a unique customer_id.
# customer_zip_code_prefix --> first five digits of customer zip code
customers_dataset <- read.csv(file.path('data','olist_customers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 
# %>% mutate(customer_zip_code_prefix=as.factor(customer_zip_code_prefix),customer_id=as.character(customer_id ))

customers_dataset %>% head()
customers_dataset %>% str()


# geo location details
# in brazil while 5 digits zip code is good enough to identify state, it may not be correct to identify city
# it can have geolocation_zip_code_prefix shared across multiple places
# refer: https://codigo-postal.org/en-us/brazil/
# The Postal Code Address (CEP) is an eight-digit number set that has by main objective to guide and accelerate the distribution of objects sent through the national mail in Brazil. The current structure of the Brazilian postal code is 8 (eight) digits, divided into two parts:

# first part composed of 5 digits where each digit represents the 1) region, 2) sub-region, 3) sector, 4) subsector, 5) subsector divider;
# the second part is composed of 3 digits, separated by a hyphen from the first part and represents the Distribution Identifiers.
# You can navigate through states, then cities and finally neighborhoods and streets until you find the corresponding postal code.


geolocation_dataset <- read.csv(file.path('data','olist_geolocation_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 

geolocation_dataset %>% head()
geolocation_dataset %>% str()


# Important: many geolocation_city have same name but different ascent component. Example: niteroi and niterÃ³i
# intially we can see 
# geolocation_dataset %>% dplyr::count(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% nrow()

# post transformation to resolve any Accent or special characters from the city name
# geolocation_dataset <- geolocation_dataset %>% mutate(geolocation_city=stri_trans_general(geolocation_city,"Any-ASCII"))

# above is not done as the way zip codes are in brazil, first five digits can correspond to different cities and so we will remove the city. We also removed state as there seems to be incorrect entries in the sellers data. This either needs to be fixed or we can limit ourselves to lat/long value


# geolocation_dataset %>% dplyr::count(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% nrow()

# as each geolocation_zip_code_prefix can have many data points, we will limit these to one by taking mean for lat, long location
geolocation_dataset <- geolocation_dataset %>% group_by(geolocation_zip_code_prefix) %>% summarise(geolocation_lat=mean(geolocation_lat),geolocation_lng=mean(geolocation_lng)) %>% ungroup()
#geolocation_dataset <- geolocation_dataset %>% group_by(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% summarise(geolocation_lat=mean(geolocation_lat),geolocation_lng=mean(geolocation_lng)) %>% ungroup()

# TODO review it
#%>% mutate(geolocation_state=as.factor(geolocation_state)) 
# TODO review if geolocation_zip_code_prefix and geolocation_city can have multiple states with them.

# remove whitespace before and after reduces one city
# still issues are there and can be seen by
# zip_codes_of_interest <- geolocation_dataset %>% mutate(geolocation_city=str_trim(geolocation_city)) %>% dplyr::count(geolocation_zip_code_prefix) %>% filter(n>1)  %>% pull(geolocation_zip_code_prefix)

# geolocation_dataset %>% filter(geolocation_zip_code_prefix %in% zip_codes_of_interest) %>% view()
# to filter out duplicates
# we need not worry about city or state, we can pick any as long as lat, lng values are similar




# seller data set with zip code prefix
# seller_zip_code_prefix --> first 5 digits of seller zip code
sellers_dataset <- read.csv(file.path('data','olist_sellers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 
# TODO review if we need it
# %>% mutate(seller_zip_code_prefix=as.factor(seller_zip_code_prefix),seller_id=as.character(seller_id ))

sellers_dataset %>% head()
sellers_dataset %>% str()




# order data set with various delivery related times
# note each customer_id is unique per order
# order_purchase_timestamp -> Shows the purchase timestamp.
# order_approved_at/payment_approved_at -> Shows the payment approval timestamp.
# order_delivered_carrier_date/handed_over_to_carrier_date -> Shows the order posting timestamp. When it was handled to the logistic partner.
# order_delivered_customer_date --> Shows the actual order delivery date to the customer.
# order_estimated_delivery_date --> Shows the estimated delivery date that was informed to customer at the purchase moment.
orders_dataset <- read.csv(file.path('data','olist_orders_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% rename(payment_approved_at=order_approved_at,handed_over_to_carrier_date=order_delivered_carrier_date)  %>% mutate(order_status=as.factor(order_status),order_purchase_timestamp=as_datetime(order_purchase_timestamp),payment_approved_at=as_datetime(payment_approved_at),handed_over_to_carrier_date=as_datetime(handed_over_to_carrier_date),order_delivered_customer_date=as_datetime(order_delivered_customer_date),order_estimated_delivery_date=as_datetime(order_estimated_delivery_date))

# TODO if we are not using payment_approved_at we can remove the filter logic based on that.
orders_dataset <- orders_dataset %>% filter(order_status=='delivered') %>% filter( !is.na(order_estimated_delivery_date ) & !is.na(order_delivered_customer_date ) & !is.na(handed_over_to_carrier_date) & !is.na(payment_approved_at) & !is.na(order_purchase_timestamp))

orders_dataset %>% head()
orders_dataset %>% str()



# orders data with item related seller info
# order_item_id --->sequential number identifying number of items included in the same order.
# so if there are three items they will be nuber 1 , 2, 3 for the given order
# shipping_limit_date ---> Shows the seller shipping limit date for handling the order over to the logistic partner.
# freight_value --> item freight value item (if an order has more than one item the freight value is splitted between items)
order_items_dataset <- read.csv(file.path('data','olist_order_items_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% mutate(shipping_limit_date=as_datetime(shipping_limit_date))

order_items_dataset %>% str()
order_items_dataset %>% head()
```

```{r RMSE}
RMSE <- function(true_ratings, predicted_ratings) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```



```{r Refine datasets}
#
# Important: many geolocation_city have same name but different ascent component. Example: niteroi and niterÃ³i
# These all need to be handled in data wrangling


# we noticed some seller_zip_code_prefix are not present in geolocation dataset. So we will ignore them
sellers_to_ignore <- sellers_dataset %>% anti_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) 

order_items_dataset %>% inner_join(sellers_to_ignore,by='seller_id') %>% nrow()
# while there are decent number of order items for the concerned sellers, we will ignore them as we will still have sufficient data for analysis.


# let's use the one of interest
sellers_dataset <- sellers_dataset %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) %>% rename(geolocation_lat_seller=geolocation_lat,geolocation_lng_seller=geolocation_lng)

# TODO there seems to be an issue (Should be resolved now)
# geolocation_dataset seems to have 4 entries for same zip code . example 13454
# sellers_dataset %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) %>% dplyr::count(seller_id) %>% slice_max(order_by=n,n=5)

# we tried another approach by joining over all zip codee, city and state but it led to no match
# sellers_dataset %>% mutate(seller_city=stri_trans_general(seller_city,"Any-ASCII")) %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix','seller_city'='geolocation_city','seller_state'='geolocation_state')) %>% str()
# TODO -- if we analyse we can see city and state for seller data differ in both datasets. so best to stick to zip code. city and state are prone to errors.
# we decided against using city and state.



# similarly let see customers for whom we don't have data in geolocation_dataset
customers_to_ignore <- customers_dataset %>% anti_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) 


orders_dataset %>% inner_join(customers_to_ignore,by='customer_id') %>% nrow()
# while there are decent number of orders for these customers, we will ignore them as we will still have sufficient data for analysis.

# let's use customers of interest
customers_dataset <- customers_dataset %>% inner_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) %>% rename(geolocation_lat_customer=geolocation_lat,geolocation_lng_customer=geolocation_lng) 
```




```{r Combine data sets}
# we can combine orders delivery information (orders_dataset) with orders items and seller related info (order_items_dataset)
orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% head()

# let's combine with seller details
orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% head()

# let's combine with unique customer and its location dataset
orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>% select(-customer_id) %>% head()

# let's combine with products data as well
orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>% inner_join(products_dataset,by='product_id') %>% select(-customer_id) %>% head()

# seller to customer distance should be right predictor to consider, as the text says "Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners."
# source: https://www.kaggle.com/olistbr/brazilian-ecommerce
# we will use distGeo from geosphere package to compute distance in km between seller and customer. considered to be highly accurate. if it takes more time, we can consider using distVincentyEllipsoid or distHaversine as we are fine with accuracy of a km.
orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>%  inner_join(products_dataset,by='product_id') %>% select(-customer_id) %>% head() %>% mutate(distance_between_two_points=round(distGeo(matrix(c(geolocation_lng_seller,geolocation_lat_seller),ncol=2),matrix(c(geolocation_lng_customer,geolocation_lat_seller),ncol=2))/1000,2))


# so raw data
raw_data <- orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>% inner_join(products_dataset,by='product_id') %>%  mutate(distance_between_two_points=round(distGeo(matrix(c(geolocation_lng_seller,geolocation_lat_seller),ncol=2),matrix(c(geolocation_lng_customer,geolocation_lat_seller),ncol=2))/1000,2)) %>% select(-geolocation_lat_customer,-geolocation_lng_customer,-geolocation_lat_seller,-geolocation_lng_seller,-customer_id,-order_status,-seller_city,-seller_state,-customer_city,-customer_state) 

p1 <- raw_data %>% ggplot(aes(distance_between_two_points)) + geom_histogram(binwidth=50)
ggplotly(p1)

# identify if there was delay in delivery to carrier
raw_data <- raw_data %>% mutate(delayed_to_carrier=if_else((as.numeric(handed_over_to_carrier_date)-as.numeric(shipping_limit_date))>0,TRUE,FALSE))

wrangled_data <- raw_data %>% select(-shipping_limit_date,-handed_over_to_carrier_date,-payment_approved_at,-order_id,-order_estimated_delivery_date,-order_item_id,-delayed_to_carrier) %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name))

# we will predict number of hours to deliver based on various predictors. This seems more appropriate. Though as a result of this, we won't be able to use night, day time data of purchase which may influence delivery time (say to carrier it is informed only next day)
wrangled_data <- wrangled_data %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp) 
```


```{r Explore product_category_name relation with number_of_hours}
wrangled_data %>% group_by(product_category_name) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(product_category_name=reorder(product_category_name,mean)) %>% ggplot(aes(x=product_category_name,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
We can see that number_of_hours of delivery varies as number product category changes



```{r Explore price relation with number_of_hours}
# first let see how are the items associated with orders priced. Most of the order items a
p1 <- wrangled_data %>% mutate(price_group=round(price/100)+1) %>% ggplot(aes(price_group)) + geom_histogram(binwidth=1) + xlab('Each price_group is of 100 units each')
ggplotly(p1)


# let's plot errorbar for number of hours vs price_group.
# we can see while for price groups number of hours for delivery vary but the relationship is not clear enough. Higher priced items can also be delivered early
wrangled_data %>% mutate(price_group=round(price/50)+1) %>% mutate(price_group=as.factor(price_group)) %>% group_by(price_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=price_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(wrangled_data$price,wrangled_data$number_of_hours)
```
Price Group for a product does not give any clear indication. It does not seem to be impacting delivery time of product.


```{r Explore distance relationship between two points with number_of_hours}
# let's plot errorbar for number of hours vs distance group
# we can see a pattern that as distance increases delivery time increases. 
wrangled_data %>% mutate(distance_group=round(distance_between_two_points/50)+1) %>% mutate(distance_group=as.factor(distance_group)) %>% group_by(distance_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=distance_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(wrangled_data$distance_between_two_points,wrangled_data$number_of_hours)
```

Distance Group can be seen to influence delivery time.


```{r Explore freight value group with number_of_hours}
# let's plot errorbar for number of hours vs freight value group
wrangled_data %>% mutate(freight_value_group=round(freight_value/5)+1) %>% mutate(freight_value_group=as.factor(freight_value_group)) %>% group_by(freight_value_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=freight_value_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(wrangled_data$freight_value,wrangled_data$number_of_hours)
```

Above graphs indicates that for freight value group where each group represents 5 units, as freight value increases, number of hours for delivery increases.


```{r Explore seller_zip_code_prefix with number_of_hours}
# TODO anyway to add regularization or remove the low impact seller_zip_code_prefix from the model
# let's plot errorbar for number of hours vs seller_zip_code_prefix
sellers_zip_of_interest <- wrangled_data %>% dplyr::count(seller_zip_code_prefix) %>% filter(n>10) %>% pull(seller_zip_code_prefix)

wrangled_data %>% filter(seller_zip_code_prefix %in% sellers_zip_of_interest) %>% group_by(seller_zip_code_prefix) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(seller_zip_code_prefix=reorder(seller_zip_code_prefix,mean)) %>%  ggplot(aes(x=seller_zip_code_prefix,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90)) + scale_y_sqrt() 
```
Indicates role between seller_zip_code and number of hours for delivery



```{r Explore customer_zip_code_prefix with number_of_hours}
# TODO anyway to add regularization or remove the low impact customer_zip_code_prefix from the model
# let's plot errorbar for number of hours vs customer_zip_code_prefix
customers_zip_of_interest <- wrangled_data %>% dplyr::count(customer_zip_code_prefix) %>% filter(n>=5) %>% pull(customer_zip_code_prefix)

wrangled_data %>% filter(customer_zip_code_prefix %in% customers_zip_of_interest) %>% group_by(customer_zip_code_prefix) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(customer_zip_code_prefix=reorder(customer_zip_code_prefix,mean)) %>%  ggplot(aes(x=customer_zip_code_prefix,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90)) + scale_y_sqrt()
```
customer_zip_code_prefix can  be seen to be  influence number of hours for delivery


```{r Explore product_id with number_of_hours}
# TODO anyway to add regularization or remove the low impact products from the model
# let's plot errorbar for number of hours vs product_id
products_of_interest <- wrangled_data %>% dplyr::count(product_id)  %>% filter(n>=10) %>% pull(product_id)

wrangled_data %>% filter(product_id %in% products_of_interest) %>% group_by(product_id) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(product_id=reorder(product_id,mean)) %>%  ggplot(aes(x=product_id,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
It influences the number of hours


```{r Explore volume of product group with number_of_hours}
# let's plot errorbar for number of hours vs volume of product group
wrangled_data %>% mutate(volume_group=round((product_length_cm*product_width_cm*product_height_cm)/1000)+1) %>% mutate(volume_group=as.factor(volume_group)) %>% group_by(volume_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=volume_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
It does not influencee much the number of hours


```{r Explore weight of product with number_of_hours}
# let's plot errorbar for number of hours vs weight of product
wrangled_data %>% mutate(weight_group=round((product_weight_g)/1000)+1) %>% mutate(weight_group=as.factor(weight_group)) %>% mutate(weight_group=as.factor(weight_group)) %>% group_by(weight_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=weight_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
Some clues but not strongly correlated.





```{r Current RMSE}
# current RMSE for predicted delivery time given by order_estimated_delivery_date vs actual outcome give by order_delivered_customer_date
# However, note that order_estimated_delivery_date has no timestamp, so we will remove timestamp component, round date using floor_date, which means any delivery done during the day say at 2017-10-10 21:25:13 is treated to be done on 2017-10-10

# first we convert it to desired date
# then we covert date to numeric i.e. to convert it to seconds
orders_dataset %>% mutate(order_delivered_customer_date=floor_date(order_delivered_customer_date,unit="day")) %>% mutate(order_delivered_customer_date=as.numeric(order_delivered_customer_date),order_estimated_delivery_date=as.numeric(order_estimated_delivery_date))  %>% head()

# then we calculate time_taken_from_order_to_delivery, estimated_time_from_order_to_delivery in seconds
orders_dataset %>% mutate(order_delivered_customer_date=floor_date(order_delivered_customer_date,unit="day"),order_purchase_timestamp=as.numeric(order_purchase_timestamp)) %>%  mutate(time_taken_from_order_to_delivery=as.numeric(order_delivered_customer_date)-order_purchase_timestamp, estimated_time_from_order_to_delivery = as.numeric(order_estimated_delivery_date) -order_purchase_timestamp,diff=estimated_time_from_order_to_delivery-time_taken_from_order_to_delivery)

# if we look at graph we can  see there is huge difference between estimate data and actual delivery date
# it is possible some products are bulky or require differen mode of transport like ship 
p1 <- orders_dataset %>% mutate(order_delivered_customer_date=floor_date(order_delivered_customer_date,unit="day"),order_purchase_timestamp=as.numeric(order_purchase_timestamp)) %>%  mutate(time_taken_from_order_to_delivery=as.numeric(order_delivered_customer_date)-order_purchase_timestamp, estimated_time_from_order_to_delivery = as.numeric(order_estimated_delivery_date) -order_purchase_timestamp,diff=estimated_time_from_order_to_delivery-time_taken_from_order_to_delivery) %>% ggplot(aes(diff)) + geom_histogram(binwidth=86400*15) + xlab('Estimation error where each bar represents 15 days')

library(plotly)
ggplotly(p1)
```





Split data into training and validation set, where validation set will be used later
```{r Split data into training and validation set}
set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(wrangled_data$number_of_hours,times=1,p=0.2,list=FALSE)

training_dataset <- wrangled_data[-test_indices,]
validation_dataset <- wrangled_data[test_indices,]
#TODO ensure validation_dataset has products only for concerned product categories which are in training_dataset 
```


We use training_dataset and split into into train and test set. So that training_dataset is sufficient to fine tune the model
```{r Split data for training}
# Let's split training_dataset into two parts
set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(training_dataset$number_of_hours,times=1,p=0.2,list=FALSE)

train_subset <- training_dataset[-test_indices,]
test_subset <- training_dataset[test_indices,]
#TODO ensure test_subset has products only for concerned product categories which are in train_subset 
```

```{r Linear model}
# trying to reduce coefficients.
# in dimensions we can use volume to decrase predictors


########### only price predictor#############
## only price complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 220.41



## only price,distance_between_two_points complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 206.98


## only price,distance_between_two_points,product_weight_g complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_weight_g,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_weight_g))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 206.24


## only price,distance_between_two_points,product_weight_g,product_length_cm complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_weight_g,product_length_cm,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix)  %>% select(price,distance_between_two_points,product_weight_g,product_length_cm))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 206.22


## only price,distance_between_two_points,product_weight_g,product_length_cm,freight_value complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix)  %>% select(price,distance_between_two_points,product_weight_g,product_length_cm,freight_value,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_weight_g,product_length_cm,freight_value))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 205.44

## only price,distance_between_two_points,product_weight_g,product_length_cm,product_height_cm,freight_value complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_weight_g,product_length_cm,freight_value,product_height_cm,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_weight_g,product_length_cm,freight_value,product_height_cm))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 205.43



## only price,distance_between_two_points,product_weight_g,product_length_cm,product_height_cm,freight_value,product_width_cm complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix)  %>% select(price,distance_between_two_points,product_weight_g,product_length_cm,freight_value,product_height_cm,product_width_cm,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix)  %>% select(price,distance_between_two_points,product_weight_g,product_length_cm,freight_value,product_height_cm,product_width_cm))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 205.41


## only product_category_name complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(product_category_name,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(product_category_name))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 218.89



# if we combine this with other predictors
## only price,distance_between_two_points,product_category_name complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_category_name,number_of_hours), method='lm')

predictions <- predict(lm_fit,test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% select(price,distance_between_two_points,product_category_name))

RMSE(predictions,(test_subset  %>% select(-customer_unique_id,-seller_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix) %>% pull(number_of_hours) ))
# 204.87
```


To improve model performance consider--> changing character categorical variables to numerical categorical variables like
  
  product_category_name=as.factor(as.numeric(product_category_name)) 

use levels from original data for completeness

look for variables which have limited to no results and we can combine them as one categorical variable
Refer: https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/
Also read : feature hashing

Existing model algorithms should handle regularization. So we should not be required to handle ourselves.


We can use conditional model i.e. for some use different model and for some use different model just like ensemble.
So basically we could say for these pin codes, delivery time= say mean of delivery time and for others use standard algorthm like random forest etc.



# Results
# Conclusion
We can make fine predictions if we can predict time for approval, time for handling of goods to carrier and time for delivery separately as final prediction would be based on that.
But when a new order is made, we only know purchase time

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
