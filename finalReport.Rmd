---
title: "CYO Project Report - Brazilian Ecommerce"
author: "Asham Vohra"
date: "6/21/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




Delivery Performance:

You will also be able to work through delivery performance and find ways to optimize delivery times.

Attention

- An order might have multiple items.
- Each item might be fulfilled by a distinct seller.
- All text identifying stores and partners where replaced by the names of Game of Thrones great houses.

olist_customers_dataset.csv
This dataset has information about the customer and its location. Use it to identify unique customers in the orders dataset and to find the orders delivery location.
At our system each order is assigned to a unique customerid. This means that the same customer will get different ids for different orders. The purpose of having a customerunique_id on the dataset is to allow you to identify customers that made repurchases at the store. Otherwise you would find that each order had a different customer associated with.

olist_order_reviews_dataset.csv
This dataset includes data about the reviews made by the customers.

After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.


olist_orders_dataset.csv
This is the core dataset. From each order you might find all other information.

olist_products_dataset.csv
Products Dataset
This dataset includes data about the products sold by Olist.


# Overview
Data set: https://www.kaggle.com/olistbr/brazilian-ecommerce

# Analysis

Delivery Performance:

You will also be able to work through delivery performance and find ways to optimize delivery times.

Let's predict delivery time/date
-- delivery distance
-- location of customer as some can be remotely connected and need special delivery merchanism. This is independent of delivery distance.
-- location of seller as shipping from some locations maybe difficult. This is independent of delivery distance.
-- size of product? 
-- weight of the product. Heavier products may take time to deliver
-- price of product (expensive nature of product as it could mean extra care or it maybe fragile?)
-- purchase time....if in night, delivery time maybe longer
-- order approved at time-> if payment approval came multiple hours after purchase time, this maybe longer
-- product category -- products of same category may show similar behavior.
-- or specific product - some products may have higher delivery time.
-- deliveries maybe impacted due to sales, festival season.

we should consider if we need time stamp or we can take purchase time as 0 and calculated approved at from it. though we may lose granularity in terms of time as truck may come everyday at 4. so after 4 you can ship , before 4 you cant
We can be agnostic to purchase time and just predict number of hours for delivery for given purchase.


estimated delivery date is present in ---> order_estimated_delivery_date
We can calculate RMSE using that and it will be our benchmark

We can see if we improved than that or not. It will give us indication of how close are we to the production model RMSE.


TODO -> if there is delayed to carrier precendence, we could use that probability and average delay to carrier to fine tune our results. Similarly we could use other metrics like payment approval time etc.



```{r Install or load packages,include=FALSE, echo = FALSE}
# TODO use conditional loading or installation of library
# Remove plotly related code for pdfs
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
if (!require('lubridate')) install.packages('lubridate'); library('lubridate')
if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')
if (!require('geosphere')) install.packages('geosphere'); library('geosphere')
if (!require('caret')) install.packages('caret'); library('caret')
if (!require('randomForest')) install.packages('randomForest'); library('randomForest')
if (!require('knitr')) install.packages('knitr'); library('knitr')
```

## Data Wrangling
Reading from raw data source and converting data to usable form

For products data set,
- we read data, remove unnecessary fields, join with english names whereever available
- remove products where dimension and related attributes are not available
- assign default category name of UNCATEGORIZED for products where same was not available
```{r Data wrangling-products data set, include=FALSE, echo=FALSE}
products_dataset <- read.csv(file.path('data','olist_products_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% select(-product_name_lenght,-product_description_lenght,-product_photos_qty)

products_dataset %>% head()
products_dataset %>% str()

product_category_name_translation <- read.csv(file.path('data','product_category_name_translation.csv'),header=TRUE,stringsAsFactors = FALSE)

product_category_name_translation %>% head()


# convert product_name to english form wherever possible
products_dataset <- products_dataset %>% left_join(product_category_name_translation,by='product_category_name') %>% mutate(product_category_name=if_else(is.na(product_category_name_english),product_category_name,product_category_name_english)) %>% select(-product_category_name_english) 

# further remove products for which data is not available i.e is NA
products_dataset <- products_dataset %>% filter(!is.na(product_weight_g) & !is.na(product_length_cm) & !is.na(product_height_cm) & !is.na(product_width_cm)  )

# assign category name for blank category
products_dataset <- products_dataset %>% mutate(product_category_name=if_else(product_category_name=='','UNCATEGORIZED',product_category_name))
```
The wrangled data looks like below:
```{r Data wrangling result-products data set, echo=FALSE}
products_dataset %>% head() %>% knitr::kable()
```

For geolocation data set
```{r Data wrangling- geolocation data set, include=FALSE, echo=FALSE}
# geo location details
# in brazil while 5 digits zip code is good enough to identify state, it may not be correct to identify city
# it can have geolocation_zip_code_prefix shared across multiple places
# refer: https://codigo-postal.org/en-us/brazil/
# The Postal Code Address (CEP) is an eight-digit number set that has by main objective to guide and accelerate the distribution of objects sent through the national mail in Brazil. The current structure of the Brazilian postal code is 8 (eight) digits, divided into two parts:

# first part composed of 5 digits where each digit represents the 1) region, 2) sub-region, 3) sector, 4) subsector, 5) subsector divider;
# the second part is composed of 3 digits, separated by a hyphen from the first part and represents the Distribution Identifiers.
# You can navigate through states, then cities and finally neighborhoods and streets until you find the corresponding postal code.


geolocation_dataset <- read.csv(file.path('data','olist_geolocation_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 

geolocation_dataset %>% head()
geolocation_dataset %>% str()


# Important: many geolocation_city have same name but different ascent component. Example: niteroi and niterói
# intially we can see 
# geolocation_dataset %>% dplyr::count(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% nrow()

# post transformation to resolve any Accent or special characters from the city name
# geolocation_dataset <- geolocation_dataset %>% mutate(geolocation_city=stri_trans_general(geolocation_city,"Any-ASCII"))

# above is not done as the way zip codes are in brazil, first five digits can correspond to different cities and so we will remove the city. We also removed state as there seems to be incorrect entries in the sellers data. This either needs to be fixed or we can limit ourselves to lat/long value


# geolocation_dataset %>% dplyr::count(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% nrow()

# as each geolocation_zip_code_prefix can have many data points, we will limit these to one by taking mean for lat, long location
geolocation_dataset <- geolocation_dataset %>% group_by(geolocation_zip_code_prefix) %>% summarise(geolocation_lat=mean(geolocation_lat),geolocation_lng=mean(geolocation_lng)) %>% ungroup()
#geolocation_dataset <- geolocation_dataset %>% group_by(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% summarise(geolocation_lat=mean(geolocation_lat),geolocation_lng=mean(geolocation_lng)) %>% ungroup()

# TODO review it
#%>% mutate(geolocation_state=as.factor(geolocation_state)) 
# TODO review if geolocation_zip_code_prefix and geolocation_city can have multiple states with them.

# remove whitespace before and after reduces one city
# still issues are there and can be seen by
# zip_codes_of_interest <- geolocation_dataset %>% mutate(geolocation_city=str_trim(geolocation_city)) %>% dplyr::count(geolocation_zip_code_prefix) %>% filter(n>1)  %>% pull(geolocation_zip_code_prefix)

# geolocation_dataset %>% filter(geolocation_zip_code_prefix %in% zip_codes_of_interest) %>% view()
# to filter out duplicates
# we need not worry about city or state, we can pick any as long as lat, lng values are similar
```



```{r Data wrangling-customers data set, include=FALSE, echo=FALSE}
# customers data set  with zip code prefix and can be used to uniquely identify a customer
# customer_id ----> key to the orders dataset. Each order has a unique customer_id.
# customer_zip_code_prefix --> first five digits of customer zip code
customers_dataset <- read.csv(file.path('data','olist_customers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 
# %>% mutate(customer_zip_code_prefix=as.factor(customer_zip_code_prefix),customer_id=as.character(customer_id ))

customers_dataset %>% head()
customers_dataset %>% str()
```



```{r Data wrangling-sellers data set, echo=FALSE, include=FALSE}
# seller data set with zip code prefix
# seller_zip_code_prefix --> first 5 digits of seller zip code
sellers_dataset <- read.csv(file.path('data','olist_sellers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 
# TODO review if we need it
# %>% mutate(seller_zip_code_prefix=as.factor(seller_zip_code_prefix),seller_id=as.character(seller_id ))

sellers_dataset %>% head()
sellers_dataset %>% str()
```


Wrangled customers dataset
- removed data where delivery was not made
- removed data which was delivered but all the delivery details were not available
```{r Data wrangling-orders data set, include=FALSE, echo=FALSE}
# order data set with various delivery related times
# note each customer_id is unique per order
# order_purchase_timestamp -> Shows the purchase timestamp.
# order_approved_at/payment_approved_at -> Shows the payment approval timestamp.
# order_delivered_carrier_date/handed_over_to_carrier_date -> Shows the order posting timestamp. When it was handled to the logistic partner.
# order_delivered_customer_date --> Shows the actual order delivery date to the customer.
# order_estimated_delivery_date --> Shows the estimated delivery date that was informed to customer at the purchase moment.
orders_dataset <- read.csv(file.path('data','olist_orders_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% 
  rename(payment_approved_at=order_approved_at,
         handed_over_to_carrier_date=order_delivered_carrier_date)  %>% 
  mutate(order_status=as.factor(order_status),
         order_purchase_timestamp=as_datetime(order_purchase_timestamp),
         payment_approved_at=as_datetime(payment_approved_at),
         handed_over_to_carrier_date=as_datetime(handed_over_to_carrier_date),
         order_delivered_customer_date=as_datetime(order_delivered_customer_date),
         order_estimated_delivery_date=as_datetime(order_estimated_delivery_date))

# TODO if we are not using payment_approved_at or few fields we can remove the filter logic based on that.
orders_dataset <- orders_dataset %>% 
  filter(order_status=='delivered') %>% 
  filter( !is.na(order_estimated_delivery_date ) & !is.na(order_delivered_customer_date ) & !is.na(handed_over_to_carrier_date) & !is.na(payment_approved_at) & !is.na(order_purchase_timestamp))

orders_dataset %>% head()
orders_dataset %>% str()
```

```{r Data wrangling-order items data set, include=FALSE, echo=FALSE}
# orders data with item related seller info
# order_item_id --->sequential number identifying number of items included in the same order.
# so if there are three items they will be nuber 1 , 2, 3 for the given order
# shipping_limit_date ---> Shows the seller shipping limit date for handling the order over to the logistic partner.
# freight_value --> item freight value item (if an order has more than one item the freight value is splitted between items)
order_items_dataset <- read.csv(file.path('data','olist_order_items_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% mutate(shipping_limit_date=as_datetime(shipping_limit_date))

order_items_dataset %>% str()
order_items_dataset %>% head()
```

Performance metric
```{r RMSE, echo=FALSE}
RMSE <- function(true_outcomes, predicted_outcomes) {
  sqrt(mean((true_outcomes - predicted_outcomes)^2))
}
```


Filter out data where sellers location details are not available
```{r Refine sellers dataset, include=FALSE, echo=FALSE}
#
# Important: many geolocation_city have same name but different ascent component. Example: niteroi and niterói
# These all need to be handled in data wrangling


# we noticed some seller_zip_code_prefix are not present in geolocation dataset. So we will ignore them
sellers_to_ignore <- sellers_dataset %>% anti_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) 

order_items_dataset %>% inner_join(sellers_to_ignore,by='seller_id') %>% nrow()
# while there are decent number of order items for the concerned sellers, we will ignore them as we will still have sufficient data for analysis.


# let's use the one of interest
sellers_dataset <- sellers_dataset %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) %>% rename(geolocation_lat_seller=geolocation_lat,geolocation_lng_seller=geolocation_lng)

# TODO there seems to be an issue (Should be resolved now)
# geolocation_dataset seems to have 4 entries for same zip code . example 13454
# sellers_dataset %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) %>% dplyr::count(seller_id) %>% slice_max(order_by=n,n=5)

# we tried another approach by joining over all zip codee, city and state but it led to no match
# sellers_dataset %>% mutate(seller_city=stri_trans_general(seller_city,"Any-ASCII")) %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix','seller_city'='geolocation_city','seller_state'='geolocation_state')) %>% str()
# TODO -- if we analyse we can see city and state for seller data differ in both datasets. so best to stick to zip code. city and state are prone to errors.
# we decided against using city and state.
```

Wrangled sellers dataset ,looks like below:
```{r Refined sellers dataset result, echo=FALSE}
sellers_dataset %>% head() %>% knitr::kable()
```


```{r Refine customers data set, include=FALSE, echo=FALSE}
# similarly let see customers for whom we don't have data in geolocation_dataset
customers_to_ignore <- customers_dataset %>% anti_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) 


orders_dataset %>% inner_join(customers_to_ignore,by='customer_id') %>% nrow()
# while there are decent number of orders for these customers, we will ignore them as we will still have sufficient data for analysis.

# let's use customers of interest
customers_dataset <- customers_dataset %>% inner_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) %>% rename(geolocation_lat_customer=geolocation_lat,geolocation_lng_customer=geolocation_lng) 
```

Wrangled customers data set looks like below:

```{r Refined customers dataset result, echo=FALSE}
customers_dataset %>% head() %>% knitr::kable()
```


Combine segregated data sets
- create derived info i.e. distance between two points

```{r Combine data sets, include=FALSE, echo=FALSE}
# we can combine orders delivery information (orders_dataset) with orders items and seller related info (order_items_dataset)
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% head()

# let's combine with seller details
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% head()

# let's combine with unique customer and its location dataset
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>% select(-customer_id) %>% head()

# let's combine with products data as well
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>% inner_join(products_dataset,by='product_id') %>% select(-customer_id,-order_status,-seller_city,-seller_state,-customer_city,-customer_state) %>% head()

# so raw data
raw_data <- orders_dataset %>% 
  inner_join(order_items_dataset,by='order_id') %>%
  inner_join(sellers_dataset,by='seller_id') %>% 
  inner_join(customers_dataset,by='customer_id') %>%  
  inner_join(products_dataset,by='product_id') %>%
  mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),
         product_id=as.factor(product_id), 
         seller_id=as.factor(seller_id),
         seller_zip_code_prefix=as.factor(seller_zip_code_prefix),
         customer_unique_id=as.factor(customer_unique_id),
         customer_zip_code_prefix=as.factor(customer_zip_code_prefix),
         product_category_name=as.factor(product_category_name)) %>%
  select(-customer_id,-order_status,-seller_city,-seller_state,-customer_city, -payment_approved_at,-order_id,-order_item_id,-product_id,-seller_zip_code_prefix,-customer_unique_id)
```


Calculating current RMSE range
```{r Current RMSE, include=FALSE, echo = FALSE}
# current RMSE can be calculated by finding number of hours for predicted delivery time given by order_estimated_delivery_date vs actual outcome/number of hours give by order_delivered_customer_date
# However, note that order_estimated_delivery_date has no timestamp, so we will remove timestamp component, round date using floor_date, which means any delivery done during the day say at 2017-10-10 21:25:13 is treated to be done on 2017-10-10

# first we convert it to desired date
# then we covert date to numeric i.e. to convert it to seconds
# then we calculate time_taken_from_order_to_delivery, estimated_time_from_order_to_delivery in seconds
data_for_rmse <- raw_data %>%
  select(order_purchase_timestamp,order_delivered_customer_date,order_estimated_delivery_date) %>%
  mutate(order_delivered_customer_date_low=floor_date(order_delivered_customer_date,unit="day"),
         order_delivered_customer_date_high=ceiling_date(order_delivered_customer_date,unit="day")) %>% 
  mutate(order_delivered_customer_date_low=as.numeric(order_delivered_customer_date_low),
         order_delivered_customer_date_high=as.numeric(order_delivered_customer_date_high),
         order_estimated_delivery_date=as.numeric(order_estimated_delivery_date))  %>%
  mutate(time_taken_from_order_to_delivery_rangemax=order_delivered_customer_date_high-order_purchase_timestamp,
         time_taken_from_order_to_delivery_rangemin=order_delivered_customer_date_low-order_purchase_timestamp,
         estimated_time_from_order_to_delivery=order_estimated_delivery_date-order_purchase_timestamp) %>% 
  mutate(estimated_time_from_order_to_delivery=estimated_time_from_order_to_delivery/3600,
         time_taken_from_order_to_delivery_rangemax = time_taken_from_order_to_delivery_rangemax/3600,
         time_taken_from_order_to_delivery_rangemin = time_taken_from_order_to_delivery_rangemin/3600) %>%
  select(estimated_time_from_order_to_delivery,time_taken_from_order_to_delivery_rangemax,time_taken_from_order_to_delivery_rangemin)


RMSE_value_based_on_max <- RMSE(data_for_rmse$time_taken_from_order_to_delivery_rangemax,data_for_rmse$estimated_time_from_order_to_delivery)
# 359.9466

RMSE_value_based_on_min <- RMSE(data_for_rmse$time_taken_from_order_to_delivery_rangemin,data_for_rmse$estimated_time_from_order_to_delivery)
# 377.9515

# if we look at graph we can  see there is huge difference between estimated data and actual delivery date
# it is possible some products are bulky or require differen mode of transport like ship 
data_for_rmse %>% mutate(diff=estimated_time_from_order_to_delivery-1/2*(time_taken_from_order_to_delivery_rangemax+time_taken_from_order_to_delivery_rangemin)) %>% ggplot(aes(diff)) + geom_histogram(binwidth = 24*2) + xlab('Estimation error where each bar represents 2 days')

rm(data_for_rmse)
```
```{r Range of current rmse, echo=FALSE}
data.frame(type=character(),value=numeric()) %>% 
  add_row(type='RMSE Max',value=RMSE_value_based_on_max) %>% 
  add_row(type='RMSE Min',value=RMSE_value_based_on_min) %>% 
  knitr::kable(caption='Current RMSE')
```


Final wrangled data looks like below:
```{r Prepare final wrangled data set, include=FALSE}
# we will predict number of hours to deliver based on various predictors. This seems more appropriate. Though as a result of this, we won't be able to use night, day time data of purchase which may influence delivery time (say to carrier it is informed only next day)
wrangled_data <- raw_data %>%
  mutate(order_delivered_customer_date=as.numeric(order_delivered_customer_date)) %>%
  mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>%
  select(-order_purchase_timestamp,-order_delivered_customer_date,-order_estimated_delivery_date)
```

```{r Apply one hot encoding to product_category_name, include = FALSE}
set.seed(1,sample.kind = 'Rounding')
# fullRank parameter to make sure there is no perfect collinearity
product_category_dummyvars <- data.frame(predict(dummyVars(~product_category_name, data=wrangled_data, fullRank = TRUE), newdata = wrangled_data))

wrangled_data <- wrangled_data %>% cbind.data.frame(product_category_dummyvars)

wrangled_data %>% str()
rm(product_category_dummyvars)
```

```{r Final wrangled data,echo=FALSE}
wrangled_data %>% head() %>% knitr::kable()
```


```{r Cleanup and save required dataset, include=FALSE, echo=FALSE}
save(wrangled_data,file='rda/wrangled_data.rda')
rm(geolocation_dataset, order_items_dataset, orders_dataset, product_category_name_translation, products_dataset, sellers_dataset, sellers_to_ignore, customers_dataset, customers_to_ignore)
# TODO delete raw_data as well.
```

TODO
Use target encoding for categorical variables
Use preprocess for scaling data against training data set. Then use mean, sd from this trained data for test data.



Derived data set should be generated later on observations
- geolocation usage to capture distance
- delay and mean delay in handling to carrier
- number of hours for delivery should be generated later
```{r Method to derive distance data, include = FALSE}
# we will use distGeo from geosphere package to compute distance in km between seller and customer. considered to be highly accurate. if it takes more time, we can consider using distVincentyEllipsoid or distHaversine as we are fine with accuracy of a km.
extract_distance_between_seller_customer <- function(df) { 
  df %>% mutate(distance_between_two_points=round(distGeo(
    matrix(c(geolocation_lng_seller,geolocation_lat_seller),ncol=2),
    matrix(c(geolocation_lng_customer,geolocation_lat_customer),ncol=2)
    )/1000,2
    )) %>% select(-geolocation_lat_customer,-geolocation_lng_customer,-geolocation_lat_seller,-geolocation_lng_seller)
}
```

```{r Method to reduce levels in customer_zip_code_prefix, include = FALSE}
# trimming zip code to three digits to be able to extract zip code component which represents region, sub-region and sector data.
compute_rough_customer_location <- function(df) {
  df %>% mutate(customer_zip_code_prefix=str_pad(customer_zip_code_prefix, c(3),pad=c('0'),side='right')) %>% 
    mutate(customer_loc = as.numeric(str_extract(customer_zip_code_prefix,"\\d\\d\\d"))) 
  #%>% select(-customer_zip_code_prefix)
}
```

```{r Method to derive delay in handover to carrier, include = FALSE}
# calculated in hours
get_delay_in_handover_to_carrier_per_seller_id <- function(df) {
  df %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600)  %>% 
    group_by(seller_id) %>% 
    summarise(mean_delay_in_handover=mean(delay))
}
```

```{r Method to compute volume from available fields, include = FALSE}
# compute volume
compute_volume_using_product_info <- function(df) {
  df %>% mutate(volume=product_length_cm*product_height_cm*product_width_cm) %>%
    select(-product_length_cm,-product_height_cm,-product_width_cm)
}
```

Split data into training and validation set, where validation set will be used later
```{r Split data into training and validation set, include = FALSE, echo = FALSE}
set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(wrangled_data$number_of_hours,times=1,p=0.2,list=FALSE)

training_dataset <- wrangled_data[-test_indices,]
validation_dataset <- wrangled_data[test_indices,]

# ensure validation_dataset has products only for concerned product categories and seller_ids which are in training_dataset 
validation_dataset <- validation_dataset %>%
  semi_join(training_dataset,by='product_category_name') %>%
  semi_join(training_dataset,by='seller_id') %>% 
  semi_join(training_dataset,by="customer_zip_code_prefix")
## TODO review if we want to change this join
# validation_dataset <- validation_dataset %>% semi_join(training_dataset,by='seller_id')

rm(test_indices)
```


We use training_dataset and split into into train and test set. So that training_dataset is sufficient to fine tune the model
```{r Split data for training, include=FALSE}
# Let's split training_dataset into two parts
set.seed(1,sample.kind = 'Rounding')
training_dataset <- training_dataset %>% compute_volume_using_product_info() %>% extract_distance_between_seller_customer() %>% compute_rough_customer_location()

test_indices <- createDataPartition(training_dataset$number_of_hours,times=1,p=0.2,list=FALSE)

train_subset <- training_dataset[-test_indices,]
test_subset <- training_dataset[test_indices,]
# ensure test_subset has products only for concerned product categories and seller_ids which are in train_subset
test_subset <- test_subset %>% semi_join(train_subset,by='product_category_name') %>% semi_join(train_subset,by='seller_id') %>% 
  semi_join(train_subset,by="customer_zip_code_prefix")
#test_subset <- test_subset %>% semi_join(train_subset,by='seller_id')

penalty_data <- train_subset %>% get_delay_in_handover_to_carrier_per_seller_id()

rm(test_indices)
```



## Explore product_category_name relation with number_of_hours

```{r Explore product_category_name relation with number_of_hours, echo=FALSE}
training_dataset %>% group_by(product_category_name) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(product_category_name=reorder(product_category_name,mean)) %>% ggplot(aes(x=product_category_name,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```

We can see that number_of_hours of delivery varies as number product category changes


##Explore price relation with number_of_hours

```{r Explore price relation with number_of_hours, echo = FALSE}
# first let see how are the items associated with orders priced. Most of the order items a
training_dataset %>% mutate(price_group=round(price/100)+1) %>% ggplot(aes(price_group)) + geom_histogram(binwidth=1) + xlab('Each price_group is of 100 units each')


# let's plot errorbar for number of hours vs price_group.
# we can see while for price groups number of hours for delivery vary but the relationship is not clear enough. Higher priced items can also be delivered early
training_dataset %>% mutate(price_group=round(price/50)+1) %>% mutate(price_group=as.factor(price_group)) %>% group_by(price_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=price_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(training_dataset$price,training_dataset$number_of_hours)
```

Price Group for a product does not give any clear indication. It does not seem to be impacting delivery time of product.

## Explore distance relationship between two points with number_of_hours

```{r Explore distance relationship between two points with number_of_hours, echo = FALSE}
# let's plot errorbar for number of hours vs distance group
# we can see a pattern that as distance increases delivery time increases. 
training_dataset %>% mutate(distance_group=round(distance_between_two_points/50)+1) %>% mutate(distance_group=as.factor(distance_group)) %>% group_by(distance_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=distance_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(training_dataset$distance_between_two_points,training_dataset$number_of_hours)
```

Distance Group can be seen to influence delivery time.


```{r Explore freight value group with number_of_hours}
# let's plot errorbar for number of hours vs freight value group
training_dataset %>% mutate(freight_value_group=round(freight_value/5)+1) %>% mutate(freight_value_group=as.factor(freight_value_group)) %>% group_by(freight_value_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=freight_value_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(training_dataset$freight_value,training_dataset$number_of_hours)
```

Above graphs indicates that for freight value group where each group represents 5 units, as freight value increases, number of hours for delivery increases.

However, it dips near the end
```{r smooth fit for freight value, echo=FALSE}
training_dataset %>% ggplot(aes(x=freight_value,y=number_of_hours)) + geom_smooth() 
```


## Explore volume of product group with number_of_hours

```{r Explore volume of product group with number_of_hours, echo=FALSE}
# let's plot errorbar for number of hours vs volume of product group
training_dataset %>% mutate(volume_group=round(volume/1000)+1) %>% mutate(volume_group=as.factor(volume_group)) %>% group_by(volume_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=volume_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
It does not influencee much the number of hours

However a smooth fit shows some pattern 
```{r smooth fit for volume, echo=FALSE}
training_dataset %>% ggplot(aes(volume,number_of_hours)) + geom_smooth()
```

## Explore weight of product with number_of_hours

```{r Explore weight of product with number_of_hours}
# let's plot errorbar for number of hours vs weight of product
training_dataset %>% mutate(weight_group=round((product_weight_g)/1000)+1) %>% mutate(weight_group=as.factor(weight_group)) %>% mutate(weight_group=as.factor(weight_group)) %>% group_by(weight_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=weight_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
Some clues but not strongly correlated.

```{r Smooth fit for product, echo= FALSE}
training_dataset  %>% ggplot(aes(product_weight_g,number_of_hours)) + geom_smooth()
```

## Explore truncated customer_zip_code_prefix with number_of_hours

```{r Explore truncated customer_zip_code_prefix with number_of_hours}
# let's plot errorbar for number of hours vs truncated customer_zip_code_prefix
training_dataset %>% mutate(czc = as.numeric(str_extract(customer_zip_code_prefix,"\\d\\d\\d"))) %>% group_by(czc) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(czc=reorder(czc,mean)) %>% ggplot(aes(x=czc,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```


```{r Prepare training and test data for ,include=FALSE}
# penalty_data is computed on train_subset and is used with test_subset as historical learning of efficiency of each seller in delivering to carrier
train_subset <- train_subset %>% left_join(penalty_data,by='seller_id') %>% select(-handed_over_to_carrier_date, -shipping_limit_date)

test_subset <- test_subset %>% left_join(penalty_data,by='seller_id') %>% select(-handed_over_to_carrier_date, -shipping_limit_date)
```

==TODO note penalty data should be calculated again if train_subset is changed. Only trained data should be used for test, so that there is no data leak.


```{r Dataframe to store target encoding weights, include=FALSE}
target_encoding_weights <- data.frame(effect=character(0),weight=double(0)) 
```

```{r Apply Target encoding for seller_id, echo = FALSE}
#TODO you may need to remove customer_state and seller_state from dataset train_subset_for_encoding before proceeding further.

  train_subset_for_encoding <- train_subset[1:30000,] %>% select(-customer_loc, -customer_zip_code_prefix, -customer_state)

  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,6,3)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  #lambda_data_set <- prepare_lambda_data_set(index)
  #train_lambda_set <- lambda_data_set[[1]]
  #test_lambda_set <- lambda_data_set[[2]]  
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>%
    semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='seller_id') 
  
  weight_rmses <- sapply(weights, function(weight) {
      seller_id_effects <- train_weight_set %>% 
        group_by(seller_id) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n) %>% rename()
      
      train_weight_set <- train_weight_set %>% left_join(seller_id_effects,by='seller_id') %>% rename(seller_id_effect=mean) %>% select(-product_category_name,-seller_id)
      
      test_weight_set <- test_weight_set %>% left_join(seller_id_effects,by='seller_id') %>% rename(seller_id_effect=mean) %>% select(-product_category_name,-seller_id)
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      # TODO remove below scaling was erroneous. train scaling needs to be used for test data
      # below is just demo of how to use scale
      # scaled_train_subset <- train_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale)) 
      # scaled_test_subset <- test_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale))
      
      # TODO can we skip modelling while evaluating weight to use or for target encoding?
      
      tuningControl=trainControl(method = "cv", number = 2, p = .8)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(27,34,41)),trControl=tuningControl)
      
      knn_fit_for_weight
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition)  
# 3

target_encoding_weights <- target_encoding_weights %>% add_row(effect='seller_id',weight=weight_to_use)
```

```{r Post regularization for seller_id effect, echo=FALSE}

# prepare seller_if_effect based on regularisation weight we have identified
  global_mean <- mean(train_subset$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset$number_of_hours,p=0.2,times=5,list=FALSE)

seller_id_effects_list <- lapply(1:5, function(index) {
    train_weight_set <- train_subset[-test_weight_indices[,index],]

    seller_id_effects <- train_weight_set %>% 
      group_by(seller_id) %>%
      summarise(mean=mean(number_of_hours),n=n()) %>%
      mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
      select(-n) %>% rename()
    
    seller_id_effects
  }
)

sie <- seller_id_effects_list[[1]]
for (index in 2:5) {
   sie <- sie %>% full_join(seller_id_effects_list[[index]],by='seller_id', suffix=c("1",index))
}

# whereever value is NA, we have used global mean
sie <- sie %>% mutate(across(-seller_id,~if_else(is.na(.x),global_mean,.x)))

# as different partitions may not have exactly same data set combined the results
seller_id_effects <- tibble(sie[,1], seller_id_effect=rowMeans(sie[,-1],na.rm = TRUE))

train_subset <- train_subset %>% left_join(seller_id_effects,by='seller_id') %>% select(-product_category_name,-seller_id) %>% mutate(seller_id_effect=if_else(is.na(seller_id_effect),global_mean,seller_id_effect))
      
test_subset <- test_subset %>% left_join(seller_id_effects,by='seller_id') %>% select(-product_category_name,-seller_id)
```


```{r Apply Target encoding for customer_state, echo = FALSE}
  train_subset_for_encoding <- train_subset[1:40000,] %>% select(-customer_loc,-customer_zip_code_prefix)
  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,30,15)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  #lambda_data_set <- prepare_lambda_data_set(index)
  #train_lambda_set <- lambda_data_set[[1]]
  #test_lambda_set <- lambda_data_set[[2]]  
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>%
    #semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='customer_state') 
  
  weight_rmses <- sapply(weights, function(weight) {
      customer_state_effects <- train_weight_set %>% 
        group_by(customer_state) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n) %>% rename()
      
      train_weight_set <- train_weight_set %>% left_join(customer_state_effects,by='customer_state') %>% rename(customer_state_effect=mean) %>% select(-customer_state) #-product_category_name
      
      test_weight_set <- test_weight_set %>% left_join(customer_state_effects,by='customer_state') %>% rename(customer_state_effect=mean) %>% select(-customer_state) # -product_category_name,
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      # TODO remove below scaling was erroneous. train scaling needs to be used for test data
      # below is just demo of how to use scale
      # scaled_train_subset <- train_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale)) 
      # scaled_test_subset <- test_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale))
      
      # TODO can we skip modelling while evaluating weight to use or for target encoding?
      
      tuningControl=trainControl(method = "cv", number = 2, p = .8)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(20,27,34)),trControl=tuningControl)
      
      knn_fit_for_weight
      
      print(knn_fit_for_weight)
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition) 
# 20

target_encoding_weights <- target_encoding_weights %>% add_row(effect='customer_state',weight=weight_to_use)
```

```{r Post regularization with consumer state effect, echo=FALSE}

# prepare consumer_state_effect based on regularisation weight we have identified
  global_mean <- mean(train_subset$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset$number_of_hours,p=0.2,times=5,list=FALSE)

customer_state_effects_list <- lapply(1:5, function(index) {
    train_weight_set <- train_subset[-test_weight_indices[,index],]

    customer_state_effects <- train_weight_set %>% 
      group_by(customer_state) %>%
      summarise(mean=mean(number_of_hours),n=n()) %>%
      mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
      select(-n) %>% rename()
    
    customer_state_effects
  }
)

cse <- customer_state_effects_list[[1]]
for (index in 2:5) {
   cse <- cse %>% full_join(customer_state_effects_list[[index]],by='customer_state', suffix=c("1",index))
}

# whereever value is NA, we have used global mean
cse <- cse %>% mutate(across(-customer_state,~if_else(is.na(.x),global_mean,.x)))

# as different partitions may not have exactly same data set combined the results
customer_state_effects <- tibble(cse[,1], customer_state_effect=rowMeans(cse[,-1],na.rm = TRUE))

train_subset <- train_subset %>% left_join(customer_state_effects,by='customer_state') %>% select(-customer_state) %>% mutate(customer_state_effect=if_else(is.na(customer_state_effect),global_mean,customer_state_effect))
      
test_subset <- test_subset %>% left_join(customer_state_effects,by='customer_state') %>% select(-customer_state)
```


```{r Apply Target encoding for customer_loc, echo = FALSE}
  train_subset_for_encoding <- train_subset[1:40000,] %>% select(-customer_zip_code_prefix)
  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,10,5)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  #lambda_data_set <- prepare_lambda_data_set(index)
  #train_lambda_set <- lambda_data_set[[1]]
  #test_lambda_set <- lambda_data_set[[2]]  
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>%
    #semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='customer_loc') 
  
  weight_rmses <- sapply(weights, function(weight) {
      customer_loc_effects <- train_weight_set %>% 
        group_by(customer_loc) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n) %>% rename()
      
      train_weight_set <- train_weight_set %>% left_join(customer_loc_effects,by='customer_loc') %>% rename(customer_loc_effect=mean) %>% select(-customer_loc) #-product_category_name
      
      test_weight_set <- test_weight_set %>% left_join(customer_loc_effects,by='customer_loc') %>% rename(customer_loc_effect=mean) %>% select(-customer_loc) # -product_category_name,
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      # TODO remove below scaling was erroneous. train scaling needs to be used for test data
      # below is just demo of how to use scale
      # scaled_train_subset <- train_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale)) 
      # scaled_test_subset <- test_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale))
      
      # TODO can we skip modelling while evaluating weight to use or for target encoding?
      
      tuningControl=trainControl(method = "cv", number = 2, p = .8)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(20,27,34)),trControl=tuningControl)
      
      knn_fit_for_weight
      
      print(knn_fit_for_weight)
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition) 
# 5/3 = 1.66667

target_encoding_weights <- target_encoding_weights %>% add_row(effect='customer_loc',weight=weight_to_use)
```

```{r Post regularization with customer loc effect, echo=FALSE}

# prepare customer_loc_effect based on regularisation weight we have identified
  global_mean <- mean(train_subset$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset$number_of_hours,p=0.2,times=10,list=FALSE)

customer_loc_effects_list <- lapply(1:10, function(index) {
    train_weight_set <- train_subset[-test_weight_indices[,index],]

    customer_loc_effects <- train_weight_set %>% 
      group_by(customer_loc) %>%
      summarise(mean=mean(number_of_hours),n=n()) %>%
      mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
      select(-n) %>% rename()
    
    customer_loc_effects
  }
)

cle <- customer_loc_effects_list[[1]]
for (index in 2:10) {
   cle <- cle %>% full_join(customer_loc_effects_list[[index]],by='customer_loc', suffix=c("1",index))
}

# whereever value is NA, we have used global mean
cle <- cle %>% mutate(across(-customer_loc,~if_else(is.na(.x),global_mean,.x)))

# as different partitions may not have exactly same data set combined the results
customer_loc_effects <- tibble(cle[,1], customer_loc_effect=rowMeans(cle[,-1],na.rm = TRUE))

train_subset <- train_subset %>% left_join(customer_loc_effects,by='customer_loc') %>% select(-customer_loc,-customer_zip_code_prefix) %>% mutate(customer_loc_effect=if_else(is.na(customer_loc_effect),global_mean,customer_loc_effect))
      
test_subset <- test_subset %>% left_join(customer_loc_effects,by='customer_loc') %>% select(-customer_loc,-customer_zip_code_prefix)
```






```{r knn model data scaled under test, echo=FALSE}
# TODO review and improve or remove this as required
set.seed(seed=1,sample.kind = 'Rounding')
preProcessScalingValues <- preProcess(train_subset %>% select(-number_of_hours), method = c("center", "scale"))

scaled_train_subset <- predict(preProcessScalingValues, train_subset) 
scaled_test_subset <- predict(preProcessScalingValues, test_subset) 

# TODO remove below scaling was erroneous. train scaling needs to be used for test data
# below is just demo of how to use scale
# scaled_train_subset <- train_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale)) 
# scaled_test_subset <- test_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale))


tuningControl=trainControl(method = "cv", number = 3, p = .8)
knn_fit <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(27,34,41)),trControl=tuningControl)

knn_fit

predictions <- predict(knn_fit,scaled_test_subset %>% select(-number_of_hours))

RMSE(predictions,(scaled_test_subset %>% pull(number_of_hours) ))
#184.7906 for k=34.

save(knn_fit,file='rda/knn_model.rda')
```


```{r correlation, eval=FALSE}
cor(train_subset %>% select(price,freight_value,customer_loc_effect,customer_state_effect,seller_id_effect,distance_between_two_points,mean_delay_in_handover,volume,number_of_hours,product_weight_g),use="pairwise.complete.obs") %>% knitr::kable()
```

Random forest
However, it is important to know your data and keep in mind that a Random Forest can’t extrapolate. It can only make a prediction that is an average of previously observed labels. In this sense it is very similar to KNN.
In other words, in a regression problem, the range of predictions a Random Forest can make is bound by the highest and lowest labels in the training data. This behavior becomes problematic in situations where the training and prediction inputs differ in their range and/or distributions. This is called covariate shift and it is difficult for most models to handle but especially for Random Forest, because it can’t extrapolate.
From : https://towardsdatascience.com/a-limitation-of-random-forest-regression-db8ed7419e9f

```{r rf model, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 2, p = .8)
rf_fit <- train(number_of_hours~., train_subset, method='rf',tuneGrid = data.frame(mtry = seq(5, 40, 10)),trControl=tuningControl,ntree=250)
#rf_fit <- train(number_of_hours~., train_subset %>% select(-seller_state), method='rf',tuneGrid = data.frame(mtry = seq(5, 50, 10)),trControl=tuningControl,ntree=250) # nodesize = 50,maxnodes=2000,ntree=200

rf_fit

predictions <- predict(rf_fit,test_subset %>% select(-number_of_hours))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 175.0988 for mtry 15. Technically there are total of 82 variables due to 73 product categories and 9 other predictors. The seller_state was not of much help. it was actually increasing RMSE by .6. So have ignored it.

save(rf_fit,file='rda/rf_model.rda')
```


```{r Gam model, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')
preProcessScalingValues <- preProcess(train_subset %>% select(-number_of_hours), method = c("center", "scale"))

scaled_train_subset <- predict(preProcessScalingValues, train_subset) 
scaled_test_subset <- predict(preProcessScalingValues, test_subset) 

tuningControl=trainControl(method = "cv", number =3, p = .8)
gam_fit <- train(number_of_hours~., scaled_train_subset, method='gam',trControl=tuningControl) # check for tuning parameters

gam_fit

predictions <- predict(gam_fit,scaled_test_subset %>% select(-number_of_hours))

RMSE(predictions,(scaled_test_subset %>% pull(number_of_hours) ))
# 185.1074 uses GCV.Cp method and select=FALSE

save(gam_fit,file='rda/gam_model.rda')
```


```{r Prepare seller_id effect based on target encoding, include = FALSE}
# prepare seller_id_effect based on regularisation weight we have identified
prepare_seller_id_effect <- function(train_df, weight_to_use) {
    global_mean <- mean(train_df$number_of_hours)
    
    set.seed(1,sample.kind='Rounding')
    test_weight_indices <- createDataPartition(train_df$number_of_hours, p=0.2,times=10,list=FALSE)
    
    seller_id_effects_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]

      seller_id_effects <- train_weight_set %>% 
        group_by(seller_id) %>%
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
        select(-n) %>% rename()
    
      seller_id_effects
    })
    
    
    sie <- seller_id_effects_list[[1]]
    for (index in 2:10) {
       sie <- sie %>% 
         full_join(seller_id_effects_list[[index]],by='seller_id', suffix=c("1",index))
    }
    
    # whereever value is NA, we have used global mean
    sie <- sie %>% mutate(across(-seller_id,~if_else(is.na(.x),global_mean,.x)))
    
    # as different partitions may not have exactly same data set combined the results
    seller_id_effects <- tibble(sie[,1], seller_id_effect=rowMeans(sie[,-1],na.rm = TRUE))
    
    #seller id effects are ready
    seller_id_effects
}
```

```{r Prepare customer_state effect based on target encoding, include = FALSE}
# prepare consumer_state_effect based on regularisation weight we have identified
prepare_customer_state_effect <- function(train_df, weight_to_use) {
  global_mean <- mean(train_df$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_df$number_of_hours,p=0.2,times=10,list=FALSE)

  customer_state_effects_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]
  
      customer_state_effects <- train_weight_set %>% 
        group_by(customer_state) %>%
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
        select(-n) %>% rename()
      
      customer_state_effects
    }
  )

  cse <- customer_state_effects_list[[1]]
  for (index in 2:10) {
     cse <- cse %>% 
       full_join(customer_state_effects_list[[index]],by='customer_state', suffix=c("1",index))
  }

  # whereever value is NA, we have used global mean
  cse <- cse %>% mutate(across(-customer_state,~if_else(is.na(.x),global_mean,.x)))
  
  # as different partitions may not have exactly same data set combined the results
  customer_state_effects <- tibble(cse[,1], customer_state_effect=rowMeans(cse[,-1],na.rm = TRUE))
  
  #customer state effects are ready
  customer_state_effects
}
```

```{r Prepare customer_loc effect based on target encoding, include = FALSE}
# prepare customer_loc_effect based on regularisation weight we have identified
prepare_customer_loc_effect <- function(train_df, weight_to_use) {
  global_mean <- mean(train_df$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_df$number_of_hours,p=0.2,times=10,list=FALSE)

  customer_loc_effects_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]
  
      customer_loc_effects <- train_weight_set %>% 
        group_by(customer_loc) %>%
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
        select(-n) %>% rename()
      
      customer_loc_effects
    }
  )

  cle <- customer_loc_effects_list[[1]]
  for (index in 2:10) {
     cle <- cle %>% full_join(customer_loc_effects_list[[index]],by='customer_loc', suffix=c("1",index))
  }

  # where-ever value is NA, we have used global mean
  cle <- cle %>% mutate(across(-customer_loc,~if_else(is.na(.x),global_mean,.x)))

  # as different partitions may not have exactly same data set combined the results
  customer_loc_effects <- tibble(cle[,1], customer_loc_effect=rowMeans(cle[,-1],na.rm = TRUE))
  
  #customer loc effects are ready
  customer_loc_effects
}
```




```{r Enhance training and test dataset for seller_id effect based on target encoding, include = FALSE}
enhance_datasets_using_seller_id_effect <- function(seller_id_effects,train_df,test_df,cleanup) {
  #we will use seller id effects computed using training data in test data.
  train_df <- train_df %>% left_join(seller_id_effects,by='seller_id') %>% mutate(seller_id_effect=if_else(is.na(seller_id_effect),global_mean,seller_id_effect))
        
  test_df <- test_df %>% left_join(seller_id_effects,by='seller_id')
  
  # cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-product_category_name,-seller_id)
    test_df <- test_df %>% select(-product_category_name,-seller_id)
  }
  
  list(train_df,test_df)
}
```


```{r Enhance training and test dataset for customer_state effect based on target encoding, include = FALSE}
enhance_datasets_using_customer_state_effect <- function(customer_state_effects,train_df,test_df,cleanup) {
  #we will use customer state effects computed using training data in test data.
  train_df <- train_df %>% left_join(customer_state_effects,by='customer_state') %>%
    mutate(customer_state_effect=if_else(is.na(customer_state_effect),
                                         global_mean,
                                         customer_state_effect))
        
  test_df <- test_df %>% left_join(customer_state_effects,by='customer_state') 
  
  # cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-customer_state)
    test_df <- test_df %>% select(-customer_state)
  }
  
  list(train_df,test_df)
}
```

```{r Enhance training and test dataset for customer_loc effect based on target encoding, include = FALSE}
enhance_datasets_using_customer_loc_effect <- function(customer_loc_effects,train_df,test_df,cleanup) {
  #we will use customer loc effects computed using training data in test data.
  train_df <- train_df %>% 
    left_join(customer_loc_effects,by='customer_loc') %>%
    mutate(customer_loc_effect=if_else(is.na(customer_loc_effect),global_mean,customer_loc_effect))
        
  test_df <- test_df %>% left_join(customer_loc_effects,by='customer_loc')
  
  # cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-customer_loc,-customer_zip_code_prefix)
    test_df <- test_df %>% select(-customer_loc,-customer_zip_code_prefix)
  }
  
  list(train_df,test_df)
}
```


```{r Prepare data set for training, include = FALSE}
# prepares data for training and testing
# returns list of data frames. First list has training dataframe. Second list has test data frame
prepare_data_for_training_testing <- function(train_ds, test_ds, cleanup) {
  print('Adding column for delay in handover')
  penalty_data <- train_ds %>% get_delay_in_handover_to_carrier_per_seller_id()
  
  #we will use penalty_data computed using training data in test data.
  train_ds <- train_ds %>% left_join(penalty_data,by='seller_id') 
  test_ds <- test_ds %>% left_join(penalty_data,by='seller_id')
  if (cleanup) {
    train_ds <- train_ds %>% select(-handed_over_to_carrier_date,-shipping_limit_date)
    test_ds <- test_ds %>% select(-handed_over_to_carrier_date,-shipping_limit_date)
  }

  print('Enhancing data for seller id effects')
  weight_to_use <- target_encoding_weights %>% filter(effect=='seller_id') %>% pull(weight)
  seller_id_effects <- prepare_seller_id_effect(train_ds,weight_to_use)
  dataset_list <- enhance_datasets_using_seller_id_effect(seller_id_effects,train_ds,test_ds,cleanup)
  train_ds <- dataset_list[[1]]
  test_ds <- dataset_list[[2]]
  
  print('Enhancing data for customer state effects')
  weight_to_use <- target_encoding_weights %>% filter(effect=='customer_state') %>% pull(weight)
  customer_state_effects <- prepare_customer_state_effect(train_ds,weight_to_use)
  dataset_list <- enhance_datasets_using_customer_state_effect(customer_state_effects,train_ds,test_ds,cleanup)
  train_ds <- dataset_list[[1]]
  test_ds <- dataset_list[[2]]
  
  print('Enhancing data for customer loc effects')
  weight_to_use <- target_encoding_weights %>% filter(effect=='customer_loc') %>% pull(weight)
  customer_loc_effects <- prepare_customer_loc_effect(train_ds,weight_to_use)
  dataset_list <- enhance_datasets_using_customer_loc_effect(customer_loc_effects,train_ds,test_ds,cleanup)
  dataset_list
}
```



```{r Prepare dataset for final model training, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')

validation_dataset <- validation_dataset %>% 
  compute_volume_using_product_info() %>% 
  extract_distance_between_seller_customer() %>% 
  compute_rough_customer_location()

dataset_list <- prepare_data_for_training_testing(training_dataset,validation_dataset,FALSE)
training_dataset <- dataset_list[[1]]
validation_dataset <- dataset_list[[2]]

validation_dataset <- validation_dataset %>%
  semi_join(training_dataset,by='product_category_name') %>% 
  semi_join(training_dataset,by='seller_id') %>% 
  semi_join(training_dataset,by='customer_state') %>%
  semi_join(training_dataset,by='customer_loc')

# cleanup
training_dataset <- training_dataset %>%
  select(-handed_over_to_carrier_date,-shipping_limit_date,-product_category_name,-seller_id,-customer_state,-customer_loc,-customer_zip_code_prefix)
validation_dataset <- validation_dataset %>%
  select(-handed_over_to_carrier_date,-shipping_limit_date,-product_category_name,-seller_id,-customer_state,-customer_loc,-customer_zip_code_prefix)
```


```{r Final Model Training, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')

mtry_to_use <- rf_fit$finalModel$mtry
ntree_to_use <- 500 #rf_fit$finalModel$ntree
#tuningControl <- trainControl(method = "cv", number = 2, p = .8)
#final_model <- train(number_of_hours~., training_dataset, method='rf',tuneGrid = data.frame(mtry = c(mtry_to_use)),trControl=tuningControl,ntree=ntree_to_use) 

final_model <- randomForest(number_of_hours~., training_dataset,mtry= mtry_to_use, ntree=ntree_to_use)

final_model

save(final_model,file='rda/final_model.rda')
```


```{r Final model validation, echo = FALSE}
predictions <- predict(final_model,validation_dataset %>% select(-number_of_hours))

RMSE((validation_dataset %>% pull(number_of_hours) ),predictions)
# 190.184 for mtry=15, ntree=500
```





===== TODO review from here

Data used maybe incorrect
Review joins and if multiple orders of same type are being delivered at the same time
refer
> training_dataset %>% filter(number_of_hours>=354.73 & number_of_hours<354.74)
   price freight_value product_category_name product_weight_g number_of_hours volume
1   59.0         13.43          garden_tools             1550        354.7386  19800
2   59.0         13.43          garden_tools             1550        354.7386  19800
3   59.0         13.43          garden_tools             1550        354.7386  19800
4   59.0         13.43          garden_tools             1550        354.7386  19800
5   59.0         13.43          garden_tools             1550        354.7386  19800
6   59.0         13.43          garden_tools             1550        354.7386  19800
7   59.0         13.43          garden_tools             1550        354.7386  19800
8   59.0         13.43          garden_tools             1550        354.7386  19800
9   59.0         13.43          garden_tools             1550        354.7386  19800
10  59.0         13.43          garden_tools             1550        354.7386  19800
11  59.0         13.43          garden_tools             1550        354.7386  19800
12  66.0         14.21       furniture_decor             1150        354.7319  28175
13  46.9         17.63        sports_leisure              300        354.7378   2304
   distance_between_two_points mean_delay_in_handover
1                       469.74                -51.60514
2                       469.74                -51.60514
3                       469.74                -51.60514
4                       469.74                -51.60514
5                       469.74                -51.60514
6                       469.74                -51.60514
7                       469.74                -51.60514
8                       469.74                -51.60514
9                       469.74                -51.60514
10                      469.74                -51.60514
11                      469.74                -51.60514
12                      807.32                -56.57523
13                     1083.46               -147.48749


```{r Derived data}
# seller to customer distance should be right predictor to consider, as the text says "Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners."
# source: https://www.kaggle.com/olistbr/brazilian-ecommerce
wrangled_data %>% head() %>% extract_distance_between_seller_customer()

# plot of how 
p1 <- wrangled_data %>% extract_distance_between_seller_customer() %>% ggplot(aes(distance_between_two_points)) + geom_histogram(binwidth=50)
ggplotly(p1)
```

```{r Explore delayed handover to carrier, echo=FALSE }
# TODO we maybe able to remove this as it is not used, because this is the info which is generally not available
# identify if there was delay in delivery to carrier
wrangled_data <- wrangled_data %>% mutate(delayed_to_carrier=if_else((as.numeric(handed_over_to_carrier_date)-as.numeric(shipping_limit_date))>0,TRUE,FALSE))

wrangled_data <- wrangled_data %>% select(-shipping_limit_date,-handed_over_to_carrier_date,-order_estimated_delivery_date)
```



===TODO review

look for variables which have limited to no results and we can combine them as one categorical variable
Refer: https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/
Also read : feature hashing

Existing model algorithms should handle regularization. So we should not be required to handle ourselves.


We can use conditional model i.e. for some use different model and for some use different model just like ensemble.
So basically we could say for these pin codes, delivery time= say mean of delivery time and for others use standard algorthm like random forest etc.

gam?
gamLoess??


```{r ensemble, echo = FALSE}
load('rda/gamLoess_model.rda')
load('rda/gam_model.rda')
load('rda/knn_model.rda')
load('rda/lm_model.rda')
load('rda/rf_model.rda')

predictions_gamLoess <- predict(gamLoess_fit,test_subset %>% select(-number_of_hours))
predictions_knn <- predict(knn_fit,test_subset %>% select(-number_of_hours))
predictions_rf <- predict(rf_fit,test_subset %>% select(-number_of_hours))
predictions_gam <- predict(gam_fit,test_subset %>% select(-number_of_hours))
predictions_glm <- predict(glm_fit,test_subset %>% select(-number_of_hours))

predictions_df <- data.frame(predictions_gamLoess,predictions_knn,predictions_rf,predictions_gam,predictions_glm)

RMSE(rowMeans(predictions_df),(test_subset %>% pull(number_of_hours) ))
# 194.8623

RMSE(apply(predictions_df %>% select(predictions_rf,predictions_gam,predictions_knn) %>% as.matrix(),1,'median'),(test_subset %>% pull(number_of_hours) ))
# 194.25
```

TODO-- what if we remove freight value as it depends on number of items in order too.as it is divided by number of items


```{r delayed to carrier experiment, eval=FALSE}
# let's see how delay in carrier influences overall delivery time
raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% group_by(delayed_to_carrier) %>% summarise(mean(number_of_hours))

# let's see time contribution due to delay or early drop to carrier 
raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600) %>% select(delay,delayed_to_carrier) %>% group_by(delayed_to_carrier) %>% summarise(mean(delay))

# sellers of interest - we have sellers to be penalized(Add delivery time) and sellers to be corrected(reduce delivery time)
seller_ids_of_interest <- raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600) %>% select(delayed_to_carrier, seller_id) %>% dplyr::count(seller_id,delayed_to_carrier) %>% spread(delayed_to_carrier,n,fill=0) %>% mutate(delay_percentage=`TRUE`/(`TRUE`+`FALSE`),total=`TRUE`+`FALSE`) %>% filter(total>=10 & (delay_percentage >=0.75 | delay_percentage <=0.25)) %>% pull(seller_id)


# dummy data points
dummy_wrangled_data <- raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% filter(seller_id %in% seller_ids_of_interest) %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600) %>% select(-shipping_limit_date,-handed_over_to_carrier_date,-payment_approved_at,-order_id,-order_estimated_delivery_date,-order_item_id,-delayed_to_carrier)


penalty_data <- wrangled_data %>% get_delay_in_handover_to_carrier_per_seller_id()


set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(dummy_wrangled_data$number_of_hours,times=1,p=0.2,list=FALSE)
dummy_data <- dummy_wrangled_data[-test_indices,]
dummy_data <- dummy_data %>% mutate(volume=product_length_cm*product_height_cm*product_width_cm) %>%  select(-customer_unique_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix,-product_length_cm,-product_height_cm,-product_width_cm)


set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(dummy_data$number_of_hours,times=1,p=0.2,list=FALSE)
dummy_train_subset <- dummy_data[-test_indices,]
dummy_test_subset <- dummy_data[test_indices,]

# this penalty data can later be reused with test data as well
penalty_data <- dummy_train_subset %>% group_by(seller_id) %>% summarise(mean_delay_in_handover=mean(delay))

dummy_train_subset <- dummy_train_subset %>% left_join(penalty_data,by='seller_id')

# only keeping those rows which have seller_id in training set
dummy_test_subset <- dummy_test_subset %>% semi_join(dummy_train_subset,by='seller_id')



dummy_train_subset <- dummy_train_subset %>% select(-seller_id,-delay)
# Let's try with rf model
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 2, p = .8)
# rf_fit_dummy <- train(number_of_hours~., dummy_train_subset, method='rf',tuneGrid = data.frame(mtry = seq(5, 15, 4)),trControl=tuningControl) # nodesize = 50,maxnodes=2000,ntree=200
rf_fit_dummy <- train(number_of_hours~., dummy_train_subset, method='rf',tuneGrid = data.frame(mtry = seq(20, 40, 8)),trControl=tuningControl) # nodesize = 50,maxnodes=2000,ntree=200

rf_fit_dummy


dummy_test_subset <- dummy_test_subset %>% left_join(penalty_data,by='seller_id') %>% select(-seller_id,-delay)
predictions <- predict(rf_fit_dummy,dummy_test_subset %>% select(-number_of_hours))

RMSE(predictions,(dummy_test_subset %>% pull(number_of_hours) ))
#184.0301 for mtry 13. Technically there are total of ~79 variables due to 72-74 product categories and 6 other predictors
```


# Results
# Conclusion
We can make fine predictions if we can predict time for approval, time for handling of goods to carrier and time for delivery separately as final prediction would be based on that.
But when a new order is made, we only know purchase time

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
