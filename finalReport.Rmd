---
title: "CYO Project Report - Brazilian Ecommerce"
author: "Asham Vohra"
date: "6/21/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




Delivery Performance:

You will also be able to work through delivery performance and find ways to optimize delivery times.

Attention

- An order might have multiple items.
- Each item might be fulfilled by a distinct seller.
- All text identifying stores and partners where replaced by the names of Game of Thrones great houses.

olist_customers_dataset.csv
This dataset has information about the customer and its location. Use it to identify unique customers in the orders dataset and to find the orders delivery location.
At our system each order is assigned to a unique customerid. This means that the same customer will get different ids for different orders. The purpose of having a customerunique_id on the dataset is to allow you to identify customers that made repurchases at the store. Otherwise you would find that each order had a different customer associated with.

olist_order_reviews_dataset.csv
This dataset includes data about the reviews made by the customers.

After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.


olist_orders_dataset.csv
This is the core dataset. From each order you might find all other information.

olist_products_dataset.csv
Products Dataset
This dataset includes data about the products sold by Olist.


# Overview
Data set: https://www.kaggle.com/olistbr/brazilian-ecommerce

# Analysis

Delivery Performance:

You will also be able to work through delivery performance and find ways to optimize delivery times.

Let's predict delivery time/date
-- delivery distance
-- location of customer as some can be remotely connected and need special delivery merchanism. This is independent of delivery distance.
-- location of seller as shipping from some locations maybe difficult. This is independent of delivery distance.
-- size of product? 
-- weight of the product. Heavier products may take time to deliver
-- price of product (expensive nature of product as it could mean extra care or it maybe fragile?)
-- purchase time....if in night, delivery time maybe longer
-- order approved at time-> if payment approval came multiple hours after purchase time, this maybe longer
-- product category -- products of same category may show similar behavior.
-- or specific product - some products may have higher delivery time.
-- deliveries maybe impacted due to sales, festival season.

we should consider if we need time stamp or we can take purchase time as 0 and calculated approved at from it. though we may lose granularity in terms of time as truck may come everyday at 4. so after 4 you can ship , before 4 you cant
We can be agnostic to purchase time and just predict number of hours for delivery for given purchase.


estimated delivery date is present in ---> order_estimated_delivery_date
We can calculate RMSE using that and it will be our benchmark

We can see if we improved than that or not. It will give us indication of how close are we to the production model RMSE.


TODO -> if there is delayed to carrier precendence, we could use that probability and average delay to carrier to fine tune our results. Similarly we could use other metrics like payment approval time etc.



```{r Install or load packages,include=FALSE, echo = FALSE}
# TODO use conditional loading or installation of library
# Remove plotly related code for pdfs
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
if (!require('lubridate')) install.packages('lubridate'); library('lubridate')
if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')
if (!require('geosphere')) install.packages('geosphere'); library('geosphere')
if (!require('caret')) install.packages('caret'); library('caret')
if (!require('randomForest')) install.packages('randomForest'); library('randomForest')
if (!require('knitr')) install.packages('knitr'); library('knitr')
```

## Data Wrangling
Reading from raw data source and converting data to usable form

For products data set,
- we read data, remove unnecessary fields, join with english names whereever available
- remove products where dimension and related attributes are not available
- assign default category name of UNCATEGORIZED for products where same was not available
```{r Data wrangling-products data set, include=FALSE, echo=FALSE}
products_dataset <- read.csv(file.path('data','olist_products_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% select(-product_name_lenght,-product_description_lenght,-product_photos_qty)

products_dataset %>% head()
products_dataset %>% str()

product_category_name_translation <- read.csv(file.path('data','product_category_name_translation.csv'),header=TRUE,stringsAsFactors = FALSE)

product_category_name_translation %>% head()


# convert product_name to english form wherever possible
products_dataset <- products_dataset %>% left_join(product_category_name_translation,by='product_category_name') %>% mutate(product_category_name=if_else(is.na(product_category_name_english),product_category_name,product_category_name_english)) %>% select(-product_category_name_english) 

# further remove products for which data is not available i.e is NA
products_dataset <- products_dataset %>% filter(!is.na(product_weight_g) & !is.na(product_length_cm) & !is.na(product_height_cm) & !is.na(product_width_cm)  )

# assign category name for blank category
products_dataset <- products_dataset %>% mutate(product_category_name=if_else(product_category_name=='','UNCATEGORIZED',product_category_name))
```
The wrangled data looks like below:
```{r Data wrangling result-products data set, echo=FALSE}
products_dataset %>% head() %>% knitr::kable()
```

For geolocation data set
```{r Data wrangling- geolocation data set, include=FALSE, echo=FALSE}
# geo location details
# in brazil while 5 digits zip code is good enough to identify state, it may not be correct to identify city
# it can have geolocation_zip_code_prefix shared across multiple places
# refer: https://codigo-postal.org/en-us/brazil/
# The Postal Code Address (CEP) is an eight-digit number set that has by main objective to guide and accelerate the distribution of objects sent through the national mail in Brazil. The current structure of the Brazilian postal code is 8 (eight) digits, divided into two parts:

# first part composed of 5 digits where each digit represents the 1) region, 2) sub-region, 3) sector, 4) subsector, 5) subsector divider;
# the second part is composed of 3 digits, separated by a hyphen from the first part and represents the Distribution Identifiers.
# You can navigate through states, then cities and finally neighborhoods and streets until you find the corresponding postal code.


geolocation_dataset <- read.csv(file.path('data','olist_geolocation_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 

geolocation_dataset %>% head()
geolocation_dataset %>% str()


# Important: many geolocation_city have same name but different ascent component. Example: niteroi and niterói
# intially we can see 
# geolocation_dataset %>% dplyr::count(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% nrow()

# post transformation to resolve any Accent or special characters from the city name
# geolocation_dataset <- geolocation_dataset %>% mutate(geolocation_city=stri_trans_general(geolocation_city,"Any-ASCII"))

# above is not done as the way zip codes are in brazil, first five digits can correspond to different cities and so we will remove the city. We also removed state as there seems to be incorrect entries in the sellers data. This either needs to be fixed or we can limit ourselves to lat/long value


# geolocation_dataset %>% dplyr::count(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% nrow()

# as each geolocation_zip_code_prefix can have many data points, we will limit these to one by taking mean for lat, long location
geolocation_dataset <- geolocation_dataset %>% group_by(geolocation_zip_code_prefix) %>% summarise(geolocation_lat=mean(geolocation_lat),geolocation_lng=mean(geolocation_lng)) %>% ungroup()
#geolocation_dataset <- geolocation_dataset %>% group_by(geolocation_zip_code_prefix,geolocation_city,geolocation_state) %>% summarise(geolocation_lat=mean(geolocation_lat),geolocation_lng=mean(geolocation_lng)) %>% ungroup()

# TODO review it
#%>% mutate(geolocation_state=as.factor(geolocation_state)) 
# TODO review if geolocation_zip_code_prefix and geolocation_city can have multiple states with them.

# remove whitespace before and after reduces one city
# still issues are there and can be seen by
# zip_codes_of_interest <- geolocation_dataset %>% mutate(geolocation_city=str_trim(geolocation_city)) %>% dplyr::count(geolocation_zip_code_prefix) %>% filter(n>1)  %>% pull(geolocation_zip_code_prefix)

# geolocation_dataset %>% filter(geolocation_zip_code_prefix %in% zip_codes_of_interest) %>% view()
# to filter out duplicates
# we need not worry about city or state, we can pick any as long as lat, lng values are similar
```



```{r Data wrangling-customers data set, include=FALSE, echo=FALSE}
# customers data set  with zip code prefix and can be used to uniquely identify a customer
# customer_id ----> key to the orders dataset. Each order has a unique customer_id.
# customer_zip_code_prefix --> first five digits of customer zip code
customers_dataset <- read.csv(file.path('data','olist_customers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 
# %>% mutate(customer_zip_code_prefix=as.factor(customer_zip_code_prefix),customer_id=as.character(customer_id ))

customers_dataset %>% head()
customers_dataset %>% str()
```



```{r Data wrangling-sellers data set, echo=FALSE, include=FALSE}
# seller data set with zip code prefix
# seller_zip_code_prefix --> first 5 digits of seller zip code
sellers_dataset <- read.csv(file.path('data','olist_sellers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 
# TODO review if we need it
# %>% mutate(seller_zip_code_prefix=as.factor(seller_zip_code_prefix),seller_id=as.character(seller_id ))

sellers_dataset %>% head()
sellers_dataset %>% str()
```


Wrangled customers dataset
- removed data where delivery was not made
- removed data which was delivered but all the delivery details were not available
```{r Data wrangling-orders data set, include=FALSE, echo=FALSE}
# order data set with various delivery related times
# note each customer_id is unique per order
# order_purchase_timestamp -> Shows the purchase timestamp.
# order_approved_at/payment_approved_at -> Shows the payment approval timestamp.
# order_delivered_carrier_date/handed_over_to_carrier_date -> Shows the order posting timestamp. When it was handled to the logistic partner.
# order_delivered_customer_date --> Shows the actual order delivery date to the customer.
# order_estimated_delivery_date --> Shows the estimated delivery date that was informed to customer at the purchase moment.
orders_dataset <- read.csv(file.path('data','olist_orders_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% 
  rename(payment_approved_at=order_approved_at,
         handed_over_to_carrier_date=order_delivered_carrier_date)  %>% 
  mutate(order_status=as.factor(order_status),
         order_purchase_timestamp=as_datetime(order_purchase_timestamp),
         payment_approved_at=as_datetime(payment_approved_at),
         handed_over_to_carrier_date=as_datetime(handed_over_to_carrier_date),
         order_delivered_customer_date=as_datetime(order_delivered_customer_date),
         order_estimated_delivery_date=as_datetime(order_estimated_delivery_date))

# TODO if we are not using payment_approved_at or few fields we can remove the filter logic based on that.
orders_dataset <- orders_dataset %>% 
  filter(order_status=='delivered') %>% 
  filter( !is.na(order_estimated_delivery_date ) & !is.na(order_delivered_customer_date ) & !is.na(handed_over_to_carrier_date) & !is.na(payment_approved_at) & !is.na(order_purchase_timestamp))

orders_dataset %>% head()
orders_dataset %>% str()
```

```{r Data wrangling-order items data set, include=FALSE, echo=FALSE}
# orders data with item related seller info
# order_item_id --->sequential number identifying number of items included in the same order.
# so if there are three items they will be nuber 1 , 2, 3 for the given order
# shipping_limit_date ---> Shows the seller shipping limit date for handling the order over to the logistic partner.
# freight_value --> item freight value item (if an order has more than one item the freight value is splitted between items)
order_items_dataset <- read.csv(file.path('data','olist_order_items_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% mutate(shipping_limit_date=as_datetime(shipping_limit_date))

order_items_dataset %>% str()
order_items_dataset %>% head()
```

Performance metric
```{r RMSE, echo=FALSE}
RMSE <- function(true_outcomes, predicted_outcomes) {
  sqrt(mean((true_outcomes - predicted_outcomes)^2))
}
```


Filter out data where sellers location details are not available
```{r Refine sellers dataset, include=FALSE, echo=FALSE}
#
# Important: many geolocation_city have same name but different ascent component. Example: niteroi and niterói
# These all need to be handled in data wrangling


# we noticed some seller_zip_code_prefix are not present in geolocation dataset. So we will ignore them
sellers_to_ignore <- sellers_dataset %>% anti_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) 

order_items_dataset %>% inner_join(sellers_to_ignore,by='seller_id') %>% nrow()
# while there are decent number of order items for the concerned sellers, we will ignore them as we will still have sufficient data for analysis.


# let's use the one of interest
sellers_dataset <- sellers_dataset %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) %>% rename(geolocation_lat_seller=geolocation_lat,geolocation_lng_seller=geolocation_lng)

# TODO there seems to be an issue (Should be resolved now)
# geolocation_dataset seems to have 4 entries for same zip code . example 13454
# sellers_dataset %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) %>% dplyr::count(seller_id) %>% slice_max(order_by=n,n=5)

# we tried another approach by joining over all zip codee, city and state but it led to no match
# sellers_dataset %>% mutate(seller_city=stri_trans_general(seller_city,"Any-ASCII")) %>% inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix','seller_city'='geolocation_city','seller_state'='geolocation_state')) %>% str()
# TODO -- if we analyse we can see city and state for seller data differ in both datasets. so best to stick to zip code. city and state are prone to errors.
# we decided against using city and state.
```

Wrangled sellers dataset ,looks like below:
```{r Refined sellers dataset result, echo=FALSE}
sellers_dataset %>% head() %>% knitr::kable()
```


```{r Refine customers data set, include=FALSE, echo=FALSE}
# similarly let see customers for whom we don't have data in geolocation_dataset
customers_to_ignore <- customers_dataset %>% anti_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) 


orders_dataset %>% inner_join(customers_to_ignore,by='customer_id') %>% nrow()
# while there are decent number of orders for these customers, we will ignore them as we will still have sufficient data for analysis.

# let's use customers of interest
customers_dataset <- customers_dataset %>% inner_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) %>% rename(geolocation_lat_customer=geolocation_lat,geolocation_lng_customer=geolocation_lng) 
```

Wrangled customers data set looks like below:

```{r Refined customers dataset result, echo=FALSE}
customers_dataset %>% head() %>% knitr::kable()
```


Combine segregated data sets
- create derived info i.e. distance between two points

```{r Combine data sets, include=FALSE, echo=FALSE}
# we can combine orders delivery information (orders_dataset) with orders items and seller related info (order_items_dataset)
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% head()

# let's combine with seller details
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% head()

# let's combine with unique customer and its location dataset
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>% select(-customer_id) %>% head()

# let's combine with products data as well
# orders_dataset %>% inner_join(order_items_dataset,by='order_id') %>% inner_join(sellers_dataset,by='seller_id') %>% inner_join(customers_dataset,by='customer_id') %>% inner_join(products_dataset,by='product_id') %>% select(-customer_id,-order_status,-seller_city,-seller_state,-customer_city,-customer_state) %>% head()

# so raw data
raw_data <- orders_dataset %>% 
  inner_join(order_items_dataset,by='order_id') %>%
  inner_join(sellers_dataset,by='seller_id') %>% 
  inner_join(customers_dataset,by='customer_id') %>%  
  inner_join(products_dataset,by='product_id') %>%
  mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),
         product_id=as.factor(product_id), 
         seller_id=as.factor(seller_id),
         seller_zip_code_prefix=as.factor(seller_zip_code_prefix),
         customer_unique_id=as.factor(customer_unique_id),
         customer_zip_code_prefix=as.factor(customer_zip_code_prefix),
         product_category_name=as.factor(product_category_name)) %>%
  select(-customer_id,-order_status,-seller_city,-seller_state,-customer_city,-customer_state, -payment_approved_at,-order_id,-order_item_id,-product_id,-seller_zip_code_prefix,-customer_unique_id,-customer_zip_code_prefix)
#  select(-customer_id,-order_status,-seller_city,-seller_state,-customer_city,-customer_state, -payment_approved_at,-order_id, -order_item_id, -product_id,-seller_zip_code_prefix,-customer_unique_id,-customer_zip_code_prefix)
```


Calculating current RMSE range
```{r Current RMSE, include=FALSE, echo = FALSE}
# current RMSE can be calculated by finding number of hours for predicted delivery time given by order_estimated_delivery_date vs actual outcome/number of hours give by order_delivered_customer_date
# However, note that order_estimated_delivery_date has no timestamp, so we will remove timestamp component, round date using floor_date, which means any delivery done during the day say at 2017-10-10 21:25:13 is treated to be done on 2017-10-10

# first we convert it to desired date
# then we covert date to numeric i.e. to convert it to seconds
# then we calculate time_taken_from_order_to_delivery, estimated_time_from_order_to_delivery in seconds
data_for_rmse <- raw_data %>%
  select(order_purchase_timestamp,order_delivered_customer_date,order_estimated_delivery_date) %>%
  mutate(order_delivered_customer_date_low=floor_date(order_delivered_customer_date,unit="day"),
         order_delivered_customer_date_high=ceiling_date(order_delivered_customer_date,unit="day")) %>% 
  mutate(order_delivered_customer_date_low=as.numeric(order_delivered_customer_date_low),
         order_delivered_customer_date_high=as.numeric(order_delivered_customer_date_high),
         order_estimated_delivery_date=as.numeric(order_estimated_delivery_date))  %>%
  mutate(time_taken_from_order_to_delivery_rangemax=order_delivered_customer_date_high-order_purchase_timestamp,
         time_taken_from_order_to_delivery_rangemin=order_delivered_customer_date_low-order_purchase_timestamp,
         estimated_time_from_order_to_delivery=order_estimated_delivery_date-order_purchase_timestamp) %>% 
  mutate(estimated_time_from_order_to_delivery=estimated_time_from_order_to_delivery/3600,
         time_taken_from_order_to_delivery_rangemax = time_taken_from_order_to_delivery_rangemax/3600,
         time_taken_from_order_to_delivery_rangemin = time_taken_from_order_to_delivery_rangemin/3600) %>%
  select(estimated_time_from_order_to_delivery,time_taken_from_order_to_delivery_rangemax,time_taken_from_order_to_delivery_rangemin)


RMSE_value_based_on_max <- RMSE(data_for_rmse$time_taken_from_order_to_delivery_rangemax,data_for_rmse$estimated_time_from_order_to_delivery)
# 359.9466

RMSE_value_based_on_min <- RMSE(data_for_rmse$time_taken_from_order_to_delivery_rangemin,data_for_rmse$estimated_time_from_order_to_delivery)
# 377.9515

# if we look at graph we can  see there is huge difference between estimated data and actual delivery date
# it is possible some products are bulky or require differen mode of transport like ship 
data_for_rmse %>% mutate(diff=estimated_time_from_order_to_delivery-1/2*(time_taken_from_order_to_delivery_rangemax+time_taken_from_order_to_delivery_rangemin)) %>% ggplot(aes(diff)) + geom_histogram(binwidth = 24*2) + xlab('Estimation error where each bar represents 2 days')

rm(data_for_rmse)
```
```{r Range of current rmse, echo=FALSE}
data.frame(type=character(),value=numeric()) %>% 
  add_row(type='RMSE Max',value=RMSE_value_based_on_max) %>% 
  add_row(type='RMSE Min',value=RMSE_value_based_on_min) %>% 
  knitr::kable(caption='Current RMSE')
```


Final wrangled data looks like below:
```{r Prepare final wrangled data set, include=FALSE}
# we will predict number of hours to deliver based on various predictors. This seems more appropriate. Though as a result of this, we won't be able to use night, day time data of purchase which may influence delivery time (say to carrier it is informed only next day)
wrangled_data <- raw_data %>%
  mutate(order_delivered_customer_date=as.numeric(order_delivered_customer_date)) %>%
  mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>%
  select(-order_purchase_timestamp,-order_delivered_customer_date,-order_estimated_delivery_date)
```

```{r Apply one hot encoding to product_category_name, include = FALSE}
set.seed(1,sample.kind = 'Rounding')
# fullRank parameter to make sure there is no perfect collinearity
product_category_dummyvars <- data.frame(predict(dummyVars(~product_category_name, data=wrangled_data, fullRank = TRUE), newdata = wrangled_data))

wrangled_data <- wrangled_data %>% cbind.data.frame(product_category_dummyvars)

wrangled_data %>% str()
rm(product_category_dummyvars)
```

```{r Final wrangled data,echo=FALSE}
wrangled_data %>% head() %>% knitr::kable()
```


```{r Cleanup and save required dataset, include=FALSE, echo=FALSE}
save(wrangled_data,file='rda/wrangled_data.rda')
rm(geolocation_dataset, order_items_dataset, orders_dataset, product_category_name_translation, products_dataset, sellers_dataset, sellers_to_ignore, customers_dataset, customers_to_ignore, raw_data)
```

TODO
Use target encoding for categorical variables
Use preprocess for scaling data against training data set. Then use mean, sd from this trained data for test data.



Derived data set should be generated later on observations
- geolocation usage to capture distance
- delay and mean delay in handling to carrier
- number of hours for delivery should be generated later
```{r Method to derive distance data, include = FALSE}
# we will use distGeo from geosphere package to compute distance in km between seller and customer. considered to be highly accurate. if it takes more time, we can consider using distVincentyEllipsoid or distHaversine as we are fine with accuracy of a km.
extract_distance_between_seller_customer <- function(df) { 
  df %>% mutate(distance_between_two_points=round(distGeo(
    matrix(c(geolocation_lng_seller,geolocation_lat_seller),ncol=2),
    matrix(c(geolocation_lng_customer,geolocation_lat_customer),ncol=2)
    )/1000,2
    )) %>% select(-geolocation_lat_customer,-geolocation_lng_customer,-geolocation_lat_seller,-geolocation_lng_seller)
}
```


```{r Method to derive delay in handover to carrier, include = FALSE}
# calculated in hours
get_delay_in_handover_to_carrier_per_seller_id <- function(df) {
  df %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600)  %>% 
    group_by(seller_id) %>% 
    summarise(mean_delay_in_handover=mean(delay))
}
```

```{r Method to compute volume from available fields, include = FALSE}
# compute volume
compute_volume_using_product_info <- function(df) {
  df %>% mutate(volume=product_length_cm*product_height_cm*product_width_cm) %>%
    select(-product_length_cm,-product_height_cm,-product_width_cm)
}
```

Split data into training and validation set, where validation set will be used later
```{r Split data into training and validation set, include = FALSE, echo = FALSE}
set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(wrangled_data$number_of_hours,times=1,p=0.2,list=FALSE)

training_dataset <- wrangled_data[-test_indices,]
validation_dataset <- wrangled_data[test_indices,]

# ensure validation_dataset has products only for concerned product categories and seller_ids which are in training_dataset 
validation_dataset <- validation_dataset %>%
  semi_join(training_dataset,by='product_category_name') %>%
  semi_join(training_dataset,by='seller_id')
# validation_dataset <- validation_dataset %>% semi_join(training_dataset,by='seller_id')

rm(test_indices)
```


We use training_dataset and split into into train and test set. So that training_dataset is sufficient to fine tune the model
```{r Split data for training, include=FALSE}
# Let's split training_dataset into two parts
set.seed(1,sample.kind = 'Rounding')
training_dataset <- training_dataset %>% compute_volume_using_product_info() %>% extract_distance_between_seller_customer()

test_indices <- createDataPartition(training_dataset$number_of_hours,times=1,p=0.2,list=FALSE)

train_subset <- training_dataset[-test_indices,]
test_subset <- training_dataset[test_indices,]
# ensure test_subset has products only for concerned product categories and seller_ids which are in train_subset
test_subset <- test_subset %>% semi_join(train_subset,by='product_category_name') %>% semi_join(train_subset,by='seller_id')
#test_subset <- test_subset %>% semi_join(train_subset,by='seller_id')

penalty_data <- train_subset %>% get_delay_in_handover_to_carrier_per_seller_id()

rm(test_indices)
```



## Explore product_category_name relation with number_of_hours

```{r Explore product_category_name relation with number_of_hours, echo=FALSE}
training_dataset %>% group_by(product_category_name) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(product_category_name=reorder(product_category_name,mean)) %>% ggplot(aes(x=product_category_name,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```

We can see that number_of_hours of delivery varies as number product category changes


##Explore price relation with number_of_hours

```{r Explore price relation with number_of_hours, echo = FALSE}
# first let see how are the items associated with orders priced. Most of the order items a
training_dataset %>% mutate(price_group=round(price/100)+1) %>% ggplot(aes(price_group)) + geom_histogram(binwidth=1) + xlab('Each price_group is of 100 units each')


# let's plot errorbar for number of hours vs price_group.
# we can see while for price groups number of hours for delivery vary but the relationship is not clear enough. Higher priced items can also be delivered early
training_dataset %>% mutate(price_group=round(price/50)+1) %>% mutate(price_group=as.factor(price_group)) %>% group_by(price_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=price_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(training_dataset$price,training_dataset$number_of_hours)
```

Price Group for a product does not give any clear indication. It does not seem to be impacting delivery time of product.

## Explore distance relationship between two points with number_of_hours

```{r Explore distance relationship between two points with number_of_hours, echo = FALSE}
# let's plot errorbar for number of hours vs distance group
# we can see a pattern that as distance increases delivery time increases. 
training_dataset %>% mutate(distance_group=round(distance_between_two_points/50)+1) %>% mutate(distance_group=as.factor(distance_group)) %>% group_by(distance_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=distance_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(training_dataset$distance_between_two_points,training_dataset$number_of_hours)
```

Distance Group can be seen to influence delivery time.


```{r Explore freight value group with number_of_hours}
# let's plot errorbar for number of hours vs freight value group
training_dataset %>% mutate(freight_value_group=round(freight_value/5)+1) %>% mutate(freight_value_group=as.factor(freight_value_group)) %>% group_by(freight_value_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=freight_value_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))

cor(training_dataset$freight_value,training_dataset$number_of_hours)
```

Above graphs indicates that for freight value group where each group represents 5 units, as freight value increases, number of hours for delivery increases.

However, it dips near the end
```{r smooth fit for freight value, echo=FALSE}
training_dataset %>% ggplot(aes(x=freight_value,y=number_of_hours)) + geom_smooth() 
```


## Explore volume of product group with number_of_hours

```{r Explore volume of product group with number_of_hours, echo=FALSE}
# let's plot errorbar for number of hours vs volume of product group
training_dataset %>% mutate(volume_group=round(volume/1000)+1) %>% mutate(volume_group=as.factor(volume_group)) %>% group_by(volume_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=volume_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
It does not influencee much the number of hours

However a smooth fit shows some pattern 
```{r smooth fit for volume, echo=FALSE}
training_dataset %>% ggplot(aes(volume,number_of_hours)) + geom_smooth()
```

## Explore weight of product with number_of_hours

```{r Explore weight of product with number_of_hours}
# let's plot errorbar for number of hours vs weight of product
training_dataset %>% mutate(weight_group=round((product_weight_g)/1000)+1) %>% mutate(weight_group=as.factor(weight_group)) %>% mutate(weight_group=as.factor(weight_group)) %>% group_by(weight_group) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% ggplot(aes(x=weight_group,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90))
```
Some clues but not strongly correlated.

```{r Smooth fit for product, echo= FALSE}
training_dataset  %>% ggplot(aes(product_weight_g,number_of_hours)) + geom_smooth()
```


```{r Prepare training and test data for ,include=FALSE}
# penalty_data is computed on train_subset and is used with test_subset as historical learning of efficiency of each seller in delivering to carrier
train_subset <- train_subset %>% left_join(penalty_data,by='seller_id') %>% select(-handed_over_to_carrier_date, -shipping_limit_date)

test_subset <- test_subset %>% left_join(penalty_data,by='seller_id') %>% select(-handed_over_to_carrier_date, -shipping_limit_date)
```



```{r Apply Target encoding, echo = FALSE}
  train_subset_for_encoding <- train_subset[1:30000,]
  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,6,3)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  #lambda_data_set <- prepare_lambda_data_set(index)
  #train_lambda_set <- lambda_data_set[[1]]
  #test_lambda_set <- lambda_data_set[[2]]  
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>%
    semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='seller_id') 
  
  weight_rmses <- sapply(weights, function(weight) {
      seller_id_effects <- train_weight_set %>% 
        group_by(seller_id) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n) %>% rename()
      
      train_weight_set <- train_weight_set %>% left_join(seller_id_effects,by='seller_id') %>% rename(seller_id_effect=mean) %>% select(-product_category_name,-seller_id)
      
      test_weight_set <- test_weight_set %>% left_join(seller_id_effects,by='seller_id') %>% rename(seller_id_effect=mean) %>% select(-product_category_name,-seller_id)
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      # TODO remove below scaling was erroneous. train scaling needs to be used for test data
      # below is just demo of how to use scale
      # scaled_train_subset <- train_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale)) 
      # scaled_test_subset <- test_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale))
      
      # TODO can we skip modelling while evaluating weight to use or for target encoding?
      
      tuningControl=trainControl(method = "cv", number = 2, p = .8)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(27,34,41)),trControl=tuningControl)
      
      knn_fit_for_weight
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition)  
```

```{r Post regularization, echo=FALSE}

# prepare seller_if_effect based on regularisation weight we have identified
  global_mean <- mean(train_subset$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset$number_of_hours,p=0.2,times=5,list=FALSE)

seller_id_effects_list <- lapply(1:5, function(index) {
    train_weight_set <- train_subset[-test_weight_indices[,index],]

    seller_id_effects <- train_weight_set %>% 
      group_by(seller_id) %>%
      summarise(mean=mean(number_of_hours),n=n()) %>%
      mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
      select(-n) %>% rename()
    
    seller_id_effects
  }
)

sie <- seller_id_effects_list[[1]]
for (index in 2:5) {
   sie <- sie %>% full_join(seller_id_effects_list[[index]],by='seller_id', suffix=c("1",index))
}

# whereever value is NA, we have used global mean
sie <- sie %>% mutate(across(-seller_id,~if_else(is.na(.x),global_mean,.x)))

# as different partitions may not have exactly same data set combined the results
seller_id_effects <- tibble(sie[,1], seller_id_effect=rowMeans(sie[,-1],na.rm = TRUE))

train_subset <- train_subset %>% left_join(seller_id_effects,by='seller_id') %>% select(-product_category_name,-seller_id) %>% mutate(seller_id_effect=if_else(is.na(seller_id_effect),global_mean,seller_id_effect))
      
test_subset <- test_subset %>% left_join(seller_id_effects,by='seller_id') %>% select(-product_category_name,-seller_id)
```



```{r knn model data scaled under test, echo=FALSE}
# TODO review and improve or remove this as required
set.seed(seed=1,sample.kind = 'Rounding')
preProcessScalingValues <- preProcess(train_subset %>% select(-number_of_hours), method = c("center", "scale"))

scaled_train_subset <- predict(preProcessScalingValues, train_subset)
scaled_test_subset <- predict(preProcessScalingValues, test_subset)

# TODO remove below scaling was erroneous. train scaling needs to be used for test data
# below is just demo of how to use scale
# scaled_train_subset <- train_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale)) 
# scaled_test_subset <- test_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale))


tuningControl=trainControl(method = "cv", number = 3, p = .8)
knn_fit <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(20,27,34)),trControl=tuningControl)

knn_fit

predictions <- predict(knn_fit,scaled_test_subset %>% select(-number_of_hours))

RMSE(predictions,(scaled_test_subset %>% pull(number_of_hours) ))
#192.8755 for k=27

save(knn_fit,file='rda/knn_model.rda')
```


Random forest
However, it is important to know your data and keep in mind that a Random Forest can’t extrapolate. It can only make a prediction that is an average of previously observed labels. In this sense it is very similar to KNN.
In other words, in a regression problem, the range of predictions a Random Forest can make is bound by the highest and lowest labels in the training data. This behavior becomes problematic in situations where the training and prediction inputs differ in their range and/or distributions. This is called covariate shift and it is difficult for most models to handle but especially for Random Forest, because it can’t extrapolate.
From : https://towardsdatascience.com/a-limitation-of-random-forest-regression-db8ed7419e9f

```{r rf model, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 2, p = .8)
rf_fit <- train(number_of_hours~., train_subset, method='rf',tuneGrid = data.frame(mtry = seq(5, 50, 10)),trControl=tuningControl,ntree=250) # nodesize = 50,maxnodes=2000,ntree=200

rf_fit

predictions <- predict(rf_fit,test_subset %>% select(-number_of_hours))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 184.5549 for mtry 15. Technically there are total of 79 variables due to 73 product categories and 6 other predictors

save(rf_fit,file='rda/rf_model.rda')
```


```{r Gam model, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number =2, p = .8)
gam_fit <- train(number_of_hours~., train_subset, method='gam',trControl=tuningControl) # check for tuning parameters

gam_fit

predictions <- predict(gam_fit,test_subset %>% select(-number_of_hours))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 195.3235

save(gam_fit,file='rda/gam_model.rda')
```


```{r Glm model}
# Let's try with glm model
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 2, p = .8)
glm_fit <- train(number_of_hours~., train_subset, method='glm',trControl=tuningControl)

glm_fit

predictions <- predict(glm_fit, test_subset %>% select(-number_of_hours))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 198.8575 (improves if we use seller_id as predictor as well)
```
Warning:
prediction from a rank-deficient fit may be misleading

```{r GamLoess model, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 3, p = .8)
#grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)
grid <- expand.grid(span = seq(0.15, 0.65, len = 3), degree = 1)
gamLoess_fit <- train(number_of_hours~., train_subset, method='gamLoess',tuneGrid=grid,trControl=tuningControl)

gamLoess_fit

predictions <- predict(gamLoess_fit,test_subset %>% select(-number_of_hours))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
#194.6215

save(gamLoess_fit,file='rda/gamLoess_model.rda')
```

Warnings received:
extrapolation not allowed with blending
 (Discovered by lowesd)liv too small.    
 (Discovered by lowesd)lv too small
 extrapolation not allowed with blending
 non-list contrasts argument ignored


```{r GamLoess model with scaling, echo = FALSE}
# TODO review-keep or remove it
set.seed(seed=1,sample.kind = 'Rounding')
scaled_train_subset <- train_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale)) 
scaled_test_subset <- test_subset %>% mutate(across(c(mean_delay_in_handover,distance_between_two_points,volume,product_weight_g,freight_value,price),scale))

tuningControl=trainControl(method = "cv", number = 3, p = .8)
#grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)
grid <- expand.grid(span = seq(0.15, 0.65, len = 3), degree = 1)
gamLoess_fit <- train(number_of_hours~., scaled_train_subset, method='gamLoess',tuneGrid=grid,trControl=tuningControl)

gamLoess_fit

predictions <- predict(gamLoess_fit,scaled_test_subset %>% select(-number_of_hours))

RMSE(predictions,(scaled_test_subset %>% pull(number_of_hours) ))
#194.7017

save(gamLoess_fit,file='rda/gamLoess_model.rda')
```
Warnings received:
extrapolation not allowed with blending
 (Discovered by lowesd)liv too small.    
 (Discovered by lowesd)lv too small
 extrapolation not allowed with blending
 non-list contrasts argument ignored


```{r Final Model Training, echo = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')

penalty_data <- training_dataset %>% get_delay_in_handover_to_carrier_per_seller_id()

training_dataset <- training_dataset %>% left_join(penalty_data,by='seller_id') %>% select(-seller_id,-handed_over_to_carrier_date,-shipping_limit_date)

mtry_to_use <- rf_fit$finalModel$mtry
ntree_to_use <- 500 #rf_fit$finalModel$ntree
tuningControl <- trainControl(method = "cv", number = 2, p = .8)
final_model <- train(number_of_hours~., training_dataset, method='rf',tuneGrid = data.frame(mtry = c(mtry_to_use)),trControl=tuningControl,ntree=ntree_to_use) 

# final_model <- randomForest(number_of_hours~., training_dataset,mtry= mtry_to_use, ntree=ntree_to_use)

final_model

save(final_model,file='rda/final_model.rda')
```


```{r Final movel validation, echo = FALSE}
validation_dataset <- validation_dataset %>% compute_volume_using_product_info() %>% extract_distance_between_seller_customer() %>% left_join(penalty_data,by='seller_id') %>% select(-seller_id,-handed_over_to_carrier_date,-shipping_limit_date)

predictions <- predict(final_model,validation_dataset %>% select(-number_of_hours))

RMSE((validation_dataset %>% pull(number_of_hours) ),predictions)
# 194.6748 for mtry=25, ntree=500
```




===== TODO review from here

Data used maybe incorrect
Review joins and if multiple orders of same type are being delivered at the same time
refer
> training_dataset %>% filter(number_of_hours>=354.73 & number_of_hours<354.74)
   price freight_value product_category_name product_weight_g number_of_hours volume
1   59.0         13.43          garden_tools             1550        354.7386  19800
2   59.0         13.43          garden_tools             1550        354.7386  19800
3   59.0         13.43          garden_tools             1550        354.7386  19800
4   59.0         13.43          garden_tools             1550        354.7386  19800
5   59.0         13.43          garden_tools             1550        354.7386  19800
6   59.0         13.43          garden_tools             1550        354.7386  19800
7   59.0         13.43          garden_tools             1550        354.7386  19800
8   59.0         13.43          garden_tools             1550        354.7386  19800
9   59.0         13.43          garden_tools             1550        354.7386  19800
10  59.0         13.43          garden_tools             1550        354.7386  19800
11  59.0         13.43          garden_tools             1550        354.7386  19800
12  66.0         14.21       furniture_decor             1150        354.7319  28175
13  46.9         17.63        sports_leisure              300        354.7378   2304
   distance_between_two_points mean_delay_in_handover
1                       469.74                -51.60514
2                       469.74                -51.60514
3                       469.74                -51.60514
4                       469.74                -51.60514
5                       469.74                -51.60514
6                       469.74                -51.60514
7                       469.74                -51.60514
8                       469.74                -51.60514
9                       469.74                -51.60514
10                      469.74                -51.60514
11                      469.74                -51.60514
12                      807.32                -56.57523
13                     1083.46               -147.48749


```{r Derived data}
# seller to customer distance should be right predictor to consider, as the text says "Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners."
# source: https://www.kaggle.com/olistbr/brazilian-ecommerce
wrangled_data %>% head() %>% extract_distance_between_seller_customer()

# plot of how 
p1 <- wrangled_data %>% extract_distance_between_seller_customer() %>% ggplot(aes(distance_between_two_points)) + geom_histogram(binwidth=50)
ggplotly(p1)
```

```{r Explore delayed handover to carrier, echo=FALSE }
# TODO we maybe able to remove this as it is not used, because this is the info which is generally not available
# identify if there was delay in delivery to carrier
wrangled_data <- wrangled_data %>% mutate(delayed_to_carrier=if_else((as.numeric(handed_over_to_carrier_date)-as.numeric(shipping_limit_date))>0,TRUE,FALSE))

wrangled_data <- wrangled_data %>% select(-shipping_limit_date,-handed_over_to_carrier_date,-order_estimated_delivery_date)
```

## Explore seller_zip_code_prefix with number_of_hours

```{r Explore seller_zip_code_prefix with number_of_hours, echo = FALSE}
# TODO anyway to add regularization or remove the low impact seller_zip_code_prefix from the model
# let's plot errorbar for number of hours vs seller_zip_code_prefix
sellers_zip_of_interest <- training_dataset %>% dplyr::count(seller_zip_code_prefix) %>% filter(n>10) %>% pull(seller_zip_code_prefix)

training_dataset %>% filter(seller_zip_code_prefix %in% sellers_zip_of_interest) %>% group_by(seller_zip_code_prefix) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(seller_zip_code_prefix=reorder(seller_zip_code_prefix,mean)) %>%  ggplot(aes(x=seller_zip_code_prefix,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90)) + scale_y_sqrt() 
```
Indicates role between seller_zip_code and number of hours for delivery


## Explore customer_zip_code_prefix with number_of_hours

```{r Explore customer_zip_code_prefix with number_of_hours, echo=FALSE}
# TODO anyway to add regularization or remove the low impact customer_zip_code_prefix from the model
# let's plot errorbar for number of hours vs customer_zip_code_prefix
customers_zip_of_interest <- training_dataset %>% dplyr::count(customer_zip_code_prefix) %>% filter(n>=5) %>% pull(customer_zip_code_prefix)

training_dataset %>% filter(customer_zip_code_prefix %in% customers_zip_of_interest) %>% group_by(customer_zip_code_prefix) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(customer_zip_code_prefix=reorder(customer_zip_code_prefix,mean)) %>%  ggplot(aes(x=customer_zip_code_prefix,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_text(angle=90)) + scale_y_sqrt()
```
customer_zip_code_prefix can  be seen to be  influence number of hours for delivery

## Explore product_id with number_of_hours

```{r Explore product_id with number_of_hours}
# TODO anyway to add regularization or remove the low impact products from the model
# let's plot errorbar for number of hours vs product_id
products_of_interest <- training_dataset %>% dplyr::count(product_id)  %>% filter(n>=10) %>% pull(product_id)

training_dataset %>% filter(product_id %in% products_of_interest) %>% group_by(product_id) %>% summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% mutate(product_id=reorder(product_id,mean)) %>%  ggplot(aes(x=product_id,y=mean,ymax=mean+2*se,ymin=mean-2*se)) + geom_point()+geom_errorbar() + theme(axis.text.x = element_blank()) 
```
It influences the number of hours

====TODO review above




### Linear model trial and analysis

```{r Linear model, eval=FALSE, echo = FALSE}
# trying to reduce coefficients.
# in dimensions we can use volume to decrase predictors


########### only price predictor#############
## only price complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(price,number_of_hours), method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(price))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 220.4085



## only price,distance_between_two_points complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(price,distance_between_two_points,number_of_hours), method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(price,distance_between_two_points))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 202.0175


## only price,distance_between_two_points,product_weight_g complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(price,distance_between_two_points,product_weight_g,number_of_hours), method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(price,distance_between_two_points,product_weight_g))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 201.1412


## only price,distance_between_two_points,product_weight_g,volume complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(price,distance_between_two_points,product_weight_g,volume,number_of_hours), method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(price,distance_between_two_points,product_weight_g,volume))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 201.1086


## only price,distance_between_two_points,product_weight_g,volume,freight_value complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(price,distance_between_two_points,product_weight_g,volume,freight_value,number_of_hours), method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(price,distance_between_two_points,product_weight_g,volume,freight_value))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 201.1047


## only product_category_name complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(product_category_name,number_of_hours), method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(product_category_name))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 218.8943



# if we combine this with other predictors
## only price,distance_between_two_points,product_category_name complete data
lm_fit <- train(number_of_hours~., train_subset %>% select(price,distance_between_two_points,product_category_name,number_of_hours), method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(price,distance_between_two_points,product_category_name))

RMSE(predictions,(test_subset %>% pull(number_of_hours) ))
# 199.8844
```

```{r Linear model with all predictors, echo=FALSE}
# above is a fine model
# with all predictors
lm_fit <- train(number_of_hours~., train_subset, method='lm')

lm_fit

predictions <- predict(lm_fit,test_subset %>% select(-number_of_hours))

RMSE((test_subset %>% pull(number_of_hours) ), predictions)
# 199.4791

save(lm_fit,file='rda/lm_model.rda')
```
Warnings received:
prediction from a rank-deficient fit may be misleading



To improve model performance consider--> changing character categorical variables to numerical categorical variables like
  
  product_category_name=as.factor(as.numeric(product_category_name)) 

use levels from original data for completeness

look for variables which have limited to no results and we can combine them as one categorical variable
Refer: https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/
Also read : feature hashing

Existing model algorithms should handle regularization. So we should not be required to handle ourselves.


We can use conditional model i.e. for some use different model and for some use different model just like ensemble.
So basically we could say for these pin codes, delivery time= say mean of delivery time and for others use standard algorthm like random forest etc.


gam?
gamLoess??




## Experimets which failed

Let's play with seller_id. It deriorates rf model and for gam model there was no improvement
```{r play with seller_id, eval=FALSE, echo = FALSE}
set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(wrangled_data$number_of_hours,times=1,p=0.2,list=FALSE)
dummy_data <- wrangled_data[-test_indices,]
dummy_data <- dummy_data %>% mutate(volume=product_length_cm*product_height_cm*product_width_cm) %>% mutate(seller_id=factor(as.numeric(seller_id))) %>% select(-customer_unique_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix,-product_length_cm,-product_height_cm,-product_width_cm)


set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(dummy_data$number_of_hours,times=1,p=0.2,list=FALSE)
dummy_train_subset <- dummy_data[-test_indices,]
dummy_test_subset <- dummy_data[test_indices,]

# only keeping those rows which have seller_id in training set
dummy_test_subset <- dummy_test_subset %>% semi_join(dummy_train_subset,by='seller_id')

# used gam model but it did not use seller id at all 
# Let's try with rf model
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 2, p = .8)
rf_fit_dummy <- train(number_of_hours~., dummy_train_subset, method='rf',tuneGrid = data.frame(mtry = seq(5, 7, 1)),trControl=tuningControl) # nodesize = 50,maxnodes=2000,ntree=200

rf_fit_dummy

predictions <- predict(rf_fit_dummy,dummy_test_subset %>% select(-number_of_hours))

RMSE(predictions,(dummy_test_subset %>% pull(number_of_hours) ))
# 203.6908


# Let's try with glm model
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 2, p = .8)
glm_fit_dummy <- train(number_of_hours~., dummy_train_subset, method='glm',trControl=tuningControl)

glm_fit_dummy

predictions <- predict(glm_fit_dummy,dummy_test_subset %>% select(-number_of_hours))

RMSE(predictions,(dummy_test_subset %>% pull(number_of_hours) ))
# 195.4884
```


```{r ensemble, echo = FALSE}
load('rda/gamLoess_model.rda')
load('rda/gam_model.rda')
load('rda/knn_model.rda')
load('rda/lm_model.rda')
load('rda/rf_model.rda')

predictions_gamLoess <- predict(gamLoess_fit,test_subset %>% select(-number_of_hours))
predictions_knn <- predict(knn_fit,test_subset %>% select(-number_of_hours))
predictions_rf <- predict(rf_fit,test_subset %>% select(-number_of_hours))
predictions_gam <- predict(gam_fit,test_subset %>% select(-number_of_hours))
predictions_glm <- predict(glm_fit,test_subset %>% select(-number_of_hours))

predictions_df <- data.frame(predictions_gamLoess,predictions_knn,predictions_rf,predictions_gam,predictions_glm)

RMSE(rowMeans(predictions_df),(test_subset %>% pull(number_of_hours) ))
# 194.8623

RMSE(apply(predictions_df %>% select(predictions_rf,predictions_gam,predictions_knn) %>% as.matrix(),1,'median'),(test_subset %>% pull(number_of_hours) ))
# 194.25
```

TODO-- what if we remove freight value as it depends on number of items in order too.as it is divided by number of items


```{r delayed to carrier experiment, eval=FALSE}
# let's see how delay in carrier influences overall delivery time
raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% group_by(delayed_to_carrier) %>% summarise(mean(number_of_hours))

# let's see time contribution due to delay or early drop to carrier 
raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600) %>% select(delay,delayed_to_carrier) %>% group_by(delayed_to_carrier) %>% summarise(mean(delay))

# sellers of interest - we have sellers to be penalized(Add delivery time) and sellers to be corrected(reduce delivery time)
seller_ids_of_interest <- raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600) %>% select(delayed_to_carrier, seller_id) %>% dplyr::count(seller_id,delayed_to_carrier) %>% spread(delayed_to_carrier,n,fill=0) %>% mutate(delay_percentage=`TRUE`/(`TRUE`+`FALSE`),total=`TRUE`+`FALSE`) %>% filter(total>=10 & (delay_percentage >=0.75 | delay_percentage <=0.25)) %>% pull(seller_id)


# dummy data points
dummy_wrangled_data <- raw_data  %>% mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),order_delivered_customer_date=as.numeric(order_delivered_customer_date),product_id=as.factor(product_id),seller_id=as.factor(seller_id),seller_zip_code_prefix=as.factor(seller_zip_code_prefix),customer_unique_id=as.factor(customer_unique_id),customer_zip_code_prefix=as.factor(customer_zip_code_prefix),product_category_name=as.factor(product_category_name)) %>% mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>% select(-order_delivered_customer_date,-order_purchase_timestamp)  %>% filter(seller_id %in% seller_ids_of_interest) %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600) %>% select(-shipping_limit_date,-handed_over_to_carrier_date,-payment_approved_at,-order_id,-order_estimated_delivery_date,-order_item_id,-delayed_to_carrier)


penalty_data <- wrangled_data %>% get_delay_in_handover_to_carrier_per_seller_id()


set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(dummy_wrangled_data$number_of_hours,times=1,p=0.2,list=FALSE)
dummy_data <- dummy_wrangled_data[-test_indices,]
dummy_data <- dummy_data %>% mutate(volume=product_length_cm*product_height_cm*product_width_cm) %>%  select(-customer_unique_id,-product_id,-customer_zip_code_prefix,-seller_zip_code_prefix,-product_length_cm,-product_height_cm,-product_width_cm)


set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(dummy_data$number_of_hours,times=1,p=0.2,list=FALSE)
dummy_train_subset <- dummy_data[-test_indices,]
dummy_test_subset <- dummy_data[test_indices,]

# this penalty data can later be reused with test data as well
penalty_data <- dummy_train_subset %>% group_by(seller_id) %>% summarise(mean_delay_in_handover=mean(delay))

dummy_train_subset <- dummy_train_subset %>% left_join(penalty_data,by='seller_id')

# only keeping those rows which have seller_id in training set
dummy_test_subset <- dummy_test_subset %>% semi_join(dummy_train_subset,by='seller_id')



dummy_train_subset <- dummy_train_subset %>% select(-seller_id,-delay)
# Let's try with rf model
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 2, p = .8)
# rf_fit_dummy <- train(number_of_hours~., dummy_train_subset, method='rf',tuneGrid = data.frame(mtry = seq(5, 15, 4)),trControl=tuningControl) # nodesize = 50,maxnodes=2000,ntree=200
rf_fit_dummy <- train(number_of_hours~., dummy_train_subset, method='rf',tuneGrid = data.frame(mtry = seq(20, 40, 8)),trControl=tuningControl) # nodesize = 50,maxnodes=2000,ntree=200

rf_fit_dummy


dummy_test_subset <- dummy_test_subset %>% left_join(penalty_data,by='seller_id') %>% select(-seller_id,-delay)
predictions <- predict(rf_fit_dummy,dummy_test_subset %>% select(-number_of_hours))

RMSE(predictions,(dummy_test_subset %>% pull(number_of_hours) ))
#184.0301 for mtry 13. Technically there are total of ~79 variables due to 72-74 product categories and 6 other predictors
```


# Results
# Conclusion
We can make fine predictions if we can predict time for approval, time for handling of goods to carrier and time for delivery separately as final prediction would be based on that.
But when a new order is made, we only know purchase time

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
