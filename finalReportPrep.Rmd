---
title: "CYO Project Report - Delivery Prediction System"
author: "Asham Vohra"
date: "6/21/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview
Prediction systems play a crucial role in the modern world. With advent of technology and availability of data, these systems are being leveraged by retail chains, banks, colleges among other businesses. Depending on the use case, these systems are used to estimate delivery time, sales figures, carry out inventory management, predict occurrence of a disease etc. 

Inspired by what all problems these systems can solve, we have attempted to build a delivery prediction system based on publicaly available [dataset of orders made at Olist Store](https://www.kaggle.com/olistbr/brazilian-ecommerce), a Brazilian e-commerce.

The dataset used here is hosted at [Kaggle website](https://www.kaggle.com). The dataset is provided by Olist itself, which is the largest department store in Brazilian marketplaces. Olist connects sellers to the customers. It is the sellers, who are responsible to fulfil any orders placed on the platform using Olist's logistic partners. 

The dataset consists of **100,000+** orders and for each order the dataset contains related details like product bought and from which category, customer who bought and his/her location, seller involved and many more attributes.

The delivery prediction system developed as part of the project, leverages the past orders, product category, delivery distance, customer location, actual delivery time and related attributes to predict delivery estimate for the new orders. 

The goal of the project was to build a delivery prediction system which can estimate number of hours required for delivery keeping RMSE(Root mean square error) minimal. 

In order to achieve this goal, we started by cleaning data, identifying right predictors, deriving insights from data already available to create new predictors. Once we had the dataset with us, we incrementally took a predictor, analysed its relationship with the number of hours of delivery, incorporated the predictor in our model after encoding, scaling it as required and tested our updated model against the test data. To ensure that the model performance was not due to random split of dataset, the model was trained and tuned using k fold cross validation and then only tested against the test dataset. Only if the predictor helped improve our metric i.e. RMSE, the predictor under analysis and evaluation was added to the model and the steps were repeated with new predictor. Otherwise the predictor was ignored

This report walks through the approach, analysis and evaluation carried out to achieve our delivery prediction system

# Analysis

## Data Wrangling
```{r Install or load packages,include=FALSE, echo = FALSE}
#Install or load packages
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
if (!require('lubridate')) install.packages('lubridate'); library('lubridate')
if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')
if (!require('geosphere')) install.packages('geosphere'); library('geosphere')
if (!require('caret')) install.packages('caret'); library('caret')
if (!require('randomForest')) install.packages('randomForest'); library('randomForest')
if (!require('knitr')) install.packages('knitr'); library('knitr')
```

An important step in using any public data set is converting it to usable form. In our case, the data set was available in multiple csv files, each file dedicated to a specific kind of information. Example: 

* There was a file for orders which had information about product bought, purchase time, delivery timestamp and other attributes. 
* Then there was a file with product details which had information about product category, product weight and other attributes.

Each of these files due to the varied dataset required different data wrangling steps to achieve a usable form which could then be unified for further analysis.

### Data wrangling for products data set

The data set here had product details like category name, dimensions and other attributes. Here the category name was in Portuguese. Some important attributes like product dimensions and weight were not populated for few of the products. To analyse this data properly, we carried out below steps:

* Read the concerned csv file and removed irrelevant columns for the analysis.
* Removed products where dimensions and related attributes were not available.
* Assigned default category name of UNCATEGORIZED for products where product_category_name was not available.
* Replaced product category names with English names where-ever mapping was available. The product categories for which equivalent English name mapping was not available were left untouched.

```{r Data wrangling-products data set, include=FALSE, echo=FALSE}
# read the data from source and remove irrelevant columns
products_dataset <- read.csv(file.path('data','olist_products_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>%
  select(-product_name_lenght,-product_description_lenght,-product_photos_qty)

# check how they are structured
products_dataset %>% head()
products_dataset %>% str()

# load the product category name translations
product_category_name_translation <- read.csv(file.path('data','product_category_name_translation.csv'),header=TRUE,stringsAsFactors = FALSE)

product_category_name_translation %>% head()

# convert product_name to english form wherever possible. If not available, existing product category name is retained
products_dataset <- products_dataset %>% 
  left_join(product_category_name_translation,by='product_category_name') %>% 
  mutate(product_category_name=if_else(is.na(product_category_name_english),
                                       product_category_name,
                                       product_category_name_english)) %>% 
  select(-product_category_name_english) 

# further remove products for which data is not available i.e is NA
products_dataset <- products_dataset %>% 
  filter(!is.na(product_weight_g) & !is.na(product_length_cm) & !is.na(product_height_cm) & !is.na(product_width_cm))

# assign default category name of UNCATEGORIZED where-ever product_category_name is blank.
products_dataset <- products_dataset %>%
  mutate(product_category_name=if_else(product_category_name=='','UNCATEGORIZED',product_category_name))
```

The wrangled data looked like below:
```{r Data wrangling result-products data set, echo=FALSE}
products_dataset %>% head(2) %>% as.matrix()  %>% t() %>% 
  knitr::kable(col.names=c('Example Value1','Example Value2'), caption='Product data set')
```

### Data wrangling for orders data set

The data set here had orders details like order id, associated customer, order status, purchase time, when was the order handed over to carrier, when was it delivered to customer and estimated delivery date and few other attributes.

This was a crucial dataset not only for us to build our delivery estimation system but to even identify the current performance metric i.e. RMSE for the approach Olist was using.

In order to leverage this dataset properly, we carried out below steps:

* Read the concerned csv file and removed irrelevant columns for the analysis.
* Parsed columns with date as characters and converted them to desired date type.
* Filtered out any orders where delivery was not made or where delivery was made but required delivery time columns were not available.

```{r Data wrangling-orders data set, include=FALSE, echo=FALSE}
# order data set with various delivery related times
# note each customer_id is unique per order
# order_purchase_timestamp -> Shows the purchase timestamp.
# order_approved_at/payment_approved_at -> Shows the payment approval timestamp.
# order_delivered_carrier_date/handed_over_to_carrier_date -> Shows the order posting timestamp. When it was handled to the logistic partner.
# order_delivered_customer_date --> Shows the actual order delivery date to the customer.
# order_estimated_delivery_date --> Shows the estimated delivery date that was informed to customer at the purchase moment.
orders_dataset <- read.csv(file.path('data','olist_orders_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% 
  rename(payment_approved_at=order_approved_at,
         handed_over_to_carrier_date=order_delivered_carrier_date)  %>% 
  mutate(order_status=as.factor(order_status),
         order_purchase_timestamp=as_datetime(order_purchase_timestamp),
         payment_approved_at=as_datetime(payment_approved_at),
         handed_over_to_carrier_date=as_datetime(handed_over_to_carrier_date),
         order_delivered_customer_date=as_datetime(order_delivered_customer_date),
         order_estimated_delivery_date=as_datetime(order_estimated_delivery_date))

# Filtered out any orders where delivery was not made 
# Also filtered out orders where delivery was made but required delivery time columns were not available.
orders_dataset <- orders_dataset %>% 
  filter(order_status=='delivered') %>% 
  filter( !is.na(order_estimated_delivery_date ) & !is.na(order_delivered_customer_date ) & !is.na(handed_over_to_carrier_date) & !is.na(payment_approved_at) & !is.na(order_purchase_timestamp))

# check how they are structured
orders_dataset %>% head()
orders_dataset %>% str()
```

The wrangled data looks like below:
```{r Data wrangling result-orders dataset, echo=FALSE}
orders_dataset %>% head(2) %>% as.matrix() %>% t() %>% 
  knitr::kable(col.names=c('Example Value1','Example Value2'), caption='Orders data set')
```

### Data wrangling for orders items data set

The data set here had order item details like which product(s) were bought for a given order, who the seller was, price and freight value and shipping limit date. While most of the attributes were used to connect with other data sets, it was the shipping limit date which could be used along with product handed over to carrier date from orders dataset to identify any delays in the shipping process.

In order to leverage this dataset properly, we carried out below steps:

* Read the concerned csv file and removed irrelevant columns for the analysis.
* Parsed columns with date as characters and converted them to desired date type.

```{r Data wrangling-order items data set, include=FALSE, echo=FALSE}
# orders data with item related seller info
# order_item_id --->sequential number identifying number of items included in the same order.
# so if there are three items they will be nuber 1 , 2, 3 for the given order
# shipping_limit_date ---> Shows the seller shipping limit date for handling the order over to the logistic partner.
# freight_value --> item freight value item (if an order has more than one item the freight value is splitted between items)
order_items_dataset <- read.csv(file.path('data','olist_order_items_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) %>% mutate(shipping_limit_date=as_datetime(shipping_limit_date))  %>% select(-order_item_id)

# check how they are structured
order_items_dataset %>% str()
order_items_dataset %>% head()
```

The wrangled data looks like below:
```{r Data wrangling result-orders items dataset, echo=FALSE}
order_items_dataset %>% head(2) %>% as.matrix() %>% t() %>% 
  knitr::kable(col.names=c('Example Value1','Example Value2'), caption='Order Items\' data set')
```
### Data wrangling for geolocation data set

The data set here had geolocation data. It had zip code prefix, lat, lang, city and state details. However, there was no unique identifier in the dataset.

It is important to note that  Brazil uses [Postal Code Address(CEP)](https://codigo-postal.org/en-us/brazil/) which is 8-digit number created mainly from mail distribution objective. It is split into two parts of 5 and 3 digits resp. separated by a hyphen('-'). The first part helps identify the rough delivery area, while it is the second part which provides precision and helps one reach the desired street, neighbourhood.

So in the geolocation data set, zip code prefix referred to the first part of the CEP and it varied from 1 to 5 digits in the dataset available to us. Because the zip code prefix does not represent the precise value, it was not unique and had many duplicate values with varying value of lat, lng for same zip code prefix. 

In order to be able to use this data with other data sets available like customer location and seller location datasets, which are discussed later, we carried out below steps:

* Read the concerned csv file and removed irrelevant columns for the analysis.
* Calculated the mean of lat, lng for given zip code prefix.

```{r Data wrangling- geolocation data set, include=FALSE, echo=FALSE}
# geo location details
# In brazil zip code is 8 digits. So first 5 digits zip code is good enough to identify an area but it does not uniquely identify customer location.
# So because of this, there are multiple rows for same geolocation_zip_code_prefix
# refer: https://codigo-postal.org/en-us/brazil/
#
# The first five digits represent the 1) region, 2) sub-region, 3) sector, 4) subsector, 5) subsector divider;
geolocation_dataset <- read.csv(file.path('data','olist_geolocation_dataset.csv'),
                                header=TRUE,stringsAsFactors = FALSE) %>%
  select(-geolocation_city,-geolocation_state) 

# check how they are structured
geolocation_dataset %>% head()
geolocation_dataset %>% str()

# check count of rows per geolocation_zip_code_prefix
geolocation_dataset %>% dplyr::count(geolocation_zip_code_prefix) %>% nrow()

# as each geolocation_zip_code_prefix can have many data points, we will limit these to one by taking mean for lat, long location
geolocation_dataset <- geolocation_dataset %>% group_by(geolocation_zip_code_prefix) %>% 
  summarise(geolocation_lat=mean(geolocation_lat),geolocation_lng=mean(geolocation_lng)) %>%
  ungroup()
```

The wrangled data looks like below:
```{r Data wrangling result-geolocation dataset, echo=FALSE}
geolocation_dataset %>% head(2) %>% summarise_all(as.character) %>% t() %>% 
  knitr::kable(col.names=c('Example Value1','Example Value2'), caption='Geolocation data set')
```

### Data wrangling for customers and sellers data set

There were two data sets one for customers and one for sellers. Apart from identifier ids like customer_unique_id, seller_id, both the datasets had similar columns i.e. zip code prefix, for which details were in geolocation data set and city and state.

To be able to leverage either of these datasets for analysis later, we carried out below steps:

* Read the concerned csv file and removed irrelevant columns for the analysis.
* Combined sellers dataset data individually with geolocation dataset in order to capture seller lat,lng. Similar operation was carried out for customers data set.
* Remove sellers or customers for which geolocation details are not available as delivery or even its estimation is not possible without the critical fields.

```{r Data wrangling-customers data set, include=FALSE, echo=FALSE}
# customers data set  with zip code prefix and can be used to uniquely identify a customer
# customer_id ----> key to the orders dataset. Each order has a unique customer_id.
# customer_zip_code_prefix --> first five digits of customer zip code
customers_dataset <- read.csv(file.path('data','olist_customers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE) 

# check how they are structured
customers_dataset %>% head()
customers_dataset %>% str()
```

```{r Data wrangling-sellers data set, echo=FALSE, include=FALSE}
# seller data set with zip code prefix
# seller_zip_code_prefix --> first 5 digits of seller zip code
sellers_dataset <- read.csv(file.path('data','olist_sellers_dataset.csv'),header=TRUE,stringsAsFactors = FALSE)

# check how they are structured
sellers_dataset %>% head()
sellers_dataset %>% str()
```

```{r Refine sellers dataset, include=FALSE, echo=FALSE}
# Filter out data where sellers location details are not available
# we noticed some seller_zip_code_prefix are not present in geolocation dataset. So we will ignore them
sellers_to_ignore <- sellers_dataset %>% anti_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) 

# while there are small number of order items for the concerned sellers, we will ignore them as we will still have sufficient data for analysis.
order_items_dataset %>% inner_join(sellers_to_ignore,by='seller_id') %>% nrow()

# let's use the one of interest
sellers_dataset <- sellers_dataset %>% 
  inner_join(geolocation_dataset,by=c('seller_zip_code_prefix'='geolocation_zip_code_prefix')) %>% 
  rename(geolocation_lat_seller=geolocation_lat,geolocation_lng_seller=geolocation_lng)
```

The wrangled sellers dataset looks like below:
```{r Refined sellers dataset result, echo=FALSE}
sellers_dataset %>% head(2) %>% as.matrix()  %>% t() %>% 
  knitr::kable(col.names=c('Example Value1','Example Value2'), caption='Sellers data set')
```


```{r Refine customers data set, include=FALSE, echo=FALSE}
# similarly let see customers for whom we don't have data in geolocation_dataset
customers_to_ignore <- customers_dataset %>% anti_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) 

# while there are small number of orders for these customers, we will ignore them as we will still have sufficient data for analysis.
orders_dataset %>% inner_join(customers_to_ignore,by='customer_id') %>% nrow()

# let's use customers of interest
customers_dataset <- customers_dataset %>% 
  inner_join(geolocation_dataset,by=c('customer_zip_code_prefix'='geolocation_zip_code_prefix')) %>% 
  rename(geolocation_lat_customer=geolocation_lat,geolocation_lng_customer=geolocation_lng) 
```

The wrangled customers dataset looks like below:
```{r Refined customers dataset result, echo=FALSE}
customers_dataset %>% head(2) %>% as.matrix()  %>% t() %>% 
  knitr::kable(col.names=c('Example Value1','Example Value2'), caption='Customers data set')
```


```{r Combine data sets, include=FALSE, echo=FALSE}
# so raw data
# in Brazil first digit of the five code zip code prefix can be 0. So we explicitly add 0 to it.
raw_data <- orders_dataset %>% 
  inner_join(order_items_dataset,by='order_id') %>%
  inner_join(sellers_dataset,by='seller_id') %>% 
  inner_join(customers_dataset,by='customer_id') %>%  
  inner_join(products_dataset,by='product_id') %>%
  mutate(order_purchase_timestamp=as.numeric(order_purchase_timestamp),
         product_id=as.factor(product_id), 
         seller_id=as.factor(seller_id),
         seller_zip_code_prefix=as.factor(seller_zip_code_prefix),
         customer_unique_id=as.factor(customer_unique_id),
         product_category_name=as.factor(product_category_name),
         customer_zip_code_prefix=str_pad(customer_zip_code_prefix, c(5),pad=c('0'),side='left')) %>% 
  select(-customer_id,-order_status,-seller_city,-seller_state,-customer_city,-order_id,-product_id,-seller_zip_code_prefix,-customer_unique_id)
```

```{r Prepare final wrangled data set, include=FALSE}
# Prepare final wrangled data set we will predict number of hours to deliver based on 
# various predictors. This seems more appropriate. Though as a result of this, we won't be able 
# to use night, day time data of purchase which may influence delivery time (say to carrier it 
# is informed only next day)
wrangled_data <- raw_data %>%
  mutate(order_delivered_customer_date=as.numeric(order_delivered_customer_date)) %>%
  mutate(number_of_hours=(order_delivered_customer_date-order_purchase_timestamp)/3600) %>%
  select(-order_delivered_customer_date,-order_estimated_delivery_date)
```

Having wrangled the datasets individually we combined them into a single dataset. While the dataset had many attributes, based on their frequency and our intuition shared earlier we zeroed on the below fields for further consideration. In addition, it is important to note that for model development we created outcome column number_of_hours based on existing fields i.e. order_delivered_customer_date and order_purchase_timestamp as this is what we would want to predict whenever we get a new order i.e. given this order, how many number of hours are required to deliver the product to the customer.

The wrangled dataset after incorporating various datasets available to us looked like below:
```{r Final wrangled data,echo=FALSE}
wrangled_data %>% head(2) %>% as.matrix()  %>% t() %>% 
  knitr::kable(col.names=c('Example Value1','Example Value2'), caption='Final wrangled data set')
```

## Performance Metric

Before we dived into building our delivery prediction system, we identified our metric of choice i.e. RMSE for evaluating current delivery estimates being made by Olist and for comparing our delivery prediction system against that, in addition to improving our system as we built it.

*  RMSE is the root mean square error and that's what was used for this project to evaluate models and to optimize them for fitment.
* As we know, $RMSE = sqrt( 1/N * sum((\hat{y}-y)^2) )$
    + Here, $\hat{y}$ is the prediction. In context of this project, predicted number of hours for delivering the order item,
    + and y is the actual outcome. In context of this project, actual number of hours taken for delivering the order item.

```{r RMSE, include=FALSE}
# Performance metric
RMSE <- function(true_outcomes, predicted_outcomes) {
  sqrt(mean((true_outcomes - predicted_outcomes)^2))
}
```

### Current RMSE

Before we dived into building our delivery prediction system, we looked into current RMSE. To calculate this, we leveraged order_estimated_delivery_date i.e. estimated delivery date and order_delivered_customer_date i.e. actual delivery timestamp, which was available in orders data set.

While order_estimated_delivery_date did not have time details, order_delivered_customer_date did. Since order_delivered_customer_date could be rounded off to same day or next day, we computed range of RMSE which can be calculated using either of the approach.

This RMSE i.e. our performance metric was later used as benchmark for modelling. The delivery prediction system being built attempted to improve on this metric.

Below were the current RMSE calculations based on how we interpret the actual delivery timestamp.

```{r Current RMSE, include=FALSE, echo = FALSE}
# current RMSE can be calculated by finding number of hours for predicted delivery time 
# order_estimated_delivery_date vs actual outcome/number of hours give by order_delivered_customer_date
# However, note that order_estimated_delivery_date has no timestamp, so we will calculate RMSE 
# for both cases i.e. delivery date was taken as previous date or next date. 
# For this, from order_delivered_customer_date we will remove timestamp component, round date 
# using floor_date, which means any delivery done during the day say at 2017-10-10 21:25:13 is treated to be done on 2017-10-10
# Similarly we will round upwards using ceiling_date.

# For this, first we convert it to desired date
# then we covert date to numeric i.e. to convert it to seconds
# then we calculate time_taken_from_order_to_delivery (min and max as shared above), estimated_time_from_order_to_delivery in seconds
data_for_rmse <- raw_data %>%
  select(order_purchase_timestamp,order_delivered_customer_date,order_estimated_delivery_date) %>%
  mutate(order_delivered_customer_date_low=floor_date(order_delivered_customer_date,unit="day"),
         order_delivered_customer_date_high=ceiling_date(order_delivered_customer_date,unit="day")) %>% 
  mutate(order_delivered_customer_date_low=as.numeric(order_delivered_customer_date_low),
         order_delivered_customer_date_high=as.numeric(order_delivered_customer_date_high),
         order_estimated_delivery_date=as.numeric(order_estimated_delivery_date))  %>%
  mutate(time_taken_from_order_to_delivery_rangemax=order_delivered_customer_date_high-order_purchase_timestamp,
         time_taken_from_order_to_delivery_rangemin=order_delivered_customer_date_low-order_purchase_timestamp,
         estimated_time_from_order_to_delivery=order_estimated_delivery_date-order_purchase_timestamp) %>% 
  mutate(estimated_time_from_order_to_delivery=estimated_time_from_order_to_delivery/3600,
         time_taken_from_order_to_delivery_rangemax = time_taken_from_order_to_delivery_rangemax/3600,
         time_taken_from_order_to_delivery_rangemin = time_taken_from_order_to_delivery_rangemin/3600) %>%
  select(estimated_time_from_order_to_delivery,time_taken_from_order_to_delivery_rangemax,time_taken_from_order_to_delivery_rangemin)


RMSE_value_based_on_max <- RMSE(data_for_rmse$time_taken_from_order_to_delivery_rangemax,data_for_rmse$estimated_time_from_order_to_delivery)
# 359.9466

RMSE_value_based_on_min <- RMSE(data_for_rmse$time_taken_from_order_to_delivery_rangemin,data_for_rmse$estimated_time_from_order_to_delivery)
# 377.9515

rm(data_for_rmse)
```


```{r Range of current rmse, echo=FALSE}
performance_metric <- data.frame(type=character(),value=numeric()) %>% 
  add_row(type='Current RMSE Max',value=RMSE_value_based_on_max) %>% 
  add_row(type='Current RMSE Min',value=RMSE_value_based_on_min) 

performance_metric %>% knitr::kable(caption='Performance Metric(RMSE) Summary')
```


```{r Cleanup and save required dataset, include=FALSE, echo=FALSE}
save(wrangled_data,file='rda/wrangled_data.rda')
rm(geolocation_dataset, order_items_dataset, orders_dataset, product_category_name_translation, products_dataset, sellers_dataset, sellers_to_ignore, customers_dataset, customers_to_ignore, raw_data)
```


```{r Method to derive distance data, include = FALSE}
# we will use distGeo from geosphere package to compute distance in km between seller and customer. considered to be highly accurate.
extract_distance_between_seller_customer <- function(df) { 
  df %>% mutate(distance_between_two_points=round(distGeo(
    matrix(c(geolocation_lng_seller,geolocation_lat_seller),ncol=2),
    matrix(c(geolocation_lng_customer,geolocation_lat_customer),ncol=2)
    )/1000,2
    )) %>% select(-geolocation_lat_customer,-geolocation_lng_customer,-geolocation_lat_seller,-geolocation_lng_seller)
}
```

```{r Method to reduce levels in customer_zip_code_prefix, include = FALSE}
# trimming zip code to three digits to be able to extract zip code component which represents region, sub-region and sector data.
compute_rough_customer_location <- function(df) {
  df %>% mutate(customer_loc = as.numeric(str_extract(customer_zip_code_prefix,"\\d\\d\\d"))) 
}
```

```{r Method to derive delay in handover to carrier, include = FALSE}
# calculated in hours
get_delay_in_handover_to_carrier_per_seller_id <- function(df) {
  df %>% mutate(delay= (as.numeric(handed_over_to_carrier_date) -as.numeric(shipping_limit_date))/3600)  %>% 
    group_by(seller_id) %>% 
    summarise(mean_delay_in_handover=mean(delay))
}
```

```{r Get delay in payment approval based on customer zip code prefix, include = FALSE}
# calculated in hours
# can be due to fraud check etc.
get_delay_in_payment_approval_per_customer_zip_code <- function(df) {
  df %>% mutate(payment_approved_at=as.numeric(payment_approved_at)) %>%
    mutate(time_to_approve_payment_hours=(payment_approved_at-order_purchase_timestamp)/3600) %>% 
    group_by(customer_zip_code_prefix) %>% 
    summarise(mean_delay_in_payment_approval=mean(time_to_approve_payment_hours))
}
```

```{r Method to compute volume from available fields, include = FALSE}
# compute volume
compute_volume_using_product_info <- function(df) {
  df %>% mutate(volume=product_length_cm*product_height_cm*product_width_cm) %>%
    select(-product_length_cm,-product_height_cm,-product_width_cm)
}
```

## Partitioning dataset for training and validation  

Before using the wrangled data for analysis and data modelling, we partitioned the data into training and validation data sets. The idea was to keep validation data set aside and use only training data for any analysis, model development and tuning. 

A split of 80/20 was made here between training and validation set to ensure we have enough data points to train and to validate the results. Secondly, the split was done such that distribution of outcome i.e. number_of_hours was similar in both training and test data.

The data sets were named as below for future reference:

* training data set as **training_dataset** 
* validation data set as **validation_dataset**

```{r Split data into training and validation set, include = FALSE, echo = FALSE}
# Split data into training and validation set, where validation set will be used later
set.seed(1,sample.kind = 'Rounding')
test_indices <- createDataPartition(wrangled_data$number_of_hours,times=1,p=0.2,list=FALSE)

training_dataset <- wrangled_data[-test_indices,]
validation_dataset <- wrangled_data[test_indices,]

# ensure validation_dataset has products only for concerned product categories and seller_ids which are in training_dataset 
validation_dataset <- validation_dataset %>%
  semi_join(training_dataset,by='product_category_name') %>%
  semi_join(training_dataset,by='seller_id') %>% 
  semi_join(training_dataset,by="customer_zip_code_prefix")

rm(test_indices)
```

Below were the details about the elements in each of the partitioned datasets.
```{r Basic info about training and validation dataset,  echo= FALSE}
# The partitioned data were named training_dataset and validation_dataset, where training_dataset was our training data set and validation_dataset was our validation data set, which we kept aside as planned earlier. 

data.frame(data_set=c("Training data set","Validation data set"),name=c('training_dataset','validation_dataset'), count=c(nrow(training_dataset),nrow(validation_dataset))) %>% knitr::kable()
```

```{r Split data for training, include=FALSE}
# We use training_dataset and split into into train and test set. So that training_dataset is sufficient to fine tune the model, identify weights for target encoding of categorical variables etc.
# Let's split training_dataset into two parts
set.seed(1,sample.kind = 'Rounding')
training_dataset <- training_dataset %>% compute_volume_using_product_info() %>% extract_distance_between_seller_customer() %>% compute_rough_customer_location()

test_indices <- createDataPartition(training_dataset$number_of_hours,times=1,p=0.2,list=FALSE)

train_subset <- training_dataset[-test_indices,]
test_subset <- training_dataset[test_indices,]

# ensure test_subset has products only for concerned product categories and seller_ids which are in train_subset
test_subset <- test_subset %>% semi_join(train_subset,by='product_category_name') %>%
  semi_join(train_subset,by='seller_id') %>% 
  semi_join(train_subset,by="customer_zip_code_prefix") %>% 
  semi_join(train_subset,by="customer_loc")

rm(test_indices)
```


```{r Dataframe to store target encoding weights, include=FALSE}
target_encoding_weights <- data.frame(effect=character(0),weight=double(0)) 
```

```{r Prepare delay in handover to carried data, include = FALSE}
# prepare delay_in_handover_to_carrier_data based on regularisation weight we have identified
prepare_delay_in_handover_to_carrier <- function(train_df, weight_to_use) {
  global_mean <- mean(train_df %>% 
                        get_delay_in_handover_to_carrier_per_seller_id() %>% 
                        pull(mean_delay_in_handover))

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_df$number_of_hours,p=0.2,times=10,list=FALSE)

  delay_in_handover_to_carrier_data_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]
  
      delay_in_handover_to_carrier_data <- train_weight_set %>% 
        get_delay_in_handover_to_carrier_per_seller_id()
      
      delay_in_handover_to_carrier_data
    }
  )

  dhc <- delay_in_handover_to_carrier_data_list[[1]]
  for (index in 2:10) {
     dhc <- dhc %>% full_join(delay_in_handover_to_carrier_data_list[[index]],by='seller_id', suffix=c("1",index))
  }

  # wherever value is NA, we have used global mean
  dhc <- dhc %>% mutate(across(-seller_id,~if_else(is.na(.x),global_mean,.x)))
  
  # as different partitions may not have exactly same data set combined the results
  delay_in_handover_to_carrier_data <- tibble(dhc[,1],
                                              mean_delay_in_handover=rowMeans(dhc[,-1],
                                                                              na.rm = TRUE))
  
  #delay_in_handover_to_carrier_data is ready
  delay_in_handover_to_carrier_data
}
```

```{r Prepare delay in payment approval, include = FALSE}
# prepare delay_in_payment_approval based on regularisation weight we have identified
prepare_delay_in_payment_approval <- function(train_df, weight_to_use) {
  global_mean <- mean(train_df %>% 
                        get_delay_in_payment_approval_per_customer_zip_code() %>% 
                        pull(mean_delay_in_payment_approval))

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_df$number_of_hours,p=0.2,times=10,list=FALSE)

  delay_in_payment_approval_data_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]
  
      delay_in_payment_approval_data <- train_weight_set %>% 
        get_delay_in_payment_approval_per_customer_zip_code()
      
      delay_in_payment_approval_data
    }
  )

  dpa <- delay_in_payment_approval_data_list[[1]]
  for (index in 2:10) {
     dpa <- dpa %>% full_join(delay_in_payment_approval_data_list[[index]],by='customer_zip_code_prefix', suffix=c("1",index))
  }

  # wherever value is NA, we have used global mean
  dpa <- dpa %>% mutate(across(-customer_zip_code_prefix,~if_else(is.na(.x),global_mean,.x)))
  
  # as different partitions may not have exactly same data set combined the results
  delay_in_payment_approval_data <- tibble(dpa[,1],
                                              mean_delay_in_payment_approval=rowMeans(dpa[,-1],
                                                                              na.rm = TRUE))
  
  #delay_in_payment_approval_data is ready
  delay_in_payment_approval_data
}
```

```{r Prepare product_category effect based on target encoding, include = FALSE}
# prepare product_category_effect based on regularisation weight we have identified
prepare_product_category_effect <- function(train_df, weight_to_use) {
  global_mean <- mean(train_df$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_df$number_of_hours,p=0.2,times=10,list=FALSE)

  product_category_effects_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]
  
      product_category_effects <- train_weight_set %>% 
        group_by(product_category_name) %>%
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
        select(-n) %>% rename()
      
      product_category_effects
    }
  )

  pce <- product_category_effects_list[[1]]
  for (index in 2:10) {
     pce <- pce %>% full_join(product_category_effects_list[[index]],by='product_category_name', suffix=c("1",index))
  }

  # wherever value is NA, we have used global mean
  pce <- pce %>% mutate(across(-product_category_name,~if_else(is.na(.x),global_mean,.x)))
  
  # as different partitions may not have exactly same data set combined the results
  product_category_effects <- tibble(pce[,1], product_category_effect=rowMeans(pce[,-1],na.rm = TRUE))
  
  #product_category effects are ready
  product_category_effects
}
```

```{r Prepare seller_id effect based on target encoding, include = FALSE}
# prepare seller_id_effect based on regularisation weight we have identified
prepare_seller_id_effect <- function(train_df, weight_to_use) {
    global_mean <- mean(train_df$number_of_hours)
    
    set.seed(1,sample.kind='Rounding')
    test_weight_indices <- createDataPartition(train_df$number_of_hours, p=0.2,times=10,list=FALSE)
    
    seller_id_effects_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]

      seller_id_effects <- train_weight_set %>% 
        group_by(seller_id) %>%
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
        select(-n) %>% rename()
    
      seller_id_effects
    })
    
    
    sie <- seller_id_effects_list[[1]]
    for (index in 2:10) {
       sie <- sie %>% 
         full_join(seller_id_effects_list[[index]],by='seller_id', suffix=c("1",index))
    }
    
    # wherever value is NA, we have used global mean
    sie <- sie %>% mutate(across(-seller_id,~if_else(is.na(.x),global_mean,.x)))
    
    # as different partitions may not have exactly same data set combined the results
    seller_id_effects <- tibble(sie[,1], seller_id_effect=rowMeans(sie[,-1],na.rm = TRUE))
    
    #seller id effects are ready
    seller_id_effects
}
```

```{r Prepare customer_state effect based on target encoding, include = FALSE}
# prepare consumer_state_effect based on regularisation weight we have identified
prepare_customer_state_effect <- function(train_df, weight_to_use) {
  global_mean <- mean(train_df$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_df$number_of_hours,p=0.2,times=10,list=FALSE)

  customer_state_effects_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]
  
      customer_state_effects <- train_weight_set %>% 
        group_by(customer_state) %>%
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
        select(-n) %>% rename()
      
      customer_state_effects
    }
  )

  cse <- customer_state_effects_list[[1]]
  for (index in 2:10) {
     cse <- cse %>% 
       full_join(customer_state_effects_list[[index]],by='customer_state', suffix=c("1",index))
  }

  # wherever value is NA, we have used global mean
  cse <- cse %>% mutate(across(-customer_state,~if_else(is.na(.x),global_mean,.x)))
  
  # as different partitions may not have exactly same data set combined the results
  customer_state_effects <- tibble(cse[,1], customer_state_effect=rowMeans(cse[,-1],na.rm = TRUE))
  
  #customer state effects are ready
  customer_state_effects
}
```

```{r Prepare customer_loc effect based on target encoding, include = FALSE}
# prepare customer_loc_effect based on regularisation weight we have identified
prepare_customer_loc_effect <- function(train_df, weight_to_use) {
  global_mean <- mean(train_df$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_df$number_of_hours,p=0.2,times=10,list=FALSE)

  customer_loc_effects_list <- lapply(1:10, function(index) {
      train_weight_set <- train_df[-test_weight_indices[,index],]
  
      customer_loc_effects <- train_weight_set %>% 
        group_by(customer_loc) %>%
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight_to_use*global_mean)/(n+weight_to_use)) %>% 
        select(-n) %>% rename()
      
      customer_loc_effects
    }
  )

  cle <- customer_loc_effects_list[[1]]
  for (index in 2:10) {
     cle <- cle %>% full_join(customer_loc_effects_list[[index]],by='customer_loc', suffix=c("1",index))
  }

  # where-ever value is NA, we have used global mean
  cle <- cle %>% mutate(across(-customer_loc,~if_else(is.na(.x),global_mean,.x)))

  # as different partitions may not have exactly same data set combined the results
  customer_loc_effects <- tibble(cle[,1], customer_loc_effect=rowMeans(cle[,-1],na.rm = TRUE))
  
  #customer loc effects are ready
  customer_loc_effects
}
```

```{r Enhance training and test dataset for delay in handover to carrier, include=FALSE}
enhance_datasets_using_delay_in_handover_to_carrier <- function(mean_delay_in_handover_to_carrier_data,train_df,test_df,cleanup) {
  #we will use mean_delay_in_handover_to_carrier_data computed using training data in test data.
  train_df <- train_df %>% left_join(mean_delay_in_handover_to_carrier_data,by='seller_id') 
  test_df <- test_df %>% left_join(mean_delay_in_handover_to_carrier_data,by='seller_id')
  
  #cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-handed_over_to_carrier_date,-shipping_limit_date)
    test_df <- test_df %>% select(-handed_over_to_carrier_date,-shipping_limit_date)
  }
  
  list(train_df,test_df)
}
```

```{r Enhance training and test dataset for delay in payment approval, include=FALSE}
enhance_datasets_using_delay_in_payment_approval <- function(delay_in_payment_approval_data,train_df,test_df,cleanup) {
  #we will use delay_in_payment_approval_data computed using training data in test data.  
  train_df <- train_df %>%
    left_join(delay_in_payment_approval_data,by='customer_zip_code_prefix')
  
  test_df <- test_df %>% left_join(delay_in_payment_approval_data,by='customer_zip_code_prefix')
  
  #cleanup
  if (cleanup) {
    train_df <- train_df %>%
      select(-customer_zip_code_prefix,-order_purchase_timestamp,-payment_approved_at)
    test_df <- test_df %>% 
      select(-customer_zip_code_prefix,-order_purchase_timestamp,-payment_approved_at)
  }
  
  list(train_df,test_df)
}
```

```{r Enhance training and test dataset for product_category effect based on target encoding, include = FALSE}
enhance_datasets_using_product_category_effect <- function(product_category_effects,train_df,test_df,cleanup) {
  global_mean <- mean(train_df$number_of_hours)
  
  #we will use product category effects computed using training data in test data.
  train_df <- train_df %>% left_join(product_category_effects,by='product_category_name') %>%
    mutate(product_category_effect=if_else(is.na(product_category_effect),global_mean,product_category_effect))
        
  test_df <- test_df %>% left_join(product_category_effects,by='product_category_name')
  
  # cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-product_category_name)
    test_df <- test_df %>% select(-product_category_name)
  }
  
  list(train_df,test_df)
}
```

```{r Enhance training and test dataset for seller_id effect based on target encoding, include = FALSE}
enhance_datasets_using_seller_id_effect <- function(seller_id_effects,train_df,test_df,cleanup) {
  global_mean <- mean(train_df$number_of_hours)
  
  #we will use seller id effects computed using training data in test data.
  train_df <- train_df %>% left_join(seller_id_effects,by='seller_id') %>%
    mutate(seller_id_effect=if_else(is.na(seller_id_effect),global_mean,seller_id_effect))
        
  test_df <- test_df %>% left_join(seller_id_effects,by='seller_id')
  
  # cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-seller_id)
    test_df <- test_df %>% select(-seller_id)
  }
  
  list(train_df,test_df)
}
```

```{r Enhance training and test dataset for customer_state effect based on target encoding, include = FALSE}
enhance_datasets_using_customer_state_effect <- function(customer_state_effects,train_df,test_df,cleanup) {
  global_mean <- mean(train_df$number_of_hours)  
  
  #we will use customer state effects computed using training data in test data.
  train_df <- train_df %>% left_join(customer_state_effects,by='customer_state') %>%
    mutate(customer_state_effect=if_else(is.na(customer_state_effect),
                                         global_mean,
                                         customer_state_effect))
        
  test_df <- test_df %>% left_join(customer_state_effects,by='customer_state') 
  
  # cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-customer_state)
    test_df <- test_df %>% select(-customer_state)
  }
  
  list(train_df,test_df)
}
```

```{r Enhance training and test dataset for customer_loc effect based on target encoding, include = FALSE}
enhance_datasets_using_customer_loc_effect <- function(customer_loc_effects,train_df,test_df,cleanup) {
  global_mean <- mean(train_df$number_of_hours)
  
  #we will use customer loc effects computed using training data in test data.
  train_df <- train_df %>% 
    left_join(customer_loc_effects,by='customer_loc') %>%
    mutate(customer_loc_effect=if_else(is.na(customer_loc_effect),global_mean,customer_loc_effect))
        
  test_df <- test_df %>% left_join(customer_loc_effects,by='customer_loc')
  
  # cleanup
  if (cleanup) {
    train_df <- train_df %>% select(-customer_loc, -customer_zip_code_prefix)
    test_df <- test_df %>% select(-customer_loc, -customer_zip_code_prefix)
  }
  
  list(train_df,test_df)
}
```

```{r Enhance training and testing datasets with derived features, include =FALSE}
# uses training data to derive features and then enhances training and test set with those features
enhance_datasets_with_derived_features <- function(train_ds,test_ds,cleanup) {
  print('Adding column for delay in handover')

  delay_in_handover_to_carrier_data <- prepare_delay_in_handover_to_carrier(train_ds,0)
  
  dataset_list <- enhance_datasets_using_delay_in_handover_to_carrier(delay_in_handover_to_carrier_data,train_ds,test_ds,cleanup)
  train_ds <- dataset_list[[1]]
  test_ds <- dataset_list[[2]]
  
  print('Adding column for delay in payment approval')
  delay_in_payment_approval_data <- prepare_delay_in_payment_approval(train_ds,0)
  
  #we will use delay_in_payment_approval computed using training data in test data.
  dataset_list <- enhance_datasets_using_delay_in_payment_approval(delay_in_payment_approval_data,train_ds,test_ds,cleanup)

  dataset_list
}  
```

```{r Prepare data set for training, include = FALSE}
# prepares data for training and testing
# returns list of data frames. First list has training dataframe. Second list has test data frame
prepare_data_for_training_testing <- function(train_ds, test_ds, cleanup) {
  test_ds <- test_ds %>%
    semi_join(train_ds,by='product_category_name') %>%
    semi_join(train_ds,by='seller_id') %>% 
    semi_join(train_ds,by='customer_zip_code_prefix') %>% 
    semi_join(train_ds,by='customer_state') %>% 
    semi_join(train_ds,by='customer_loc')
  
  #adding derived features and target encoded categorical features
  print('Adding derived features')
  dataset_list <- enhance_datasets_with_derived_features(train_ds,test_ds,cleanup)
  train_ds <- dataset_list[[1]]
  test_ds <- dataset_list[[2]]
  
  print('Enhancing data for product category effects')
  weight_to_use <- target_encoding_weights %>% 
    filter(effect=='product_category') %>% 
    pull(weight)
  product_category_effects <- prepare_product_category_effect(train_ds,weight_to_use)
  dataset_list <- enhance_datasets_using_product_category_effect(product_category_effects,train_ds,test_ds,cleanup)
  train_ds <- dataset_list[[1]]
  test_ds <- dataset_list[[2]]

  print('Enhancing data for seller id effects')
  weight_to_use <- target_encoding_weights %>% filter(effect=='seller_id') %>% pull(weight)
  seller_id_effects <- prepare_seller_id_effect(train_ds,weight_to_use)
  dataset_list <- enhance_datasets_using_seller_id_effect(seller_id_effects,train_ds,test_ds,cleanup)
  train_ds <- dataset_list[[1]]
  test_ds <- dataset_list[[2]]
  
  print('Enhancing data for customer state effects')
  weight_to_use <- target_encoding_weights %>% filter(effect=='customer_state') %>% pull(weight)
  customer_state_effects <- prepare_customer_state_effect(train_ds,weight_to_use)
  dataset_list <- enhance_datasets_using_customer_state_effect(customer_state_effects,train_ds,test_ds,cleanup)
  train_ds <- dataset_list[[1]]
  test_ds <- dataset_list[[2]]
  
  print('Enhancing data for customer loc effects')
  weight_to_use <- target_encoding_weights %>% filter(effect=='customer_loc') %>% pull(weight)
  customer_loc_effects <- prepare_customer_loc_effect(train_ds,weight_to_use)
  dataset_list <- enhance_datasets_using_customer_loc_effect(customer_loc_effects,train_ds,test_ds,cleanup)
  dataset_list
}
```

== TODO additive smoothing/Bayes estimator with target encoding

## Feature Analysis and Selection
Since we needed to build delivery prediction system which could estimate number of hours required for delivery for new orders, while maintaining minimum RMSE, we used our intuition and understanding of the domain to guide us in deriving insights from available predictors. The same are shared below:

* _**Distance between seller and customer**_ could likely influence delivery estimate.
* _**Delay in payment approval**_ could influence delivery time as it would delay shipment from the seller. This could be related to customer zip code as often some customer locations require fraud analysis before payment is approved.
* _**Delay in handover of product from seller to carrier**_ would influence delivery time as any time spent there is wasted. This could be seller related as some sellers maybe known to take more time in processing order compared to other sellers. 
* Some _**customer locations**_ maybe difficult to reach or need special delivery mechanism due to their remote nature. Such locations may need more delivery time compared to other locations irrespective of the distance involved. We need not analyse for each customer location but a rough area could be used based on combination of zip codes.
* _**Weight of the product**_ could influence delivery time as heavy items say bed, furniture may be transported slowly while lighter items maybe shipped more quickly.
* _**Price of the product**_ could influence delivery time. Expensive items may require considerable more attention and could influence delivery like priority shipping or longer delivery time due to caution.
* _**Volume of the product**_ could influence delivery time as smaller products can be shipped together while others may need separate truck and therefore more time for dedicated truck to be available.
* _**Product category**_ could be used for analysis as different product categories would have different packing, assembly, processing requirements and shelf life would could lead to impact in delivery time. 
* _**Freight value**_ could indicate complexity of shipping like fragility of the product, distance, toll booth charges on the way among others. This could be considered for analysis.
* _**Customer state**_ could influence the delivery estimate due to state border checks, paper work before goods delivery among others.
* Different _**sellers**_ may take different amount of time or sell specific type of products which may require different shipping time. So products from a seller selling apparels may be delivered quickly while products from one selling customised products or say furniture may take relatively more time. This could be useful information for predicting delivery time.

For analysis, we used only the training_dataset or subset of it, ensuring validation_dataset was left untouched. 

### Derived features

#### Distance between seller and customer feature
As part of the dataset we had customer and seller location details available in the form of latitude(lat), longitude(lng). We used this crucial knowledge of their location to determine shortest distance between the two on an ellipsoid.

We decided to look more into this intuition. For this we plotted numbers of hours taken for delivery against the distance between seller and the customer. In order to see the trend in this over-plotted graph, gam method based smoothing curve was also plotted on the graph.

```{r Explore distance_between_two_points relation with number_of_hours, echo=FALSE, message=FALSE}
training_dataset  %>% ggplot(aes(distance_between_two_points,number_of_hours)) + geom_point() + geom_smooth() + xlab('Distance between seller and the customer') + ylab('Number of hours taken for delivery')
```
In above plot, we noted that number of hours taken for delivery showed stronger correlation for smaller distances. However, once delivery distance increased beyond a threshold, the correlation weakened between the fields. Though, we could see broader confidence interval for larger distances.

```{r correlation analysis for distance_between_two_points with number of hours, echo=FALSE}
# correlation analysis for distance_between_two_points with number of hours

analysis_output_rows <- sapply(seq(0,2000,1000), function(offset) {  
  list(delay_limit=str_c('>=',offset,' & <',offset+1000),
       correlation=cor(training_dataset %>% 
                         filter(distance_between_two_points>=offset & distance_between_two_points< offset+1000) %>% 
                         pull(distance_between_two_points), 
                       training_dataset %>% 
                         filter(distance_between_two_points>=offset & distance_between_two_points< offset+1000) %>% 
                         pull(number_of_hours))) 
  },simplify=TRUE)

analysis_output_rows %>% t() %>% rbind( c(str_c('>=',3000),
                          round(cor(training_dataset %>% 
                                      filter(distance_between_two_points>=3000) %>%
                                      pull(distance_between_two_points), 
                                    training_dataset %>% 
                                      filter(distance_between_two_points>=3000) %>%
                                      pull(number_of_hours)),7))
                        ) %>% as.data.frame() %>% knitr::kable(caption='Correlation analysis between distance between seller & carrier and number of hours taken for delivery')

rm(analysis_output_rows)
```
This behavior could be useful in estimating delivery timelines. We decided to consider this as a feature for further analysis for inclusion in the model. 

#### Delay in handover of product from seller to carrier feature
As part of the dataset, there were two important attributes i.e. handed_over_to_carrier_date and shipping_limit_date. Here, handed_over_to_carrier_date is the actual timestamp when product ordered was handed over the carrier for delivery. While, shipping_limit_date is the date by which seller should handover the order to the carrier.

Together these fields were considered useful as they could be used to understand if there was a delay or not in handing over product ordered by seller to the carrier. Also, this delay or early handover, could influence delivery timeline even if everything else was to take the same time.

We decided to look more into this intuition. For this, the feature i.e mean_delay_in_handover was computed for each seller. The computed data was then joined with training_dataset and the below graph was used to visualize the relation between delay in handover to carrier and number of hours taken for delivery. In order to see the trend in this over-plotted graph, gam method based smoothing curve was also plotted on the graph.

```{r Explore delay_in_handover_to_carrier relation with number_of_hours, echo=FALSE, message=FALSE}
delay_in_handover_to_carrier_analysis_dataset <- training_dataset %>%
  get_delay_in_handover_to_carrier_per_seller_id()

analysis_joined_data <- training_dataset %>%
  left_join(delay_in_handover_to_carrier_analysis_dataset,by='seller_id')

analysis_joined_data %>% ggplot(aes(mean_delay_in_handover,number_of_hours)) + geom_point() + geom_smooth() + xlab('Delay in handover of product to carrier') + ylab('Number of hours taken for delivery')
```

In above plot, we noted that number of hours taken for delivery shows weaker correlation with  delay in handover by seller to the carrier as long as the ordered product is handed over to carried before the shipping_limit_date. However, post the date, we could see relatively stronger correlation between the fields. 

```{r correlation analysis for mean_delay_in_handover with number of hours, echo=FALSE}
# correlation analysis for mean_delay_in_handover with number of hours
data.frame(delay_limit=character(0),correlation=numeric(0)) %>% 
  add_row(delay_limit='<= 0',correlation=cor(analysis_joined_data %>% 
                                               filter(mean_delay_in_handover<= 0) %>% 
                                               pull(mean_delay_in_handover), 
                                             analysis_joined_data %>% 
                                               filter(mean_delay_in_handover<= 0) %>% 
                                               pull(number_of_hours))
          ) %>% 
  add_row(delay_limit='> 0',correlation=cor(analysis_joined_data %>% 
                                              filter(mean_delay_in_handover> 0) %>% 
                                              pull(mean_delay_in_handover), 
                                            analysis_joined_data %>% 
                                              filter(mean_delay_in_handover> 0) %>% 
                                              pull(number_of_hours))) %>% 
  knitr::kable(caption='Correlation analysis between delay in handover to carrier and number of hours taken for delivery')
```

This behavior could be useful in estimating delivery timelines. We decided to consider this as a feature for further analysis for inclusion in the model. However, as for new orders this information would not be available at time of placing the order, we were required to compute this feature using training data and apply the captured learnings later on test data. 

To be able to do this, as shared earlier, the feature i.e. mean_delay_in_handover was computed for each seller based on training dataset and then applied to test dataset.

```{r cleanup for delay in handover to carrier feature analysis, include=FALSE}
rm(delay_in_handover_to_carrier_analysis_dataset,analysis_joined_data)
```

#### Delay in payment approval feature
In addition to above, as part of the dataset, there were two other important attributes i.e. order_purchase_timestamp and payment_approved_at. Here, payment_approved_at is the actual timestamp when payment is approved for a given order. While, order_purchase_timestamp is the timestamp at which product was ordered.

Together these fields were considered useful as they could be used to understand if there was a delay or not in approving the payment. Also, this delay or early handover, could influence delivery timeline even if everything else was to take the same time.

We decided to look more into this intuition. For this, the feature i.e mean_delay_in_payment_approval was computed for each customer_zip_code_prefix. The computed data was then joined with training_dataset and the below graph was used to visualize the relation between delay in payment approval and number of hours taken for delivery. In order to see the trend in this over-plotted graph, gam method based smoothing curve was also plotted on the graph.

```{r Explore delay_in_payment_approval relation with number_of_hours, echo=FALSE, message=FALSE}
delay_in_payment_approval_analysis_dataset <- training_dataset %>%
  get_delay_in_payment_approval_per_customer_zip_code()

analysis_joined_data <- training_dataset %>%
  left_join(delay_in_payment_approval_analysis_dataset,by='customer_zip_code_prefix')

analysis_joined_data %>% ggplot(aes(mean_delay_in_payment_approval,log10(number_of_hours))) + geom_point() + geom_smooth() + xlab('Delay in payment approval') + ylab('Number of hours taken for delivery [log10(n) scale]')
```
In above plot, we noted that number of hours taken for delivery follow a non linear pattern.

```{r correlation analysis for mean_delay_in_payment_approval with number of hours, echo=FALSE}
# correlation analysis for mean_delay_in_payment_approval with number of hours
analysis_output_rows <- sapply(seq(0,125,25), function(offset) {  
  list(delay_in_approval=str_c('>=',offset,' & <',offset+25),
       correlation=cor(analysis_joined_data %>% 
                         filter(mean_delay_in_payment_approval>=offset & mean_delay_in_payment_approval< offset+25) %>% 
                         pull(mean_delay_in_payment_approval), 
                       analysis_joined_data %>% 
                         filter(mean_delay_in_payment_approval>=offset & mean_delay_in_payment_approval< offset+25) %>% 
                         pull(number_of_hours))) 
  },simplify=TRUE)

analysis_output_rows %>% t() %>% rbind( c(str_c('>=',150),
                          round(cor(analysis_joined_data %>% 
                                      filter(mean_delay_in_payment_approval>=150) %>%
                                      pull(mean_delay_in_payment_approval), 
                                    analysis_joined_data %>% 
                                      filter(mean_delay_in_payment_approval>=150) %>%
                                      pull(number_of_hours)),7))
                        ) %>% as.data.frame() %>% knitr::kable(caption='Correlation analysis between delay in payment approval and number of hours taken for delivery')

rm(analysis_output_rows)
```
This behavior could be useful in estimating delivery timelines. We decided to consider this as a feature for further analysis for inclusion in the model. However, as for new orders this information would not be available at time of placing the order, we were required to compute this feature using training data and apply the captured learnings later on test data. 

To be able to do this, as shared earlier, the feature i.e. mean_delay_in_payment_approval was computed for each customer_zip_code_prefix based on training dataset and then applied to test dataset.


```{r cleanup for delay in payment approval feature analysis, include=FALSE}
rm(delay_in_payment_approval_analysis_dataset,analysis_joined_data)
```

#### Volume feature
As part of the dataset, there were three attributes about product dimensions i.e. product_length_cm, product_height_cm and product_width_cm. Together these fields were considered useful as they could be used to derive volume of the product.

We looked into the intuition that product volume could influence the delivery timelines as larger products need special delivery mechanism and even dedicated trucks at times for shipment, which may not be readily available.

To understand its influence on the delivery timelines, we plotted numbers of hours taken for delivery against the volume of the product ordered in cubic centimeter($cm^3$). In order to see the trend in this over-plotted graph, gam method based smoothing curve was also plotted on the graph.

```{r Explore volume relation with number_of_hours, echo=FALSE, message=FALSE}
training_dataset  %>% ggplot(aes(volume,log10(number_of_hours))) + geom_point() + geom_smooth() + xlab('Volume of ordered product') + ylab('Number of hours taken for delivery [log10(n) scale]')
```
In above plot, we noted that number of hours taken for delivery vary as the product volume changes. There appeared to be stronger influence on delivery timelines for bulkier products beyond a limit. 

```{r correlation analysis for volume with number of hours, echo=FALSE}
# correlation analysis for volume with number of hours

analysis_output_rows <- sapply(seq(0,210000,70000), function(offset) {  
  list(delay_limit=str_c('>=',offset,' & <',offset+70000),
       correlation=cor(training_dataset %>% 
                         filter(volume>=offset & volume< offset+70000) %>% 
                         pull(volume), 
                       training_dataset %>% 
                         filter(volume>=offset & volume< offset+70000) %>% 
                         pull(number_of_hours))) 
  },simplify=TRUE)

analysis_output_rows %>% t() %>% rbind( c(str_c('>=',280000),
                          round(cor(training_dataset %>% 
                                      filter(volume>=280000) %>%
                                      pull(volume), 
                                    training_dataset %>% 
                                      filter(volume>=280000) %>%
                                      pull(number_of_hours)),7))
                        ) %>% as.data.frame() %>% knitr::kable(caption='Correlation analysis between volume and number of hours taken for delivery')

rm(analysis_output_rows)
```

This behavior could be useful in estimating delivery timelines. We decided to consider this as a feature for further analysis for inclusion in the model.

### Categorical features

#### Product category effects
We decided to look into our intuition that different product categories may have different delivery timelines and explore its influence on the prediction results.

Below errorbar plot depicts how average number of hours taken for delivery vary across product categories while at the same time capturing the confidence interval of number of hours taken for delivery for each product category.

```{r Explore product_category_name relation with number_of_hours, echo=FALSE}
training_dataset %>% group_by(product_category_name) %>%
  summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>% 
  mutate(product_category_name=reorder(product_category_name,mean)) %>% 
  ggplot(aes(x=product_category_name,y=mean,ymax=mean+2*se,ymin=mean-2*se)) +
  geom_point()+ geom_errorbar() + theme(axis.text.x = element_text(angle=90,size=8)) + xlab('Product Categories') + ylab('Avg number of hours')
```

In above plot, we noted that average number of hours taken for delivery vary considerably from one product category to another. So our intuition that product categories influence delivery time was confirmed. 

Given the influence, we decided to consider this as a feature for further analysis for inclusion in the model. However, product categories are categorical in nature and they are quite a few categories, incorporating them directly would not have worked for many machine learning models which require numerical values. Additionally in some cases, each category could end up being treated as a predictor which is also the case with one-hot encoding. 

However, to convert categorical variables to numeric values and still be able to limit features and make model training and evaluation less expensive, we applied **target encoding** on the product categories attribute. 

But to do that, first, we checked number of observations per product category and noticed that the distribution could require **regularization** to prevent overfitting. Secondly, target encoding could be erroneous if done against the random dataset.

To solve these, we identified the regularization weight to use by carrying out target encoding for different datasets set of weights, selecting single weight per dataset based on performance metric i.e. RMSE and then we took a mean of weights identified across the datasets.

Once weight was identified, to avoid any randomness due to choice of the dataset, we computed target encoding for product category attribute across 10 datasets and then took a mean for them to settle at the final target encoded values to use for the concerned feature.

For the above analysis, knn model was used, given that it does not make assumptions about the dataset, which was useful in the early phase of analysis. However, to use knn model, we **scaled and normalized** the features as knn model uses Euclidean distance. Scaling and normalization was carried our using training data and then used for test data.

```{r Apply Target encoding for product_category_name, include = FALSE, eval=FALSE}
#TODO marked eval=FALSE to help generate report quickly
  train_subset_for_encoding <- train_subset[1:40000,] %>% select(-customer_loc, -customer_state)

  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,10,5)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>%
    semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='seller_id') %>%
    semi_join(train_weight_set,by='customer_zip_code_prefix')
  
  dataset_list <- enhance_datasets_with_derived_features(train_weight_set,test_weight_set,TRUE)
  train_weight_set <- dataset_list[[1]] %>% select(-seller_id)
  test_weight_set <- dataset_list[[2]] %>% select(-seller_id)
  
  # calculating rmse for each weight
  weight_rmses <- sapply(weights, function(weight) {
      product_category_effects <- train_weight_set %>% 
        group_by(product_category_name) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n)
      
      train_weight_set <- train_weight_set %>% left_join(product_category_effects,by='product_category_name') %>% rename(product_category_effect=mean) %>% select(-product_category_name)
      
      test_weight_set <- test_weight_set %>% left_join(product_category_effects,by='product_category_name') %>% rename(product_category_effect=mean) %>% select(-product_category_name)
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      tuningControl=trainControl(method = "cv", number = 3)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(41,47,53)),trControl=tuningControl)
      
      knn_fit_for_weight
      print(knn_fit_for_weight)
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition)  
# 5

target_encoding_weights <- target_encoding_weights %>% add_row(effect='product_category',weight=weight_to_use)
```

#### Seller id effects
Next we looked into the intuition that orders for products from different sellers based on what they sell, how they operate could influence delivery timelines.

Below errorbar plot depicts how average number of hours taken for delivery vary across sellers while at the same time capturing the confidence interval of number of hours taken for delivery for each seller. Since there were **`r training_dataset %>% dplyr::distinct(seller_id) %>% nrow()`** sellers, we sampled **300** of them to represent their influence effectively in the graph. 

```{r Explore seller_id relation with number_of_hours, echo=FALSE, warning = FALSE}
set.seed(1, sample.kind = 'Rounding')
training_dataset %>% group_by(seller_id) %>%
  summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>%
  mutate(seller_id=reorder(seller_id,mean)) %>% 
  filter(seller_id %in% sample(seller_id,300,replace=FALSE)) %>%
  ggplot(aes(x=seller_id,y=mean,ymax=mean+2*se,ymin=mean-2*se)) +
  geom_point()+ geom_errorbar() + theme(axis.text.x = element_blank()) + xlab('Seller Ids') + ylab('Avg number of hours')
```
In above plot, we noted that average number of hours taken for delivery vary considerably from one seller to another. So our intuition that sellers influence delivery time was confirmed. 

Given the influence, we decided to consider this as a feature for further analysis for inclusion in the model. However, seller ids similar to product categories are categorical in nature and there are **`r training_dataset %>% dplyr::distinct(seller_id) %>% nrow()`** sellers, which are a lot. Incorporating them as it is would not have worked for many machine learning models which require numerical values. As a result, for seller ids we applied target encoding and followed strategy used for product categories.

```{r Apply Target encoding for seller_id, include = FALSE, eval=FALSE}
#TODO marked eval=FALSE to help generate report quickly
  train_subset_for_encoding <- train_subset[1:40000,] %>% select(-customer_loc, -customer_state)

  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,9,3)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>%
    semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='seller_id') %>% 
    semi_join(train_weight_set,by='customer_zip_code_prefix')
  
  ## adding derived features and target encoded categorical features
  dataset_list <- enhance_datasets_with_derived_features(train_weight_set,test_weight_set,FALSE)
  train_weight_set <- dataset_list[[1]]
  test_weight_set <- dataset_list[[2]]
  
  weight_to_use <- target_encoding_weights %>% 
    filter(effect=='product_category') %>% pull(weight)

  product_category_effects <- prepare_product_category_effect(train_weight_set,weight_to_use)
  dataset_list <- enhance_datasets_using_product_category_effect(product_category_effects,train_weight_set,test_weight_set,FALSE)

  train_weight_set <- dataset_list[[1]] %>% select(-order_purchase_timestamp,-handed_over_to_carrier_date,-shipping_limit_date,-customer_zip_code_prefix,-payment_approved_at,-product_category_name)
  test_weight_set <- dataset_list[[2]] %>% select(-order_purchase_timestamp,-handed_over_to_carrier_date,-shipping_limit_date,-customer_zip_code_prefix,-payment_approved_at,-product_category_name)
  ##
  
  # calculating rmse for each weight
  weight_rmses <- sapply(weights, function(weight) {
      seller_id_effects <- train_weight_set %>% 
        group_by(seller_id) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n)
      
      train_weight_set <- train_weight_set %>% left_join(seller_id_effects,by='seller_id') %>% rename(seller_id_effect=mean) %>% select(-seller_id)
      
      test_weight_set <- test_weight_set %>% left_join(seller_id_effects,by='seller_id') %>% rename(seller_id_effect=mean) %>% select(-seller_id)
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      tuningControl=trainControl(method = "cv", number = 3)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(41,47,53)),trControl=tuningControl)
      
      knn_fit_for_weight
      
      print(knn_fit_for_weight)
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition)  
# 8

target_encoding_weights <- target_encoding_weights %>% add_row(effect='seller_id',weight=weight_to_use)
```

#### Customer state effects
Next we looked into the intuition that customer states can have different regulations, paper work, approvals which could influence delivery timelines.

Below errorbar plot depicts how average number of hours taken for delivery vary across customer state while at the same time capturing the confidence interval of number of hours taken for delivery for each customer state. 

```{r Explore customer_state relation with number_of_hours, echo=FALSE, warning = FALSE}
set.seed(1, sample.kind = 'Rounding')
training_dataset %>% group_by(customer_state) %>%
  summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>%
  mutate(customer_state=reorder(customer_state,mean)) %>% 
  ggplot(aes(x=customer_state,y=mean,ymax=mean+2*se,ymin=mean-2*se)) +
  geom_point()+ geom_errorbar() + theme(axis.text.x = element_text(angle=90,size=8)) + xlab('Customer States') + ylab('Avg number of hours')
```
In above plot, we noted that average number of hours taken for delivery vary considerably from one customer state to another. So our intuition that customer state influences delivery time was confirmed. 

Given the influence, we decided to consider this as a feature for further analysis for inclusion in the model. However, customer states similar to product categories are categorical in nature and there are **`r training_dataset %>% dplyr::distinct(customer_state) %>% nrow()`** customer states. Incorporating them as it is would not have worked for many machine learning models which require numerical values. As a result, for customer states we applied target encoding and followed strategy used for product categories.

```{r Apply Target encoding for customer_state, include = FALSE,eval=FALSE}
#TODO marked eval=FALSE to help generate report quickly
  train_subset_for_encoding <- train_subset[1:40000,] %>% select(-customer_loc)
  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,30,15)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>% semi_join(train_weight_set,by='customer_state') 
  
  test_weight_set <- test_weight_set %>%
    semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='seller_id') %>% 
    semi_join(train_weight_set,by='customer_zip_code_prefix') %>% 
    semi_join(train_weight_set,by='customer_state')
  
  ## adding derived features and target encoded categorical features
  dataset_list <- enhance_datasets_with_derived_features(train_weight_set,test_weight_set,FALSE)
  train_weight_set <- dataset_list[[1]]
  test_weight_set <- dataset_list[[2]]
  
  weight_to_use <- target_encoding_weights %>% 
    filter(effect=='product_category') %>% pull(weight)

  product_category_effects <- prepare_product_category_effect(train_weight_set,weight_to_use)
  dataset_list <- enhance_datasets_using_product_category_effect(product_category_effects,train_weight_set,test_weight_set,FALSE)

  train_weight_set <- dataset_list[[1]] %>% select(-order_purchase_timestamp,-handed_over_to_carrier_date,-shipping_limit_date,-customer_zip_code_prefix,-payment_approved_at,-product_category_name)
  test_weight_set <- dataset_list[[2]] %>% select(-order_purchase_timestamp,-handed_over_to_carrier_date,-shipping_limit_date,-customer_zip_code_prefix,-payment_approved_at,-product_category_name)
  
  weight_to_use <- target_encoding_weights %>% filter(effect=='seller_id') %>% pull(weight)
  
  seller_id_effects <- prepare_seller_id_effect(train_weight_set,weight_to_use)
  dataset_list <- enhance_datasets_using_seller_id_effect(seller_id_effects,train_weight_set,test_weight_set,FALSE)

  train_weight_set <- dataset_list[[1]] %>% select(-seller_id)
  test_weight_set <- dataset_list[[2]] %>% select(-seller_id)
  ##

  # calculating rmse for each weight
  weight_rmses <- sapply(weights, function(weight) {
      customer_state_effects <- train_weight_set %>% 
        group_by(customer_state) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n)
      
      train_weight_set <- train_weight_set %>% left_join(customer_state_effects,by='customer_state') %>% rename(customer_state_effect=mean) %>% select(-customer_state) #-product_category_name
      
      test_weight_set <- test_weight_set %>% left_join(customer_state_effects,by='customer_state') %>% rename(customer_state_effect=mean) %>% select(-customer_state) # -product_category_name,
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      tuningControl=trainControl(method = "cv", number = 3)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(34,41,48)),trControl=tuningControl)
      
      knn_fit_for_weight
      
      print(knn_fit_for_weight)
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition) 
# 5

target_encoding_weights <- target_encoding_weights %>%
  add_row(effect='customer_state',weight=weight_to_use)
```

#### Customer location(loc) effects
Next we looked into the intuition that rough information about customer location i.e. combination of co-located zip codes can influence the delivery timelines as customer location can be remote or require different means of transport, among others.

Below errorbar plot depicts how average number of hours taken for delivery vary across customer locations while at the same time capturing the confidence interval of number of hours taken for delivery for each customer location. 

```{r Explore customer_loc relation with number_of_hours, echo=FALSE, warning = FALSE}
set.seed(1, sample.kind = 'Rounding')
training_dataset %>% group_by(customer_loc) %>%
  summarise(mean=mean(number_of_hours),se=sd(number_of_hours)/sqrt(n())) %>%
  mutate(customer_loc=reorder(customer_loc,mean)) %>% 
  ggplot(aes(x=customer_loc,y=mean,ymax=mean+2*se,ymin=mean-2*se)) +
  geom_point()+ geom_errorbar() + theme(axis.text.x = element_blank()) + xlab('Customer Locations') + ylab('Avg number of hours')
```
In above plot, we noted that average number of hours taken for delivery vary considerably from one customer location to another. So our intuition that customer location influences delivery time was confirmed. 

Given the influence, we decided to consider this as a feature for further analysis for inclusion in the model. However, customer locations similar to product categories are categorical in nature and there are **`r training_dataset %>% dplyr::distinct(customer_loc) %>% nrow()`** customer locations. Incorporating them as it is would not have worked for many machine learning models which require numerical values. As a result, for customer states we applied target encoding and followed strategy used for product categories.


```{r Apply Target encoding for customer_loc, include = FALSE,eval=FALSE}
#TODO marked eval=FALSE to help generate report quickly
  train_subset_for_encoding <- train_subset[1:40000,]
  global_mean <- mean(train_subset_for_encoding$number_of_hours)

  set.seed(1,sample.kind='Rounding')
  test_weight_indices <- createDataPartition(train_subset_for_encoding$number_of_hours,p=0.2,times=3,list=FALSE)
  
  weights <- seq(0,10,5)
  
  weight_to_use_per_partition <- sapply(1:3, function(index) {
  train_weight_set <- train_subset_for_encoding[-test_weight_indices[,index],]
  test_weight_set <- train_subset_for_encoding[test_weight_indices[,index],]
  
  test_weight_set <- test_weight_set %>%
    semi_join(train_weight_set,by='product_category_name') %>%
    semi_join(train_weight_set,by='seller_id') %>% 
    semi_join(train_weight_set,by='customer_zip_code_prefix') %>% 
    semi_join(train_weight_set,by='customer_state') %>% 
    semi_join(train_weight_set,by='customer_loc')
  
  ## adding derived features and target encoded categorical features
  dataset_list <- enhance_datasets_with_derived_features(train_weight_set,test_weight_set,FALSE)
  train_weight_set <- dataset_list[[1]]
  test_weight_set <- dataset_list[[2]]
  
  weight_to_use <- target_encoding_weights %>% 
    filter(effect=='product_category') %>% pull(weight)

  product_category_effects <- prepare_product_category_effect(train_weight_set,weight_to_use)
  dataset_list <- enhance_datasets_using_product_category_effect(product_category_effects,train_weight_set,test_weight_set,FALSE)

  train_weight_set <- dataset_list[[1]] %>% select(-order_purchase_timestamp,-handed_over_to_carrier_date,-shipping_limit_date,-customer_zip_code_prefix,-payment_approved_at,-product_category_name)
  test_weight_set <- dataset_list[[2]] %>% select(-order_purchase_timestamp,-handed_over_to_carrier_date,-shipping_limit_date,-customer_zip_code_prefix,-payment_approved_at,-product_category_name)
  
  weight_to_use <- target_encoding_weights %>% filter(effect=='seller_id') %>% pull(weight)
  
  seller_id_effects <- prepare_seller_id_effect(train_weight_set,weight_to_use)
  dataset_list <- enhance_datasets_using_seller_id_effect(seller_id_effects,train_weight_set,test_weight_set,FALSE)

  train_weight_set <- dataset_list[[1]] %>% select(-seller_id)
  test_weight_set <- dataset_list[[2]] %>% select(-seller_id)
  
  weight_to_use <- target_encoding_weights %>% filter(effect=='customer_state') %>% pull(weight)

  customer_state_effects <- prepare_customer_state_effect(train_weight_set,weight_to_use)
  dataset_list <- enhance_datasets_using_customer_state_effect(customer_state_effects,train_weight_set,test_weight_set,TRUE)

  train_weight_set <- dataset_list[[1]]
  test_weight_set <- dataset_list[[2]]
  ##
  
  # calculating rmse for each weight
  weight_rmses <- sapply(weights, function(weight) {
      customer_loc_effects <- train_weight_set %>% 
        group_by(customer_loc) %>% 
        summarise(mean=mean(number_of_hours),n=n()) %>%
        mutate(mean=(n*mean+weight*global_mean)/(n+weight)) %>% 
        select(-n) %>% rename()
      
      train_weight_set <- train_weight_set %>% left_join(customer_loc_effects,by='customer_loc') %>% rename(customer_loc_effect=mean) %>% select(-customer_loc) #-product_category_name
      
      test_weight_set <- test_weight_set %>% left_join(customer_loc_effects,by='customer_loc') %>% rename(customer_loc_effect=mean) %>% select(-customer_loc) # -product_category_name,
      
      set.seed(seed=1,sample.kind = 'Rounding')
      preProcessScalingValues <- preProcess(train_weight_set %>% select(-number_of_hours), method = c("center", "scale"))
      
      scaled_train_subset <- predict(preProcessScalingValues, train_weight_set)
      scaled_test_subset <- predict(preProcessScalingValues, test_weight_set)
      
      tuningControl=trainControl(method = "cv", number = 3)
      knn_fit_for_weight <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(35,43,49)),trControl=tuningControl)
      
      knn_fit_for_weight
      
      print(knn_fit_for_weight)
      
      predictions <- predict(knn_fit_for_weight,scaled_test_subset %>% select(-number_of_hours))

      RMSE(scaled_test_subset$number_of_hours,predictions) 
  })
  
  # selecting weight leading to lowest RMSE
  weight_to_use <- weights[which.min(weight_rmses)]
  paste('Weight to use: ', weight_to_use, ', corresponding RMSE: ', weight_rmses[which.min(weight_rmses)])

  print(weight_rmses)
  weight_to_use
})

weight_to_use_per_partition

weight_to_use <- mean(weight_to_use_per_partition) 
# 20/3

target_encoding_weights <- target_encoding_weights %>% add_row(effect='customer_loc',weight=weight_to_use)
```

```{r Save target encoding weights, include=FALSE, eval=FALSE}
save(target_encoding_weights,file='rda/target_encoding_weights.rda')
```

```{r Load target encoding weights, include=FALSE}
# required only to generate report or resume modeling quickly
load('rda/target_encoding_weights.rda')
```

### Other features

#### Price feature
We looked into the intuition that order item's price could influence the delivery timelines as expensive items ordered may have priority shipping or be delivered with caution. 

To understand its influence on the delivery timelines, we plotted numbers of hours taken for delivery against the price of the ordered item. In order to see the trend, gam method based smoothing curve was plotted on the graph.

```{r Explore price relation with number_of_hours, echo=FALSE, message=FALSE}
training_dataset  %>% ggplot(aes(price,number_of_hours)) + geom_smooth() + xlab('Price of ordered item') + ylab('Number of hours taken for delivery')
```
In above plot, we noted that number of hours taken for delivery vary as the price of the ordered item changes. It appeared to increase significantly as the price of ordered item increased and decreased when the price reached towards higher limits. Though confidence interval indicated higher variability in delivery timelines at higher prices.

This behavior could be useful in estimating delivery timelines. We decided to consider this as a feature for further analysis for inclusion in the model.

#### Product weight feature
Next we looked into the intuition that product weight can influence the delivery timelines as heavier items like furniture, bed may be transported slowly compared to lighter items.

For this we plotted numbers of hours taken for delivery against the weight of the product ordered in grams. In order to see the trend in this over-plotted graph, gam method based smoothing curve was also plotted on the graph.

```{r Explore product weight relation with number_of_hours, echo=FALSE, message=FALSE}
training_dataset  %>% ggplot(aes(product_weight_g,log10(number_of_hours))) + geom_point() + geom_smooth() + xlab('Weight of ordered product in grams') + ylab('Number of hours taken for delivery [log10(n) scale]')
```
In above plot, we noted that number of hours taken for delivery vary as the product weight changes. This behavior could be useful in estimating delivery timelines. We decided to consider this as a feature for further analysis for inclusion in the model.  

#### Freight value feature
Next we looked into the intuition that freight value can help determine the delivery timelines as higher freight value could indicate complexity in shipment, extra processing which would need time, among others.

For this we plotted numbers of hours taken for delivery against the freight value of the product ordered. In order to see the trend in this over-plotted graph, gam method based smoothing curve was also plotted on the graph.

```{r Explore freight value relation with number_of_hours, echo=FALSE, message=FALSE}
training_dataset  %>% ggplot(aes(freight_value,log10(number_of_hours))) + geom_point() + geom_smooth() + xlab('Freight value of ordered product') + ylab('Number of hours taken for delivery [log10(n) scale]')
```
In above plot, we noted that number of hours taken for delivery vary as the freight value of product changes. It appeared to increase as freight value increased for smaller freight values, then plateaued and decreased as freight value reached higher limits The decrease could be due to priority shipping coming at a cost. This behavior could be useful in estimating delivery timelines. We decided to consider this as a feature for further analysis for inclusion in the model.  

## Model Selection
```{r Prepare training and test data for model tuning and development,include=FALSE}
dataset_list <- prepare_data_for_training_testing(train_subset,test_subset,FALSE)
train_subset <- dataset_list[[1]]
test_subset <- dataset_list[[2]]

# cleanup
train_subset <- train_subset %>%
  select(-handed_over_to_carrier_date,-shipping_limit_date,-product_category_name,-seller_id,-customer_state,-customer_loc,-customer_zip_code_prefix,-order_purchase_timestamp,-payment_approved_at)
test_subset <- test_subset %>%
  select(-handed_over_to_carrier_date,-shipping_limit_date,-product_category_name,-seller_id,-customer_state,-customer_loc,-customer_zip_code_prefix,-order_purchase_timestamp,-payment_approved_at)
```

Post evaluation with multiple features, the final set of features identified for model selection were as below:: 

```{r Final set of features identified, echo=FALSE}
train_subset %>% colnames() %>% knitr::kable(col.names=c('Feature'),caption='Features List')
```

To start building our model, training and test subset were prepared. For this, we further partitioned our training data set i.e. training_dataset into a split of 80/20 between train_subset and test_subset to ensure we use only the train_subset for training and tuning the model and test_subset to evaluate the model and measure RMSE. This would ensure there is no data leak. The split like earlier was done such that distribution of outcome i.e. number_of_hours was similar in both training and test data.

The derived features selected earlier were prepared based on train_subset and applied over test_subset, to ensure we maintain the environmental condition where test data is not known to us.

In addition, as earlier shared, categorical features were converted to numeric form with the help of target encoding with a weight term computed earlier for purpose of regularization and to avoid overfitting. Also, any undesired features were removed, reducing train_subset and test_subset to only the desired features.

### k-nearest neighbors(knn) model
k-nearest neighbors(knn) relies on finding nearest neighbors and uses average of the nearest neigbbors for prediction. In addition, it does not make assumptions about the dataset. In our case limited dataset and that the alogrithm does not make any assumptions, made it a good candidate for model development and performance evaluation. 

To use knn model, features were scaled and normalized to prevent bias towards any features, as knn internally calculates Euclidean distance for finding the nearest data-points. 

In addition, for knn model training and tuning, we carried our 5-fold cross validation providing a range of values of k to use for training the model. Here, k means number of neighbors to be considered. The cross validation helped in removing any randomness from our tuning parameters. In addition, this helped reduce the probability of overfitting.

```{r knn model data scaled under test, include=FALSE, eval=FALSE}
## TODO make echo=FALSE 
set.seed(seed=1,sample.kind = 'Rounding')
preProcessScalingValues <- preProcess(train_subset %>% select(-number_of_hours), method = c("center", "scale"))

scaled_train_subset <- predict(preProcessScalingValues, train_subset) 
scaled_test_subset <- predict(preProcessScalingValues, test_subset) 

tuningControl=trainControl(method = "cv", number = 5)
knn_fit <- train(number_of_hours~., scaled_train_subset, method='knn',tuneGrid=data.frame(k=c(45,71,77,83)),trControl=tuningControl)

save(knn_fit,file='rda/knn_model.rda')
```

Details of the trained knn model can be found below:
 
```{r trained knn model based on train_subset, echo=FALSE }
load('rda/knn_model.rda')
knn_fit

knn_fit %>% ggplot()
```

Based on model results, k value of `r knn_fit$finalModel$k` was identified to be the one with lowest RMSE.

```{r trained knn model validation, include=FALSE, eval=FALSE}
predictions <- predict(knn_fit,scaled_test_subset %>% select(-number_of_hours))

performance_metric <- performance_metric %>% 
  add_row(type='Knn model(partial dataset)',
          value=RMSE((scaled_test_subset %>% pull(number_of_hours)),predictions))

performance_metric
#183.0028 for k=77

save(performance_metric,file='rda/performance_metric.rda')
```

Below is the summary of each model and knn model(which used partial dataset) against the performance metric i.e. RMSE

```{r performance metric for knn model based on test data, echo =FALSE}
load('rda/performance_metric.rda')
performance_metric[1:3,] %>% knitr::kable(caption='Performance Metric(RMSE) Summary')
```

As we can see the performance metric i.e. RMSE is significantly better for knn model (though with partial dataset at this point of time), than the one being used by Olist store.

### Random forest model
Next, we used Random forest model, which is basically ensemble of multiple decision trees. The training dataset is split into multiple subsets. For each such subset, decision tree is created. However, randomness is introduced in creation of decision trees as at each node, the tree leverages a number of random features to determine the best split. Once the various models are ready, they are used to prepare predictions. The final prediction is reached by taking average of all the trees.

The ensemble of multiple decision trees helps prevent overfitting and creates a more stable prediction. While, the number of random features at each node of decision tree to determine best split, helps reduce bias and variance in the model. It is these qualities of random forest, which prompted us to consider this for model development and evaluation.

To train and tune random forest model, we carried our 4-fold cross validation providing a range of values of mtry to use for training the model. Here, mtry means number of random features used to determine the best split at each node of the decision tree. In addition, we configured the number of decision trees to create as 200. Here, the cross validation not only helped to reduce the probability of overfitting but also helped in identifying the tuning parameter i.e. mtry which would work across randomly selected datasets. 

```{r rf model, include= FALSE, eval = FALSE}
# TODO make it echo=FALSE
set.seed(seed=1,sample.kind = 'Rounding')
tuningControl=trainControl(method = "cv", number = 4)
rf_fit <- train(number_of_hours~., train_subset, method='rf',tuneGrid = data.frame(mtry = c(1,2,3,4,5)),trControl=tuningControl,ntree=200)

save(rf_fit,file='rda/rf_model.rda')
```

Details of the trained random forest model can be found below:
```{r trained random forest model based on train_subset, echo=FALSE}
load('rda/rf_model.rda')
rf_fit

rf_fit %>% ggplot()
```

```{r trained random forest model validation, include=FALSE, eval=FALSE}
predictions <- predict(rf_fit,test_subset %>% select(-number_of_hours))

performance_metric <- performance_metric %>% 
  add_row(type='Random forest model(partial dataset)',
          value=RMSE((test_subset %>% pull(number_of_hours)),predictions))

performance_metric
# 173.3481 for mtry 2. There are total of 11 predictors.
```

Based on model results, mtry value of `r rf_fit$finalModel$mtry` was identified to be the one with lowest RMSE.

Below is the summary of each model and random forest model(which used partial dataset) against the performance metric i.e. RMSE

```{r performance metric for random forest model based on test data, echo =FALSE}
load('rda/performance_metric.rda')
performance_metric[1:4,] %>% knitr::kable(caption='Performance Metric(RMSE) Summary')
```

As we can see the performance metric i.e. RMSE was significantly better for random forest model (though with partial dataset at this point of time), than the one being used by Olist store. In addition, we could see improvement in RMSE when compared to knn model for the same dataset.

### Ensemble model 
The ensemble models use multiple algorithms and if properly developed, they can be built to take learnings from the different models and generate better predictions. Since we had knn and random forest models tuned and prepared, we used them to create variety of ensemble models.

#### Average based ensemble model
We took previously trained and tuned knn and random forest model and created ensemble of both. For this, predictions using both knn and random forest models were computed. Then mean of the predicitions was taken. This was used to calculate performance metric i.e. RMSE.

```{r Ensemble of knn and rf model using partial data, include=FALSE, eval= FALSE}
load('rda/knn_model.rda')
load('rda/rf_model.rda')
set.seed(1, sample.kind = 'Rounding')

predictions_knn <- predict(knn_fit,scaled_test_subset %>% select(-number_of_hours))
predictions_rf <- predict(rf_fit,test_subset %>% select(-number_of_hours))

predictions_df <- data.frame(predictions_knn,predictions_rf)

# mean approach
performance_metric <- performance_metric %>% 
  add_row(type='Avg based ensemble model(partial dataset)',
          value=RMSE(rowMeans(predictions_df),(test_subset %>% pull(number_of_hours) )))
#175.7183
```

```{r Save performance metric, include=FALSE, eval=FALSE}
save(performance_metric,file='rda/performance_metric.rda')
```

Below is the summary of each model and ensemble model(which used partial dataset) against the performance metric i.e. RMSE

```{r performance metric for ensemble model based on test data, echo =FALSE}
# required only to generate report
load('rda/performance_metric.rda')
performance_metric[1:5,] %>% knitr::kable(caption='Performance Metric(RMSE) Summary')
```

As we can see the performance metric i.e. RMSE was significantly better for ensemble model (though with partial dataset at this point of time), than the one being used by Olist store. However, it was not better than the performance metric captured for random forest model.

#### Weighted average based ensemble model
Other alternative ensemble technique like weighted average approach was explored. Again, we took previously trained and tuned knn and random forest model and created ensemble of both. However, this time, final predictions were based on weighted average of the predictions of the constituient models. 

The results of performance metric i.e. RMSE for the ensemble model against increasing weights for knn model based predictions are plotted on the graph.

```{r weighted average ensemble approach with partial dataset, include=FALSE, eval=FALSE}
# weighted approach
rmses_weighted_ensemble_model <- sapply(seq(0,1,0.15),function(a) {
  RMSE((test_subset %>% pull(number_of_hours)),
        predictions_df %>% mutate(value=predictions_knn*a+predictions_rf*(1-a)) %>% 
          pull(value)
       )
  })
save(rmses_weighted_ensemble_model,file='rda/rmses_weighted_ensemble_model.rda')
```

```{r Summary of performance metric for weighted ensemble approach, echo=FALSE}
load('rda/rmses_weighted_ensemble_model.rda')
weights_for_ensemble_model <- seq(0,1,0.15)

data.frame(weight_for_knn_pred=weights_for_ensemble_model,rmse=rmses_weighted_ensemble_model) %>%
  ggplot(aes(weight_for_knn_pred,rmse)) + geom_line() + geom_point() + 
  xlab('Weight used(for knn prediction)') + ylab('Resultant RMSE')
```

We noted that weighted ensemble model had lowest RMSE of `r min(rmses_weighted_ensemble_model)` at weight=`r weights_for_ensemble_model[which.min(rmses_weighted_ensemble_model)]`

In addition, other ensembling techniques like stacking were explored, however they could not improve the RMSE.

### Selected model
Given above results of performance metric i.e. RMSE across knn, random forest and ensemble models, we selected random forest as the final model for our delivery prediction system.

# Results
Post model tuning and evaluation, random forest model emerged as the best fit for our data among the models considered for selection. The tuning parameters identified for the final random forest algorithm are shared below:

```{r Tuning parameters to use for final random forest, echo=FALSE}
load('rda/rf_model.rda')

data.frame(tuning_parameter='mtry', value=rf_fit$finalModel$mtry) %>% add_row(tuning_parameter='number_of_trees', value=500) %>% knitr::kable(caption='Random forest tuning parameters')
```

Till this point, we had used only training data set i.e. training_dataset to identify the features, evaluate various models and tune them. Having selected the final model, we prepared first the derived features, which as shared earlier were prepared on training_dataset. The features were then applied over validation_dataset, which was the dataset kept aside till now.

In addition, categorical features were converted to numeric form with the help of target encoding with a weight term computed earlier for purpose of regularization and to avoid overfitting. Also, any undesired features were removed, reducing training_dataset and validation_dataset to only the desired features.

```{r Prepare dataset for final model training, include = FALSE}
set.seed(seed=1,sample.kind = 'Rounding')

validation_dataset <- validation_dataset %>% 
  compute_volume_using_product_info() %>% 
  extract_distance_between_seller_customer() %>% 
  compute_rough_customer_location()

dataset_list <- prepare_data_for_training_testing(training_dataset,validation_dataset,FALSE)
training_dataset <- dataset_list[[1]]
validation_dataset <- dataset_list[[2]]

# cleanup
training_dataset <- training_dataset %>%
  select(-handed_over_to_carrier_date,-shipping_limit_date,-product_category_name,-seller_id,-customer_state,-customer_loc,-customer_zip_code_prefix,-order_purchase_timestamp,-payment_approved_at)
validation_dataset <- validation_dataset %>%
  select(-handed_over_to_carrier_date,-shipping_limit_date,-product_category_name,-seller_id,-customer_state,-customer_loc,-customer_zip_code_prefix,-order_purchase_timestamp,-payment_approved_at)
```


```{r Final Model Training, include=FALSE,eval = FALSE}
# TODO marked eval=FALSE to prevent it from running during report generation
set.seed(seed=1,sample.kind = 'Rounding')

mtry_to_use <- rf_fit$finalModel$mtry
ntree_to_use <- 500 #rf_fit$finalModel$ntree

final_model <- randomForest(number_of_hours~., training_dataset,mtry= mtry_to_use, ntree=ntree_to_use)

save(final_model,file='rda/final_model.rda')
```

```{r trained final random forest model based on training_dataset, echo=FALSE}
load('rda/final_model.rda')
#final_model

plot(final_model,main='Random forest model error against number of trees')
```

```{r Final model validation, include = FALSE, eval=FALSE}
# TODO marked eval=FALSE to prevent it from running during report generation
predictions <- predict(final_model,validation_dataset %>% select(-number_of_hours))

performance_metric <- performance_metric %>% 
  add_row(type='Final model',
          value=RMSE((validation_dataset %>% pull(number_of_hours) ),predictions))
# 188.5793 for mtry=2, ntree=500

performance_metric
```

Below is the summary of models against the performance metric i.e. RMSE
```{r Performance metric post final model validation, echo=FALSE}
performance_metric %>% knitr::kable(caption='Performance Metric(RMSE) Summary')
```


```{r Final knn model traing with data scaled, include=FALSE, eval= FALSE}
set.seed(seed=1,sample.kind = 'Rounding')
preProcessScalingValues <- preProcess(training_dataset %>% select(-number_of_hours), method = c("center", "scale"))

scaled_train_subset <- predict(preProcessScalingValues, training_dataset) 
scaled_test_subset <- predict(preProcessScalingValues, validation_dataset) 

final_knn_fit <- knnreg(number_of_hours~., scaled_train_subset,k = knn_fit$finalModel$k)

final_knn_fit

predictions <- predict(final_knn_fit,scaled_test_subset %>% select(-number_of_hours))

RMSE(predictions,(scaled_test_subset %>% pull(number_of_hours) ))
# 202.8552

save(final_knn_fit,file='rda/final_knn_fit.rda')
```


```{r Final ensemble model, include=FALSE, eval= FALSE}
load('rda/final_knn_fit.rda')
load('rda/final_model.rda')

set.seed(seed=1,sample.kind = 'Rounding')

predictions_knn <- predict(final_knn_fit,scaled_test_subset %>% select(-number_of_hours))
predictions_rf <- predict(final_model,validation_dataset %>% select(-number_of_hours))

predictions_df <- data.frame(predictions_knn,predictions_rf)

RMSE(rowMeans(predictions_df),(validation_dataset %>% pull(number_of_hours) ))
# 193.3459
```

For the **final model**, the performance metric i.e. RMSE = **`r performance_metric %>% filter(type=='Final model') %>% pull(value)`** captured against **validation_dataset** was much better than what Olist store had as identified from the dataset. The newly built model can be used for predicting number of hours required for delivery of a given ordered item.

# Conclusion
As part of this project, the goal was to build a delivery prediction system which could predict the number of hours required to delivered the ordered item rating with minimal RMSE. 

During the course of project, we carried out data cleaning, identified features and derived features from existing information available. In addition, converted categorical features to numeric form and applied various techniques like target encoding, scaling and normalization as required and regularized the required features. 

Once the feature analysis and selection was completed, we used the features to tune models like k-nearest neighbors, random forest. We carried out k-fold cross validation to remove any randomness from the model training and tuning process. This additionally helped to reduce probability of overfitting. Post this, we explored multiple ensemble models using techniques like averaging, weighted average, stacking, among others. Finally we selected random forest based model and the model so trained performs within desired range of RMSE and can be useful for predicting number of hours required for delivery of a given ordered item. Due care was taken to prevent overfitting from influencing our model.

While the model is a significant improvement over what Olist store had, there is lot of room to improve it. It can be improved for incremental learning so that it can learn from new orders and their delivery details once they become available. In addition, it could be improved to handle new sellers or product categories.

Having said that the delivery prediction system built as part of the project is performant and can be leveraged to develop further.